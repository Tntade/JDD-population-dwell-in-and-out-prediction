{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018.12.10由于很多新建的特征，对于Test并不适用，所以在V2.1上进行进一步整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import date\n",
    "import datetime\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowdf=pd.read_csv('../data/flow_train.csv')\n",
    "trandf=pd.read_csv('../data/transition_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20180302, 20180303, 20180304, 20180305, 20180306, 20180307, 20180308, 20180309, 20180310, 20180311, 20180312, 20180313, 20180314, 20180315, 20180316]\n"
     ]
    }
   ],
   "source": [
    "start='20180301'\n",
    "dates=[]\n",
    "for i in range(1,16):\n",
    "    dd=datetime.datetime.strptime(start,\"%Y%m%d\")\n",
    "    date=dd+datetime.timedelta(days=i)\n",
    "    dates.append(int(datetime.datetime.strftime(date,\"%Y%m%d\")))\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trandf['start_address']=trandf['o_city_code'].astype(str)+':'+trandf['o_district_code'].astype(str)\n",
    "trandf['end_address']=trandf['d_city_code'].astype(str)+':'+trandf['d_district_code'].astype(str)\n",
    "\n",
    "flowdf['address']=flowdf['city_code'].astype(str)+':'+flowdf['district_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_address=list(set(trandf['start_address']))\n",
    "end_address=list(set(trandf['end_address']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==========没办法，得先建立个tran_train.testdf，否则脑子转不动===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142590, 4)\n",
      "(142590, 7) Index(['start_address', 'end_address', 'date_dt', 'o_city_code',\n",
      "       'o_district_code', 'd_city_code', 'd_district_code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def get_df(k=98,add_list=start_address):\n",
    "    df=pd.DataFrame()\n",
    "    df['row_id']=np.arange(9604)\n",
    "    df['start_address']='0'\n",
    "    df['end_address']='0'\n",
    "    for i in range(0,9603,98):\n",
    "        df['start_address'][i:(i+k)]=add_list[int(i/k)]\n",
    "        for index in range(i,i+k):\n",
    "            df.loc[index,'end_address']=add_list[int(index%k)]\n",
    "    return df\n",
    "\n",
    "test=get_df(k=98,add_list=start_address)\n",
    "test['date_dt']=dates[0]\n",
    "\n",
    "for i in range(1,15,1):\n",
    "    df=get_df(k=98,add_list=start_address)\n",
    "    df['date_dt']=dates[i]\n",
    "    test=pd.concat([test,df],axis=0)\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "test=test[test['start_address']!=test['end_address']]\n",
    "print(test.shape)\n",
    "\n",
    "del test['row_id']\n",
    "\n",
    "test['o_city_code']=test['start_address'].apply(lambda x:str(x).split(':')[0])\n",
    "test['o_district_code']=test['start_address'].apply(lambda x:str(x).split(':')[1])\n",
    "test['d_city_code']=test['end_address'].apply(lambda x:str(x).split(':')[0])\n",
    "test['d_district_code']=test['end_address'].apply(lambda x:str(x).split(':')[1])\n",
    "print(test.shape,test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ======哦哟，以上终于建立好了tran_train.Testdf========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>=====索性flow_train.testdf一起建立好了<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(flowdf.shape,flowdf.columns)\n",
    "\n",
    "# dates=[20180302, 20180303, 20180304, 20180305, 20180306, 20180307, 20180308, 20180309, 20180310, \n",
    "#        20180311, 20180312, 20180313, 20180314, 20180315, 20180316]\n",
    "\n",
    "# n=98\n",
    "# flow_test=pd.DataFrame()\n",
    "# flow_test['row_id']=np.arange(0,1470)\n",
    "# flow_test['date_dt']=0\n",
    "# for i in range(15):     \n",
    "#     flow_test['date_dt'][i*n:(i+1)*n]=dates[i]\n",
    "#     flow_test['address']=0\n",
    "#     for j in range(0,1470):\n",
    "#         flow_test.loc[flow_test['row_id']==j,'address']=start_address[j%n]\n",
    "# flow_test['city_code']=flow_test['address'].apply(lambda x:str(x).split(':')[0])\n",
    "# flow_test['district_code']=flow_test['address'].apply(lambda x:str(x).split(':')[1])\n",
    "# print(flow_test.groupby('date_dt').address.count())\n",
    "\n",
    "# del flow_test['row_id']\n",
    "\n",
    "# print(flow_test.shape,flow_test.columns)\n",
    "\n",
    "# flow_test.to_csv('../data/flow_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>特征工程<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month(date):\n",
    "    date=str(date)\n",
    "    return int(date[4:6])\n",
    "\n",
    "def day(date):\n",
    "    date=str(date)\n",
    "    return int(date[6:8])\n",
    "\n",
    "def weekd(date):\n",
    "    date=str(date)\n",
    "    return str(datetime.strptime(date,'%Y%m%d').weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_test=pd.read_csv('../data/flow_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "trandf['month']=trandf['date_dt'].apply(month)\n",
    "trandf['month']=trandf['month'].apply(lambda x:int(x+12) if x <4 else x)\n",
    "trandf['day']=trandf['date_dt'].apply(day)\n",
    "trandf['weekd']=trandf['date_dt'].apply(weekd)\n",
    "\n",
    "test['month']=test['date_dt'].apply(month)\n",
    "test['month']=test['month'].apply(lambda x:int(x+12) if x<4 else x)\n",
    "test['day']=test['date_dt'].apply(day)\n",
    "test['weekd']=test['date_dt'].apply(weekd)\n",
    "\n",
    "flowdf['month']=flowdf['date_dt'].apply(month)\n",
    "flowdf['month']=flowdf['month'].apply(lambda x:int(x+12) if x <4 else x)\n",
    "flowdf['day']=flowdf['date_dt'].apply(day)\n",
    "flowdf['weekd']=flowdf['date_dt'].apply(weekd)\n",
    "\n",
    "flow_test['month']=flow_test['date_dt'].apply(month)\n",
    "flow_test['month']=flow_test['month'].apply(lambda x:int(x+12) if x <4 else x)\n",
    "flow_test['day']=flow_test['date_dt'].apply(day)\n",
    "flow_test['weekd']=flow_test['date_dt'].apply(weekd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========数据合并后一起处理======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([trandf,test],axis=0)\n",
    "data['cnt']=round(data['cnt'],6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标签数值化\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "\n",
    "le.fit(data['o_city_code'])\n",
    "data['o_city_code']=le.transform(data['o_city_code'])\n",
    "data['d_city_code']=le.transform(data['d_city_code'])\n",
    "\n",
    "le.fit(data['o_district_code'])\n",
    "data['o_district_code']=le.transform(data['o_district_code'])\n",
    "data['d_district_code']=le.transform(data['d_district_code'])\n",
    "\n",
    "le.fit(data['start_address'])\n",
    "data['start_address']=le.transform(data['start_address'])\n",
    "flowdf['address']=le.transform(flowdf['address'])\n",
    "flow_test['address_code']=flow_test['address']\n",
    "flow_test['address_code']=le.transform(flow_test['address_code'])\n",
    "data['end_address']=le.transform(data['end_address'])\n",
    "\n",
    "flow_test.to_csv('../data/flow_test1.csv',index=False)\n",
    "flowdf.to_csv('../data/flow_train1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>基本特征<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>mean  ·  nunique ·  count特征<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day  notnull_mean\n",
      "0     1      0.857565\n",
      "1     2      0.873446\n",
      "2     3      0.865991\n",
      "3     4      0.854906\n",
      "4     5      0.847746\n",
      "5     6      0.836793\n",
      "6     7      0.844189\n",
      "7     8      0.836445\n",
      "8     9      0.840161\n",
      "9    10      0.846947\n",
      "10   11      0.845310\n",
      "11   12      0.835038\n",
      "12   13      0.832419\n",
      "13   14      0.858050\n",
      "14   15      0.844272\n",
      "15   16      0.834408\n",
      "16   17      0.820314\n",
      "17   18      0.840883\n",
      "18   19      0.845215\n",
      "19   20      0.852713\n",
      "20   21      0.856026\n",
      "21   22      0.863342\n",
      "22   23      0.849249\n",
      "23   24      0.867202\n",
      "24   25      0.851493\n",
      "25   26      0.842167\n",
      "26   27      0.849426\n",
      "27   28      0.870810\n",
      "28   29      0.863141\n",
      "29   30      0.888666\n",
      "30   31      0.856422\n"
     ]
    }
   ],
   "source": [
    "# print(data[data['cnt'].notnull()][['day','cnt']].groupby('day').cnt.mean().reset_index(name='notnull_mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day  alldf_mean\n",
      "0     1    0.857565\n",
      "1     2    0.873446\n",
      "2     3    0.865991\n",
      "3     4    0.854906\n",
      "4     5    0.847746\n",
      "5     6    0.836793\n",
      "6     7    0.844189\n",
      "7     8    0.836445\n",
      "8     9    0.840161\n",
      "9    10    0.846947\n",
      "10   11    0.845310\n",
      "11   12    0.835038\n",
      "12   13    0.832419\n",
      "13   14    0.858050\n",
      "14   15    0.844272\n",
      "15   16    0.834408\n",
      "16   17    0.820314\n",
      "17   18    0.840883\n",
      "18   19    0.845215\n",
      "19   20    0.852713\n",
      "20   21    0.856026\n",
      "21   22    0.863342\n",
      "22   23    0.849249\n",
      "23   24    0.867202\n",
      "24   25    0.851493\n",
      "25   26    0.842167\n",
      "26   27    0.849426\n",
      "27   28    0.870810\n",
      "28   29    0.863141\n",
      "29   30    0.888666\n",
      "30   31    0.856422\n"
     ]
    }
   ],
   "source": [
    "# print(data[['day','cnt']].groupby('day').cnt.mean().reset_index(name='alldf_mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day  notnull_cnt\n",
      "0     1        90246\n",
      "1     2        81291\n",
      "2     3        81732\n",
      "3     4        81587\n",
      "4     5        81387\n",
      "5     6        80876\n",
      "6     7        80969\n",
      "7     8        81426\n",
      "8     9        81457\n",
      "9    10        81487\n",
      "10   11        81304\n",
      "11   12        81159\n",
      "12   13        81322\n",
      "13   14        81422\n",
      "14   15        81188\n",
      "15   16        80871\n",
      "16   17        80863\n",
      "17   18        81365\n",
      "18   19        81536\n",
      "19   20        81521\n",
      "20   21        81824\n",
      "21   22        82169\n",
      "22   23        81578\n",
      "23   24        81783\n",
      "24   25        81669\n",
      "25   26        81571\n",
      "26   27        81536\n",
      "27   28        81763\n",
      "28   29        72747\n",
      "29   30        73105\n",
      "30   31        45566\n"
     ]
    }
   ],
   "source": [
    "# print(data[data['cnt'].notnull()][['day','cnt']].groupby('day').cnt.count().reset_index(name='notnull_cnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day  alldf_count\n",
      "0     1        90246\n",
      "1     2        81291\n",
      "2     3        81732\n",
      "3     4        81587\n",
      "4     5        81387\n",
      "5     6        80876\n",
      "6     7        80969\n",
      "7     8        81426\n",
      "8     9        81457\n",
      "9    10        81487\n",
      "10   11        81304\n",
      "11   12        81159\n",
      "12   13        81322\n",
      "13   14        81422\n",
      "14   15        81188\n",
      "15   16        80871\n",
      "16   17        80863\n",
      "17   18        81365\n",
      "18   19        81536\n",
      "19   20        81521\n",
      "20   21        81824\n",
      "21   22        82169\n",
      "22   23        81578\n",
      "23   24        81783\n",
      "24   25        81669\n",
      "25   26        81571\n",
      "26   27        81536\n",
      "27   28        81763\n",
      "28   29        72747\n",
      "29   30        73105\n",
      "30   31        45566\n"
     ]
    }
   ],
   "source": [
    "# print(data[['day','cnt']].groupby('day').cnt.count().reset_index(name='alldf_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>当test中的cnt没有用0填补的时候，计算cnt的mean()或count()都不会将test的计入<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622910, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timefea=['day','weekd']\n",
    "addfea=['start_address','end_address']\n",
    "\n",
    "#时间一维\n",
    "for tf in timefea:\n",
    "    temp=data[[tf,'cnt']]\n",
    "    meandf=temp.groupby(tf).cnt.mean().reset_index(name=tf+'_cnt_mean')\n",
    "    cntdf1=temp.groupby(tf).cnt.count().reset_index(name=tf+'_cnt_count')\n",
    "    data=data.merge(meandf,on=tf,how='left')\n",
    "    data=data.merge(cntdf1,on=tf,how='left')\n",
    "print(data.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622910, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#地址一维\n",
    "for af in addfea:\n",
    "    temp=data[[af,'cnt']]\n",
    "    meandf=temp.groupby(af).cnt.mean().reset_index(name=af+'_cnt_mean')\n",
    "    cntdf=temp.groupby(af).cnt.count().reset_index(name=af+'_cnt_count')\n",
    "    data=data.merge(meandf,on=af,how='left')\n",
    "    data=data.merge(cntdf,on=af,how='left')\n",
    "\n",
    "print(data.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622910, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#时间二维\n",
    "temp=data[['cnt','day','weekd']]\n",
    "meandf=temp.groupby(['day','weekd']).cnt.mean().reset_index(name='day_week_cnt_mean')\n",
    "cntdf1=temp.groupby(['day','weekd']).cnt.count().reset_index(name='day_week__cnt_count')\n",
    "data=data.merge(meandf,on=['day','weekd'],how='left')\n",
    "data=data.merge(cntdf1,on=['day','weekd'],how='left')\n",
    "print(data.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622910, 23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#地址二维\n",
    "temp=data[['cnt','start_address','end_address']]\n",
    "meandf=temp.groupby(['start_address','end_address']).cnt.mean().reset_index(name='start_end_cnt_mean')\n",
    "cntdf=temp.groupby(['start_address','end_address']).cnt.count().reset_index(name='start_end_cnt_count')\n",
    "data=data.merge(meandf,on=['start_address','end_address'],how='left')\n",
    "data=data.merge(cntdf,on=['start_address','end_address'],how='left')\n",
    "print(data.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622910, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #时间地址交叉维度\n",
    "for tf in timefea:\n",
    "    for af in addfea:\n",
    "        temp=data[[tf,af,'cnt']]\n",
    "        meandf=temp.groupby([tf,af]).cnt.mean().reset_index(name=tf+'_'+af+'_cnt_mean')\n",
    "        cntdf=temp.groupby([tf,af]).cnt.count().reset_index(name=tf+'_'+af+'_cnt_count')\n",
    "        data=data.merge(meandf,on=[tf,af],how='left')\n",
    "        data=data.merge(cntdf,on=[tf,af],how='left')\n",
    "print(data.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622910, 35)\n"
     ]
    }
   ],
   "source": [
    "#时间二维与单地址的交叉\n",
    "for af in addfea:\n",
    "    temp=data[['day','weekd',af,'cnt']]\n",
    "    meandf=temp.groupby(['day','weekd',af]).cnt.mean().reset_index(name='day_week_'+af+'_cnt_mean')\n",
    "    cntdf=temp.groupby(['day','weekd',af]).cnt.count().reset_index(name='day_week_'+af+'_cnt_count')\n",
    "    data=data.merge(meandf,on=['day','weekd',af],how='left')\n",
    "    data=data.merge(cntdf,on=['day','weekd',af],how='left')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2622910, 39)\n"
     ]
    }
   ],
   "source": [
    "#时间一维与地址二维度的交叉\n",
    "for tf in timefea:\n",
    "    temp=data[['start_address','end_address',tf,'cnt']]\n",
    "    meandf=temp.groupby(['start_address','end_address',tf]).cnt.mean().reset_index(name='st_end_'+tf+'_cnt_mean')\n",
    "    cntdf=temp.groupby(['start_address','end_address',tf]).cnt.count().reset_index(name='st_end_'+tf+'_cnt_count')\n",
    "    data=data.merge(meandf,on=['start_address','end_address',tf],how='left')\n",
    "    data=data.merge(cntdf,on=['start_address','end_address',tf],how='left')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del temp\n",
    "del meandf\n",
    "del cntdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>由于因变量y值呈偏右态长尾分布，其中17%的数据的y值均大于1的，相对于0.0003类的数据，1.0已然偏<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cnt'][data['cnt'].notnull()]=data['cnt'][data['cnt'].notnull()].apply(lambda x:round(np.sqrt(x),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt                                 142590\n",
      "d_city_code                              0\n",
      "d_district_code                          0\n",
      "date_dt                                  0\n",
      "day                                      0\n",
      "end_address                              0\n",
      "month                                    0\n",
      "o_city_code                              0\n",
      "o_district_code                          0\n",
      "start_address                            0\n",
      "weekd                                    0\n",
      "day_cnt_mean                             0\n",
      "day_cnt_count                            0\n",
      "weekd_cnt_mean                           0\n",
      "weekd_cnt_count                          0\n",
      "start_address_cnt_mean                   0\n",
      "start_address_cnt_count                  0\n",
      "end_address_cnt_mean                     0\n",
      "end_address_cnt_count                    0\n",
      "day_week_cnt_mean                        0\n",
      "day_week__cnt_count                      0\n",
      "start_end_cnt_mean                       0\n",
      "start_end_cnt_count                      0\n",
      "day_start_address_cnt_mean               0\n",
      "day_start_address_cnt_count              0\n",
      "day_end_address_cnt_mean                 0\n",
      "day_end_address_cnt_count                0\n",
      "weekd_start_address_cnt_mean             0\n",
      "weekd_start_address_cnt_count            0\n",
      "weekd_end_address_cnt_mean               0\n",
      "weekd_end_address_cnt_count              0\n",
      "day_week_start_address_cnt_mean          0\n",
      "day_week_start_address_cnt_count         0\n",
      "day_week_end_address_cnt_mean            0\n",
      "day_week_end_address_cnt_count           0\n",
      "st_end_day_cnt_mean                     95\n",
      "st_end_day_cnt_count                     0\n",
      "st_end_weekd_cnt_mean                    6\n",
      "st_end_weekd_cnt_count                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.isnull(data).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>以上几个缺失值，再用lightgbm的时候，是可以不用补上的，算法自动处理；如果用其他算法的时候，建议直接0补全<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 历史转移率特征，来挖掘历史转移信息，同时防止过拟合（参考历史点击率特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['period'] = data['day'].astype(int)\n",
    "# for feat_1 in addfea:\n",
    "#     res = pd.DataFrame()\n",
    "#     temp = data[data['cnt']!=0][[feat_1, 'period', 'cnt']]\n",
    "#     for period in range(1, 32):\n",
    "#         if period == 1:\n",
    "#             count = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].count()).reset_index(name=feat_1 + '_all')\n",
    "#             count1 = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].sum()).reset_index(name=feat_1 + '_1')\n",
    "#         else:\n",
    "#             count = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].count()).reset_index(name=feat_1 + '_all')\n",
    "#             count1 = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].sum()).reset_index(name=feat_1 + '_1')\n",
    "#         count[feat_1 + '_1'] = count1[feat_1 + '_1']\n",
    "#         count.fillna(value=0, inplace=True)\n",
    "#         count[feat_1 + '_rate'] = round(count[feat_1 + '_1'] / count[feat_1 + '_all'], 6)\n",
    "#         count['period'] = period\n",
    "#         count.drop([feat_1 + '_all', feat_1 + '_1'], axis=1, inplace=True)\n",
    "#         count.fillna(value=0, inplace=True)\n",
    "#         res = res.append(count, ignore_index=True)\n",
    "#     print(feat_1, ' over')\n",
    "#     data = pd.merge(data, res, how='left', on=[feat_1, 'period'])\n",
    "# print(data.shape)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['period'] = data['day']\n",
    "# feat_1=['start_address','end_address']\n",
    "# res = pd.DataFrame()\n",
    "# temp = data[data['cnt']!=0][['start_address','end_address','period', 'cnt']]\n",
    "# for period in range(1, 32):\n",
    "#     if period == 1:\n",
    "#         count = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].count()).reset_index(name=str(period)+'_all')\n",
    "#         count1 = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].sum()).reset_index(name=str(period)+'_1')\n",
    "#     else:\n",
    "#         count = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].count()).reset_index(name=str(period)+'_all')\n",
    "#         count1 = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].sum()).reset_index(name=str(period)+'_1')\n",
    "#     count[str(period)+'_1'] = count1[str(period)+'_1']\n",
    "#     count.fillna(value=0, inplace=True)\n",
    "#     count[str(period)+'_rate'] = round(count[str(period)+'_1'] / count[str(period)+'_all'], 6)\n",
    "#     count['period'] = period\n",
    "#     count.drop([str(period)+'_all', str(period)+'_1'], axis=1, inplace=True)\n",
    "#     count.fillna(value=0, inplace=True)\n",
    "#     res = res.append(count, ignore_index=True)\n",
    "# data = pd.merge(data, res, how='left', on=['start_address','end_address', 'period'])\n",
    "# print(data.shape)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['period'] = data['weekd'].astype(int)\n",
    "# for feat_1 in addfea:\n",
    "#     res = pd.DataFrame()\n",
    "#     temp = data[data['cnt']!=0][[feat_1, 'period', 'cnt']]\n",
    "#     for period in range(0, 7):\n",
    "#         if period == 0:\n",
    "#             count = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].count()).reset_index(name=feat_1 + '_all')\n",
    "#             count1 = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].sum()).reset_index(name=feat_1 + '_1')\n",
    "#         else:\n",
    "#             count = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].count()).reset_index(name=feat_1 + '_all')\n",
    "#             count1 = temp.groupby([feat_1]).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].sum()).reset_index(name=feat_1 + '_1')\n",
    "#         count[feat_1 + '_1'] = count1[feat_1 + '_1']\n",
    "#         count.fillna(value=0, inplace=True)\n",
    "#         count[feat_1 + '_rate'] = round(count[feat_1 + '_1'] / count[feat_1 + '_all'], 6)\n",
    "#         count['period'] = period\n",
    "#         count.drop([feat_1 + '_all', feat_1 + '_1'], axis=1, inplace=True)\n",
    "#         count.fillna(value=0, inplace=True)\n",
    "#         res = res.append(count, ignore_index=True)\n",
    "#     print(feat_1, ' over')\n",
    "#     data = pd.merge(data, res, how='left', on=[feat_1, 'period'])\n",
    "# print(data.shape)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['period'] = data['weekd'].astype(int)\n",
    "# feat_1=['start_address','end_address']\n",
    "# res = pd.DataFrame()\n",
    "# temp = data[data['cnt']!=0][['start_address','end_address','period', 'cnt']]\n",
    "# for period in range(0, 7):\n",
    "#     if period == 0:\n",
    "#         count = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].count()).reset_index(name=str(period)+'_all')\n",
    "#         count1 = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] <= period).values].sum()).reset_index(name=str(period)+'_1')\n",
    "#     else:\n",
    "#         count = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].count()).reset_index(name=str(period)+'_all')\n",
    "#         count1 = temp.groupby(feat_1).apply(\n",
    "#                 lambda x: x['cnt'][(x['period'] < period).values].sum()).reset_index(name=str(period)+'_1')\n",
    "#     count[str(period)+'_1'] = count1[str(period)+'_1']\n",
    "#     count.fillna(value=0, inplace=True)\n",
    "#     count[str(period)+'_rate'] = round(count[str(period)+'_1'] / count[str(period)+'_all'], 6)\n",
    "#     count['period'] = period\n",
    "#     count.drop([str(period)+'_all', str(period)+'_1'], axis=1, inplace=True)\n",
    "#     count.fillna(value=0, inplace=True)\n",
    "#     res = res.append(count, ignore_index=True)\n",
    "# data = pd.merge(data, res, how='left', on=['start_address','end_address', 'period'])\n",
    "# print(data.shape)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========数据本地保存========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======按照出发城市进行分集存储后提取训练==============\n",
    "\n",
    "#按照出发城市进行分集存储\n",
    "def save_data(path,data):\n",
    "    for i in range(0,7):\n",
    "        ndata=data[data['o_city_code']==i]\n",
    "        ndata.to_csv(path+'data_'+str(i)+'.csv',index=False)\n",
    "        \n",
    "path='../data/'\n",
    "save_data(path,data)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用来调最优参数的自定义函数\n",
    "def get_best_lgb(path,i,N):\n",
    "    data=pd.read_csv(path+'data_'+str(i)+'.csv')\n",
    "    traindf=data[data['month']!=15]\n",
    "    testdf=data[data['month']==15]\n",
    "    print(traindf.shape,testdf.shape)\n",
    "    \n",
    "    train_xy,off_xy=train_test_split(traindf,test_size=0.1,random_state=1)\n",
    "    tr,val=train_test_split(train_xy,test_size=0.15,random_state=1)\n",
    "    \n",
    "    drop_list=['cnt','o_city_code','o_district_code','date_dt','month']\n",
    "    y_train=tr.cnt\n",
    "    x_train=tr.drop(drop_list,axis=1)\n",
    "    \n",
    "    y_val=val.cnt\n",
    "    x_val=val.drop(drop_list,axis=1)\n",
    "    \n",
    "    y_off=off_xy.cnt\n",
    "    x_off=off_xy.drop(drop_list,axis=1)\n",
    "    \n",
    "    test_x=testdf.drop(drop_list,axis=1)\n",
    "    \n",
    "    gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=70,\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=N)\n",
    "\n",
    "    gbm.fit(x_train, y_train,\n",
    "        eval_set=[(x_train,y_train),(x_val,y_val)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=100)\n",
    "\n",
    "    y_pred = gbm.predict(x_val, num_iteration=gbm.best_iteration_)\n",
    "    y_eval=rmsle(y_val,y_pred)[1]\n",
    "    print('The rmlse of prediction is:',y_eval)\n",
    "    print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "    y_pred_off=gbm.predict(x_off,num_iteration=gbm.best_iteration_)\n",
    "    y_off_eval=rmsle(y_off, y_pred_off)[1]\n",
    "    print('The rmlse of prediction is:',y_off_eval)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb(path,i,N):\n",
    "    data=pd.read_csv(path+'data_'+str(i)+'.csv')\n",
    "    traindf=data[data['month']!=15]\n",
    "    testdf=data[data['month']==15]\n",
    "    print(traindf.shape,testdf.shape)\n",
    "    \n",
    "    train_xy,off_xy=train_test_split(traindf,test_size=0.1,random_state=1)\n",
    "    tr,val=train_test_split(train_xy,test_size=0.15,random_state=1)\n",
    "    \n",
    "    drop_list=['cnt','o_city_code','date_dt']\n",
    "    \n",
    "    #用于后面的数据融合\n",
    "    y_traindf=traindf.cnt\n",
    "    x_traindf=traindf.drop(drop_list,axis=1)\n",
    "    \n",
    "    #将traindf分成训练和线下验证二种，训练集又分成训练和验证二种\n",
    "    y_train=tr.cnt\n",
    "    x_train=tr.drop(drop_list,axis=1)\n",
    "    \n",
    "    y_val=val.cnt\n",
    "    x_val=val.drop(drop_list,axis=1)\n",
    "    \n",
    "    y_off=off_xy.cnt\n",
    "    x_off=off_xy.drop(drop_list,axis=1)\n",
    "    \n",
    "    test_x=testdf.drop(drop_list,axis=1)\n",
    "    \n",
    "    gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=50,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=N)\n",
    "\n",
    "    gbm.fit(x_train, y_train,\n",
    "        eval_set=[(x_train,y_train),(x_val,y_val)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=50)\n",
    "\n",
    "    y_pred = gbm.predict(x_val, num_iteration=gbm.best_iteration_)\n",
    "    y_eval=rmsle(y_val,y_pred)[1]\n",
    "    # print('The rmse of prediction is:', mean_squared_error(y_val, y_pred) ** 0.5)\n",
    "    print('The rmlse of prediction is:',y_eval)\n",
    "    # print('Feature importances:', list(gbm.feature_importances_))\n",
    "\n",
    "    y_pred_off=gbm.predict(x_off,num_iteration=gbm.best_iteration_)\n",
    "    y_off_eval=rmsle(y_off, y_pred_off)[1]\n",
    "    # print('The rmse of prediction is:', mean_squared_error(y_off, y_pred_off) ** 0.5)\n",
    "    print('The rmlse of prediction is:',y_off_eval)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    traindf['cnt_p']=gbm.predict(x_traindf,num_iteration=gbm.best_iteration_)\n",
    "    testdf['cnt_p']=gbm.predict(test_x,num_iteration=gbm.best_iteration_)\n",
    "    \n",
    "    datadf=pd.concat([traindf,testdf],axis=0)\n",
    "    datadf[['date_dt','start_address','end_address','cnt','cnt_p']].to_csv(path+'dataf_'+str(i)+'.csv',index=False)\n",
    "    \n",
    "    return y_eval,y_off_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273015, 36) (16980, 36)\n",
      "[1]\ttraining's l1: 1.11249\tvalid_1's l1: 1.14016\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttraining's l1: 1.05753\tvalid_1's l1: 1.0837\n",
      "[3]\ttraining's l1: 1.00538\tvalid_1's l1: 1.03005\n",
      "[4]\ttraining's l1: 0.95589\tvalid_1's l1: 0.97916\n",
      "[5]\ttraining's l1: 0.908918\tvalid_1's l1: 0.9308\n",
      "[6]\ttraining's l1: 0.864299\tvalid_1's l1: 0.884972\n",
      "[7]\ttraining's l1: 0.821913\tvalid_1's l1: 0.841365\n",
      "[8]\ttraining's l1: 0.781668\tvalid_1's l1: 0.800018\n",
      "[9]\ttraining's l1: 0.74348\tvalid_1's l1: 0.76075\n",
      "[10]\ttraining's l1: 0.707203\tvalid_1's l1: 0.723458\n",
      "[11]\ttraining's l1: 0.672789\tvalid_1's l1: 0.688095\n",
      "[12]\ttraining's l1: 0.640098\tvalid_1's l1: 0.654498\n",
      "[13]\ttraining's l1: 0.609091\tvalid_1's l1: 0.622677\n",
      "[14]\ttraining's l1: 0.579694\tvalid_1's l1: 0.592482\n",
      "[15]\ttraining's l1: 0.551782\tvalid_1's l1: 0.563794\n",
      "[16]\ttraining's l1: 0.525367\tvalid_1's l1: 0.536614\n",
      "[17]\ttraining's l1: 0.500339\tvalid_1's l1: 0.5109\n",
      "[18]\ttraining's l1: 0.476615\tvalid_1's l1: 0.486602\n",
      "[19]\ttraining's l1: 0.454156\tvalid_1's l1: 0.463639\n",
      "[20]\ttraining's l1: 0.432873\tvalid_1's l1: 0.441912\n",
      "[21]\ttraining's l1: 0.412761\tvalid_1's l1: 0.421383\n",
      "[22]\ttraining's l1: 0.393746\tvalid_1's l1: 0.401981\n",
      "[23]\ttraining's l1: 0.375744\tvalid_1's l1: 0.383639\n",
      "[24]\ttraining's l1: 0.358746\tvalid_1's l1: 0.366322\n",
      "[25]\ttraining's l1: 0.342658\tvalid_1's l1: 0.349923\n",
      "[26]\ttraining's l1: 0.32746\tvalid_1's l1: 0.334477\n",
      "[27]\ttraining's l1: 0.313089\tvalid_1's l1: 0.319866\n",
      "[28]\ttraining's l1: 0.299478\tvalid_1's l1: 0.305998\n",
      "[29]\ttraining's l1: 0.286615\tvalid_1's l1: 0.292914\n",
      "[30]\ttraining's l1: 0.274424\tvalid_1's l1: 0.280556\n",
      "[31]\ttraining's l1: 0.262932\tvalid_1's l1: 0.268882\n",
      "[32]\ttraining's l1: 0.252025\tvalid_1's l1: 0.257849\n",
      "[33]\ttraining's l1: 0.241756\tvalid_1's l1: 0.247449\n",
      "[34]\ttraining's l1: 0.232008\tvalid_1's l1: 0.237638\n",
      "[35]\ttraining's l1: 0.222805\tvalid_1's l1: 0.228412\n",
      "[36]\ttraining's l1: 0.214109\tvalid_1's l1: 0.219738\n",
      "[37]\ttraining's l1: 0.205864\tvalid_1's l1: 0.211499\n",
      "[38]\ttraining's l1: 0.198076\tvalid_1's l1: 0.203711\n",
      "[39]\ttraining's l1: 0.190717\tvalid_1's l1: 0.196361\n",
      "[40]\ttraining's l1: 0.183795\tvalid_1's l1: 0.189456\n",
      "[41]\ttraining's l1: 0.177247\tvalid_1's l1: 0.182946\n",
      "[42]\ttraining's l1: 0.17106\tvalid_1's l1: 0.176834\n",
      "[43]\ttraining's l1: 0.16521\tvalid_1's l1: 0.171091\n",
      "[44]\ttraining's l1: 0.159677\tvalid_1's l1: 0.165661\n",
      "[45]\ttraining's l1: 0.15448\tvalid_1's l1: 0.16053\n",
      "[46]\ttraining's l1: 0.149564\tvalid_1's l1: 0.155698\n",
      "[47]\ttraining's l1: 0.144917\tvalid_1's l1: 0.151156\n",
      "[48]\ttraining's l1: 0.140527\tvalid_1's l1: 0.14687\n",
      "[49]\ttraining's l1: 0.136412\tvalid_1's l1: 0.142847\n",
      "[50]\ttraining's l1: 0.132498\tvalid_1's l1: 0.139038\n",
      "[51]\ttraining's l1: 0.128827\tvalid_1's l1: 0.135448\n",
      "[52]\ttraining's l1: 0.125345\tvalid_1's l1: 0.132061\n",
      "[53]\ttraining's l1: 0.122058\tvalid_1's l1: 0.128884\n",
      "[54]\ttraining's l1: 0.118959\tvalid_1's l1: 0.125867\n",
      "[55]\ttraining's l1: 0.116033\tvalid_1's l1: 0.12307\n",
      "[56]\ttraining's l1: 0.113293\tvalid_1's l1: 0.120425\n",
      "[57]\ttraining's l1: 0.110735\tvalid_1's l1: 0.117955\n",
      "[58]\ttraining's l1: 0.108318\tvalid_1's l1: 0.115633\n",
      "[59]\ttraining's l1: 0.106088\tvalid_1's l1: 0.113489\n",
      "[60]\ttraining's l1: 0.103958\tvalid_1's l1: 0.111429\n",
      "[61]\ttraining's l1: 0.101971\tvalid_1's l1: 0.109517\n",
      "[62]\ttraining's l1: 0.100086\tvalid_1's l1: 0.107717\n",
      "[63]\ttraining's l1: 0.0983064\tvalid_1's l1: 0.106011\n",
      "[64]\ttraining's l1: 0.0966877\tvalid_1's l1: 0.104463\n",
      "[65]\ttraining's l1: 0.0951473\tvalid_1's l1: 0.102998\n",
      "[66]\ttraining's l1: 0.0937235\tvalid_1's l1: 0.101661\n",
      "[67]\ttraining's l1: 0.0923745\tvalid_1's l1: 0.10041\n",
      "[68]\ttraining's l1: 0.0910887\tvalid_1's l1: 0.0991973\n",
      "[69]\ttraining's l1: 0.0899132\tvalid_1's l1: 0.0981011\n",
      "[70]\ttraining's l1: 0.0888168\tvalid_1's l1: 0.0970773\n",
      "[71]\ttraining's l1: 0.0877997\tvalid_1's l1: 0.0961537\n",
      "[72]\ttraining's l1: 0.0868541\tvalid_1's l1: 0.0952642\n",
      "[73]\ttraining's l1: 0.0859579\tvalid_1's l1: 0.0944389\n",
      "[74]\ttraining's l1: 0.0851189\tvalid_1's l1: 0.0936801\n",
      "[75]\ttraining's l1: 0.0843211\tvalid_1's l1: 0.0929718\n",
      "[76]\ttraining's l1: 0.0835911\tvalid_1's l1: 0.0923314\n",
      "[77]\ttraining's l1: 0.082922\tvalid_1's l1: 0.0917443\n",
      "[78]\ttraining's l1: 0.0822552\tvalid_1's l1: 0.0911776\n",
      "[79]\ttraining's l1: 0.0816295\tvalid_1's l1: 0.0906495\n",
      "[80]\ttraining's l1: 0.0811075\tvalid_1's l1: 0.0902332\n",
      "[81]\ttraining's l1: 0.0805639\tvalid_1's l1: 0.0897602\n",
      "[82]\ttraining's l1: 0.0801049\tvalid_1's l1: 0.0893758\n",
      "[83]\ttraining's l1: 0.0796238\tvalid_1's l1: 0.0889695\n",
      "[84]\ttraining's l1: 0.079157\tvalid_1's l1: 0.0885964\n",
      "[85]\ttraining's l1: 0.0787827\tvalid_1's l1: 0.0883092\n",
      "[86]\ttraining's l1: 0.0783963\tvalid_1's l1: 0.0879886\n",
      "[87]\ttraining's l1: 0.0780607\tvalid_1's l1: 0.087745\n",
      "[88]\ttraining's l1: 0.077747\tvalid_1's l1: 0.0875317\n",
      "[89]\ttraining's l1: 0.0774059\tvalid_1's l1: 0.0872683\n",
      "[90]\ttraining's l1: 0.0771266\tvalid_1's l1: 0.0870948\n",
      "[91]\ttraining's l1: 0.0768719\tvalid_1's l1: 0.0869188\n",
      "[92]\ttraining's l1: 0.0766026\tvalid_1's l1: 0.0867176\n",
      "[93]\ttraining's l1: 0.0763228\tvalid_1's l1: 0.0865379\n",
      "[94]\ttraining's l1: 0.0760927\tvalid_1's l1: 0.0863676\n",
      "[95]\ttraining's l1: 0.0758607\tvalid_1's l1: 0.0862145\n",
      "[96]\ttraining's l1: 0.0756463\tvalid_1's l1: 0.0860795\n",
      "[97]\ttraining's l1: 0.0754292\tvalid_1's l1: 0.0859579\n",
      "[98]\ttraining's l1: 0.0752276\tvalid_1's l1: 0.0858307\n",
      "[99]\ttraining's l1: 0.0750036\tvalid_1's l1: 0.0856747\n",
      "[100]\ttraining's l1: 0.0748648\tvalid_1's l1: 0.0856237\n",
      "[101]\ttraining's l1: 0.0746813\tvalid_1's l1: 0.085513\n",
      "[102]\ttraining's l1: 0.074487\tvalid_1's l1: 0.0853881\n",
      "[103]\ttraining's l1: 0.0743265\tvalid_1's l1: 0.0852977\n",
      "[104]\ttraining's l1: 0.074223\tvalid_1's l1: 0.085244\n",
      "[105]\ttraining's l1: 0.0741007\tvalid_1's l1: 0.0851789\n",
      "[106]\ttraining's l1: 0.074\tvalid_1's l1: 0.0851351\n",
      "[107]\ttraining's l1: 0.073899\tvalid_1's l1: 0.0851122\n",
      "[108]\ttraining's l1: 0.073756\tvalid_1's l1: 0.0850264\n",
      "[109]\ttraining's l1: 0.0736688\tvalid_1's l1: 0.0850056\n",
      "[110]\ttraining's l1: 0.0735247\tvalid_1's l1: 0.0849071\n",
      "[111]\ttraining's l1: 0.0734484\tvalid_1's l1: 0.0849065\n",
      "[112]\ttraining's l1: 0.0733629\tvalid_1's l1: 0.0848759\n",
      "[113]\ttraining's l1: 0.0733002\tvalid_1's l1: 0.0849089\n",
      "[114]\ttraining's l1: 0.0732061\tvalid_1's l1: 0.0848699\n",
      "[115]\ttraining's l1: 0.0730763\tvalid_1's l1: 0.0848072\n",
      "[116]\ttraining's l1: 0.0730189\tvalid_1's l1: 0.084836\n",
      "[117]\ttraining's l1: 0.0728977\tvalid_1's l1: 0.0847949\n",
      "[118]\ttraining's l1: 0.0728435\tvalid_1's l1: 0.0848453\n",
      "[119]\ttraining's l1: 0.072781\tvalid_1's l1: 0.0848483\n",
      "[120]\ttraining's l1: 0.0726751\tvalid_1's l1: 0.0848056\n",
      "[121]\ttraining's l1: 0.0726138\tvalid_1's l1: 0.084803\n",
      "[122]\ttraining's l1: 0.0725117\tvalid_1's l1: 0.0847642\n",
      "[123]\ttraining's l1: 0.0724688\tvalid_1's l1: 0.0848094\n",
      "[124]\ttraining's l1: 0.0724262\tvalid_1's l1: 0.084833\n",
      "[125]\ttraining's l1: 0.0723901\tvalid_1's l1: 0.084841\n",
      "[126]\ttraining's l1: 0.07229\tvalid_1's l1: 0.0847992\n",
      "[127]\ttraining's l1: 0.072242\tvalid_1's l1: 0.0848258\n",
      "[128]\ttraining's l1: 0.0722061\tvalid_1's l1: 0.0848783\n",
      "[129]\ttraining's l1: 0.0721356\tvalid_1's l1: 0.0848681\n",
      "[130]\ttraining's l1: 0.0720543\tvalid_1's l1: 0.0848468\n",
      "[131]\ttraining's l1: 0.0719726\tvalid_1's l1: 0.0848427\n",
      "[132]\ttraining's l1: 0.071904\tvalid_1's l1: 0.0848143\n",
      "[133]\ttraining's l1: 0.071839\tvalid_1's l1: 0.0847949\n",
      "[134]\ttraining's l1: 0.0718096\tvalid_1's l1: 0.0848428\n",
      "[135]\ttraining's l1: 0.0717596\tvalid_1's l1: 0.0848509\n",
      "[136]\ttraining's l1: 0.0717325\tvalid_1's l1: 0.0849129\n",
      "[137]\ttraining's l1: 0.0716851\tvalid_1's l1: 0.084912\n",
      "[138]\ttraining's l1: 0.0716199\tvalid_1's l1: 0.0848879\n",
      "[139]\ttraining's l1: 0.0715744\tvalid_1's l1: 0.0848865\n",
      "[140]\ttraining's l1: 0.0715044\tvalid_1's l1: 0.0848749\n",
      "[141]\ttraining's l1: 0.0714409\tvalid_1's l1: 0.0848777\n",
      "[142]\ttraining's l1: 0.0713605\tvalid_1's l1: 0.0848565\n",
      "[143]\ttraining's l1: 0.0713042\tvalid_1's l1: 0.0848394\n",
      "[144]\ttraining's l1: 0.0712267\tvalid_1's l1: 0.0848064\n",
      "[145]\ttraining's l1: 0.0711699\tvalid_1's l1: 0.0847993\n",
      "[146]\ttraining's l1: 0.0710952\tvalid_1's l1: 0.0847867\n",
      "[147]\ttraining's l1: 0.0710654\tvalid_1's l1: 0.0848253\n",
      "[148]\ttraining's l1: 0.0710044\tvalid_1's l1: 0.0848046\n",
      "[149]\ttraining's l1: 0.0709482\tvalid_1's l1: 0.0847961\n",
      "[150]\ttraining's l1: 0.0708951\tvalid_1's l1: 0.0847947\n",
      "[151]\ttraining's l1: 0.07083\tvalid_1's l1: 0.0847623\n",
      "[152]\ttraining's l1: 0.0707871\tvalid_1's l1: 0.0847591\n",
      "[153]\ttraining's l1: 0.0707415\tvalid_1's l1: 0.0847879\n",
      "[154]\ttraining's l1: 0.0706952\tvalid_1's l1: 0.0847969\n",
      "[155]\ttraining's l1: 0.0706415\tvalid_1's l1: 0.0848089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156]\ttraining's l1: 0.0705996\tvalid_1's l1: 0.0848102\n",
      "[157]\ttraining's l1: 0.0705508\tvalid_1's l1: 0.0848075\n",
      "[158]\ttraining's l1: 0.0705178\tvalid_1's l1: 0.0848279\n",
      "[159]\ttraining's l1: 0.0704832\tvalid_1's l1: 0.0848538\n",
      "[160]\ttraining's l1: 0.0704374\tvalid_1's l1: 0.0848532\n",
      "[161]\ttraining's l1: 0.0704068\tvalid_1's l1: 0.0848767\n",
      "[162]\ttraining's l1: 0.0703517\tvalid_1's l1: 0.0848617\n",
      "[163]\ttraining's l1: 0.0703177\tvalid_1's l1: 0.0849019\n",
      "[164]\ttraining's l1: 0.0702922\tvalid_1's l1: 0.0849368\n",
      "[165]\ttraining's l1: 0.0702556\tvalid_1's l1: 0.084938\n",
      "[166]\ttraining's l1: 0.0702355\tvalid_1's l1: 0.084977\n",
      "[167]\ttraining's l1: 0.0701974\tvalid_1's l1: 0.0849779\n",
      "[168]\ttraining's l1: 0.0701664\tvalid_1's l1: 0.0849876\n",
      "[169]\ttraining's l1: 0.0701399\tvalid_1's l1: 0.0850084\n",
      "[170]\ttraining's l1: 0.0701145\tvalid_1's l1: 0.0850171\n",
      "[171]\ttraining's l1: 0.070092\tvalid_1's l1: 0.0850367\n",
      "[172]\ttraining's l1: 0.0700661\tvalid_1's l1: 0.0850511\n",
      "[173]\ttraining's l1: 0.0700303\tvalid_1's l1: 0.0850612\n",
      "[174]\ttraining's l1: 0.0699969\tvalid_1's l1: 0.085063\n",
      "[175]\ttraining's l1: 0.0699575\tvalid_1's l1: 0.0850599\n",
      "[176]\ttraining's l1: 0.0699372\tvalid_1's l1: 0.0850761\n",
      "[177]\ttraining's l1: 0.0699181\tvalid_1's l1: 0.085099\n",
      "[178]\ttraining's l1: 0.069898\tvalid_1's l1: 0.0851242\n",
      "[179]\ttraining's l1: 0.0698782\tvalid_1's l1: 0.0851313\n",
      "[180]\ttraining's l1: 0.0698593\tvalid_1's l1: 0.0851459\n",
      "[181]\ttraining's l1: 0.0698359\tvalid_1's l1: 0.0851702\n",
      "[182]\ttraining's l1: 0.0698054\tvalid_1's l1: 0.0851761\n",
      "[183]\ttraining's l1: 0.0697863\tvalid_1's l1: 0.0851948\n",
      "[184]\ttraining's l1: 0.069766\tvalid_1's l1: 0.0852079\n",
      "[185]\ttraining's l1: 0.0697336\tvalid_1's l1: 0.0852065\n",
      "[186]\ttraining's l1: 0.0697136\tvalid_1's l1: 0.0852241\n",
      "[187]\ttraining's l1: 0.0696893\tvalid_1's l1: 0.0852435\n",
      "[188]\ttraining's l1: 0.0696657\tvalid_1's l1: 0.0852801\n",
      "[189]\ttraining's l1: 0.0696488\tvalid_1's l1: 0.0853008\n",
      "[190]\ttraining's l1: 0.0696306\tvalid_1's l1: 0.0853108\n",
      "[191]\ttraining's l1: 0.0696037\tvalid_1's l1: 0.0853138\n",
      "[192]\ttraining's l1: 0.0695858\tvalid_1's l1: 0.0853298\n",
      "[193]\ttraining's l1: 0.069566\tvalid_1's l1: 0.0853469\n",
      "[194]\ttraining's l1: 0.0695381\tvalid_1's l1: 0.0853461\n",
      "[195]\ttraining's l1: 0.0695183\tvalid_1's l1: 0.08539\n",
      "[196]\ttraining's l1: 0.0694988\tvalid_1's l1: 0.0854084\n",
      "[197]\ttraining's l1: 0.0694805\tvalid_1's l1: 0.0854295\n",
      "[198]\ttraining's l1: 0.06946\tvalid_1's l1: 0.0854546\n",
      "[199]\ttraining's l1: 0.0694394\tvalid_1's l1: 0.0854672\n",
      "[200]\ttraining's l1: 0.0694211\tvalid_1's l1: 0.0854987\n",
      "[201]\ttraining's l1: 0.0693939\tvalid_1's l1: 0.0855027\n",
      "[202]\ttraining's l1: 0.069378\tvalid_1's l1: 0.0855172\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's l1: 0.0707871\tvalid_1's l1: 0.0847591\n",
      "The rmlse of prediction is: 0.06634\n",
      "Feature importances: [5, 69, 293, 11, 40, 0, 95, 212, 114, 105, 34, 13, 6, 12, 11, 438, 737, 383, 0, 218, 193, 185, 177, 109, 176, 77, 125, 751, 352, 654, 302, 1551]\n",
      "The rmlse of prediction is: 0.0625\n"
     ]
    }
   ],
   "source": [
    "rmsle0=get_best_lgb(path,0,N=942) #it is ok!\n",
    "#     gbm = lgb.LGBMRegressor(objective='regression',\n",
    "#                         num_leaves=50,\n",
    "#                         learning_rate=0.05,\n",
    "#                         n_estimators=N)\n",
    "\n",
    "#     gbm.fit(x_train, y_train,\n",
    "#         eval_set=[(x_train,y_train),(x_val,y_val)],\n",
    "#         eval_metric='l1',\n",
    "#         early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256286, 36) (15500, 36)\n",
      "[1]\ttraining's l1: 3.81284\tvalid_1's l1: 3.7846\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\ttraining's l1: 3.6253\tvalid_1's l1: 3.59818\n",
      "[3]\ttraining's l1: 3.44698\tvalid_1's l1: 3.42083\n",
      "[4]\ttraining's l1: 3.27804\tvalid_1's l1: 3.253\n",
      "[5]\ttraining's l1: 3.11749\tvalid_1's l1: 3.09329\n",
      "[6]\ttraining's l1: 2.96496\tvalid_1's l1: 2.94165\n",
      "[7]\ttraining's l1: 2.82041\tvalid_1's l1: 2.7981\n",
      "[8]\ttraining's l1: 2.68328\tvalid_1's l1: 2.66179\n",
      "[9]\ttraining's l1: 2.55293\tvalid_1's l1: 2.53232\n",
      "[10]\ttraining's l1: 2.4292\tvalid_1's l1: 2.40937\n",
      "[11]\ttraining's l1: 2.31189\tvalid_1's l1: 2.29274\n",
      "[12]\ttraining's l1: 2.20067\tvalid_1's l1: 2.18225\n",
      "[13]\ttraining's l1: 2.09499\tvalid_1's l1: 2.07719\n",
      "[14]\ttraining's l1: 1.99479\tvalid_1's l1: 1.97762\n",
      "[15]\ttraining's l1: 1.89975\tvalid_1's l1: 1.88338\n",
      "[16]\ttraining's l1: 1.80968\tvalid_1's l1: 1.79403\n",
      "[17]\ttraining's l1: 1.72435\tvalid_1's l1: 1.70941\n",
      "[18]\ttraining's l1: 1.64362\tvalid_1's l1: 1.62907\n",
      "[19]\ttraining's l1: 1.56714\tvalid_1's l1: 1.55288\n",
      "[20]\ttraining's l1: 1.49476\tvalid_1's l1: 1.48092\n",
      "[21]\ttraining's l1: 1.42625\tvalid_1's l1: 1.41292\n",
      "[22]\ttraining's l1: 1.36141\tvalid_1's l1: 1.3486\n",
      "[23]\ttraining's l1: 1.30006\tvalid_1's l1: 1.28803\n",
      "[24]\ttraining's l1: 1.24183\tvalid_1's l1: 1.23044\n",
      "[25]\ttraining's l1: 1.1868\tvalid_1's l1: 1.1761\n",
      "[26]\ttraining's l1: 1.13467\tvalid_1's l1: 1.12455\n",
      "[27]\ttraining's l1: 1.08543\tvalid_1's l1: 1.07598\n",
      "[28]\ttraining's l1: 1.03885\tvalid_1's l1: 1.03009\n",
      "[29]\ttraining's l1: 0.994777\tvalid_1's l1: 0.986818\n",
      "[30]\ttraining's l1: 0.953223\tvalid_1's l1: 0.946042\n",
      "[31]\ttraining's l1: 0.913908\tvalid_1's l1: 0.907552\n",
      "[32]\ttraining's l1: 0.876849\tvalid_1's l1: 0.871565\n",
      "[33]\ttraining's l1: 0.842055\tvalid_1's l1: 0.837894\n",
      "[34]\ttraining's l1: 0.809351\tvalid_1's l1: 0.806326\n",
      "[35]\ttraining's l1: 0.778379\tvalid_1's l1: 0.776477\n",
      "[36]\ttraining's l1: 0.749202\tvalid_1's l1: 0.748595\n",
      "[37]\ttraining's l1: 0.721752\tvalid_1's l1: 0.722461\n",
      "[38]\ttraining's l1: 0.695876\tvalid_1's l1: 0.697601\n",
      "[39]\ttraining's l1: 0.671369\tvalid_1's l1: 0.674294\n",
      "[40]\ttraining's l1: 0.648472\tvalid_1's l1: 0.652277\n",
      "[41]\ttraining's l1: 0.62666\tvalid_1's l1: 0.631409\n",
      "[42]\ttraining's l1: 0.606235\tvalid_1's l1: 0.611551\n",
      "[43]\ttraining's l1: 0.586793\tvalid_1's l1: 0.592835\n",
      "[44]\ttraining's l1: 0.568393\tvalid_1's l1: 0.575282\n",
      "[45]\ttraining's l1: 0.551297\tvalid_1's l1: 0.558924\n",
      "[46]\ttraining's l1: 0.534975\tvalid_1's l1: 0.543323\n",
      "[47]\ttraining's l1: 0.51989\tvalid_1's l1: 0.529054\n",
      "[48]\ttraining's l1: 0.505486\tvalid_1's l1: 0.515334\n",
      "[49]\ttraining's l1: 0.49205\tvalid_1's l1: 0.502554\n",
      "[50]\ttraining's l1: 0.479363\tvalid_1's l1: 0.490657\n",
      "[51]\ttraining's l1: 0.467434\tvalid_1's l1: 0.479435\n",
      "[52]\ttraining's l1: 0.456223\tvalid_1's l1: 0.468864\n",
      "[53]\ttraining's l1: 0.445509\tvalid_1's l1: 0.45886\n",
      "[54]\ttraining's l1: 0.435701\tvalid_1's l1: 0.449864\n",
      "[55]\ttraining's l1: 0.426508\tvalid_1's l1: 0.441498\n",
      "[56]\ttraining's l1: 0.41787\tvalid_1's l1: 0.433434\n",
      "[57]\ttraining's l1: 0.4098\tvalid_1's l1: 0.426113\n",
      "[58]\ttraining's l1: 0.402048\tvalid_1's l1: 0.419004\n",
      "[59]\ttraining's l1: 0.395\tvalid_1's l1: 0.412421\n",
      "[60]\ttraining's l1: 0.388331\tvalid_1's l1: 0.406185\n",
      "[61]\ttraining's l1: 0.382115\tvalid_1's l1: 0.400446\n",
      "[62]\ttraining's l1: 0.376234\tvalid_1's l1: 0.394953\n",
      "[63]\ttraining's l1: 0.37084\tvalid_1's l1: 0.389953\n",
      "[64]\ttraining's l1: 0.365692\tvalid_1's l1: 0.385199\n",
      "[65]\ttraining's l1: 0.360931\tvalid_1's l1: 0.380843\n",
      "[66]\ttraining's l1: 0.356424\tvalid_1's l1: 0.37657\n",
      "[67]\ttraining's l1: 0.352275\tvalid_1's l1: 0.372756\n",
      "[68]\ttraining's l1: 0.348436\tvalid_1's l1: 0.369336\n",
      "[69]\ttraining's l1: 0.344851\tvalid_1's l1: 0.365986\n",
      "[70]\ttraining's l1: 0.341481\tvalid_1's l1: 0.362929\n",
      "[71]\ttraining's l1: 0.338181\tvalid_1's l1: 0.360126\n",
      "[72]\ttraining's l1: 0.335091\tvalid_1's l1: 0.357506\n",
      "[73]\ttraining's l1: 0.332254\tvalid_1's l1: 0.355072\n",
      "[74]\ttraining's l1: 0.329541\tvalid_1's l1: 0.352655\n",
      "[75]\ttraining's l1: 0.326953\tvalid_1's l1: 0.350561\n",
      "[76]\ttraining's l1: 0.324552\tvalid_1's l1: 0.348417\n",
      "[77]\ttraining's l1: 0.322311\tvalid_1's l1: 0.346575\n",
      "[78]\ttraining's l1: 0.320284\tvalid_1's l1: 0.344843\n",
      "[79]\ttraining's l1: 0.318431\tvalid_1's l1: 0.343509\n",
      "[80]\ttraining's l1: 0.316601\tvalid_1's l1: 0.342065\n",
      "[81]\ttraining's l1: 0.314883\tvalid_1's l1: 0.340654\n",
      "[82]\ttraining's l1: 0.313456\tvalid_1's l1: 0.339661\n",
      "[83]\ttraining's l1: 0.311861\tvalid_1's l1: 0.338399\n",
      "[84]\ttraining's l1: 0.310295\tvalid_1's l1: 0.33716\n",
      "[85]\ttraining's l1: 0.309057\tvalid_1's l1: 0.33633\n",
      "[86]\ttraining's l1: 0.3079\tvalid_1's l1: 0.335506\n",
      "[87]\ttraining's l1: 0.306878\tvalid_1's l1: 0.334747\n",
      "[88]\ttraining's l1: 0.305897\tvalid_1's l1: 0.334008\n",
      "[89]\ttraining's l1: 0.304742\tvalid_1's l1: 0.333125\n",
      "[90]\ttraining's l1: 0.303848\tvalid_1's l1: 0.332537\n",
      "[91]\ttraining's l1: 0.302823\tvalid_1's l1: 0.331774\n",
      "[92]\ttraining's l1: 0.301794\tvalid_1's l1: 0.331174\n",
      "[93]\ttraining's l1: 0.300847\tvalid_1's l1: 0.330513\n",
      "[94]\ttraining's l1: 0.299904\tvalid_1's l1: 0.329919\n",
      "[95]\ttraining's l1: 0.298963\tvalid_1's l1: 0.329193\n",
      "[96]\ttraining's l1: 0.298238\tvalid_1's l1: 0.328813\n",
      "[97]\ttraining's l1: 0.297557\tvalid_1's l1: 0.328437\n",
      "[98]\ttraining's l1: 0.296717\tvalid_1's l1: 0.327862\n",
      "[99]\ttraining's l1: 0.296046\tvalid_1's l1: 0.327479\n",
      "[100]\ttraining's l1: 0.295538\tvalid_1's l1: 0.327334\n",
      "[101]\ttraining's l1: 0.294865\tvalid_1's l1: 0.326938\n",
      "[102]\ttraining's l1: 0.294475\tvalid_1's l1: 0.326799\n",
      "[103]\ttraining's l1: 0.294128\tvalid_1's l1: 0.326765\n",
      "[104]\ttraining's l1: 0.293786\tvalid_1's l1: 0.326687\n",
      "[105]\ttraining's l1: 0.293407\tvalid_1's l1: 0.326568\n",
      "[106]\ttraining's l1: 0.29279\tvalid_1's l1: 0.326212\n",
      "[107]\ttraining's l1: 0.292487\tvalid_1's l1: 0.326135\n",
      "[108]\ttraining's l1: 0.292084\tvalid_1's l1: 0.326086\n",
      "[109]\ttraining's l1: 0.291484\tvalid_1's l1: 0.32568\n",
      "[110]\ttraining's l1: 0.290963\tvalid_1's l1: 0.325315\n",
      "[111]\ttraining's l1: 0.290501\tvalid_1's l1: 0.325098\n",
      "[112]\ttraining's l1: 0.290269\tvalid_1's l1: 0.325201\n",
      "[113]\ttraining's l1: 0.289677\tvalid_1's l1: 0.324846\n",
      "[114]\ttraining's l1: 0.289182\tvalid_1's l1: 0.324549\n",
      "[115]\ttraining's l1: 0.288925\tvalid_1's l1: 0.32461\n",
      "[116]\ttraining's l1: 0.288713\tvalid_1's l1: 0.32459\n",
      "[117]\ttraining's l1: 0.288526\tvalid_1's l1: 0.32463\n",
      "[118]\ttraining's l1: 0.288341\tvalid_1's l1: 0.324728\n",
      "[119]\ttraining's l1: 0.287937\tvalid_1's l1: 0.324545\n",
      "[120]\ttraining's l1: 0.287571\tvalid_1's l1: 0.324377\n",
      "[121]\ttraining's l1: 0.287389\tvalid_1's l1: 0.324498\n",
      "[122]\ttraining's l1: 0.287172\tvalid_1's l1: 0.324488\n",
      "[123]\ttraining's l1: 0.286794\tvalid_1's l1: 0.324338\n",
      "[124]\ttraining's l1: 0.286563\tvalid_1's l1: 0.324377\n",
      "[125]\ttraining's l1: 0.286354\tvalid_1's l1: 0.324421\n",
      "[126]\ttraining's l1: 0.286141\tvalid_1's l1: 0.32449\n",
      "[127]\ttraining's l1: 0.285981\tvalid_1's l1: 0.324634\n",
      "[128]\ttraining's l1: 0.285747\tvalid_1's l1: 0.3246\n",
      "[129]\ttraining's l1: 0.285528\tvalid_1's l1: 0.324607\n",
      "[130]\ttraining's l1: 0.285265\tvalid_1's l1: 0.324549\n",
      "[131]\ttraining's l1: 0.28496\tvalid_1's l1: 0.324455\n",
      "[132]\ttraining's l1: 0.284764\tvalid_1's l1: 0.324423\n",
      "[133]\ttraining's l1: 0.284458\tvalid_1's l1: 0.324333\n",
      "[134]\ttraining's l1: 0.284132\tvalid_1's l1: 0.324173\n",
      "[135]\ttraining's l1: 0.28384\tvalid_1's l1: 0.324037\n",
      "[136]\ttraining's l1: 0.283594\tvalid_1's l1: 0.32396\n",
      "[137]\ttraining's l1: 0.28346\tvalid_1's l1: 0.324039\n",
      "[138]\ttraining's l1: 0.283104\tvalid_1's l1: 0.323877\n",
      "[139]\ttraining's l1: 0.282791\tvalid_1's l1: 0.323784\n",
      "[140]\ttraining's l1: 0.282492\tvalid_1's l1: 0.323709\n",
      "[141]\ttraining's l1: 0.282395\tvalid_1's l1: 0.323885\n",
      "[142]\ttraining's l1: 0.282073\tvalid_1's l1: 0.323756\n",
      "[143]\ttraining's l1: 0.281775\tvalid_1's l1: 0.323681\n",
      "[144]\ttraining's l1: 0.28153\tvalid_1's l1: 0.323602\n",
      "[145]\ttraining's l1: 0.281215\tvalid_1's l1: 0.32342\n",
      "[146]\ttraining's l1: 0.281005\tvalid_1's l1: 0.323471\n",
      "[147]\ttraining's l1: 0.280806\tvalid_1's l1: 0.323469\n",
      "[148]\ttraining's l1: 0.280573\tvalid_1's l1: 0.323439\n",
      "[149]\ttraining's l1: 0.280321\tvalid_1's l1: 0.323444\n",
      "[150]\ttraining's l1: 0.280107\tvalid_1's l1: 0.323435\n",
      "[151]\ttraining's l1: 0.279951\tvalid_1's l1: 0.323476\n",
      "[152]\ttraining's l1: 0.279758\tvalid_1's l1: 0.323456\n",
      "[153]\ttraining's l1: 0.279492\tvalid_1's l1: 0.323374\n",
      "[154]\ttraining's l1: 0.279291\tvalid_1's l1: 0.323334\n",
      "[155]\ttraining's l1: 0.279101\tvalid_1's l1: 0.323329\n",
      "[156]\ttraining's l1: 0.278967\tvalid_1's l1: 0.323379\n",
      "[157]\ttraining's l1: 0.278806\tvalid_1's l1: 0.32328\n",
      "[158]\ttraining's l1: 0.278635\tvalid_1's l1: 0.323291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159]\ttraining's l1: 0.278473\tvalid_1's l1: 0.323269\n",
      "[160]\ttraining's l1: 0.278295\tvalid_1's l1: 0.323222\n",
      "[161]\ttraining's l1: 0.278136\tvalid_1's l1: 0.323193\n",
      "[162]\ttraining's l1: 0.277952\tvalid_1's l1: 0.323178\n",
      "[163]\ttraining's l1: 0.277849\tvalid_1's l1: 0.323224\n",
      "[164]\ttraining's l1: 0.277732\tvalid_1's l1: 0.323303\n",
      "[165]\ttraining's l1: 0.277548\tvalid_1's l1: 0.32322\n",
      "[166]\ttraining's l1: 0.277352\tvalid_1's l1: 0.323212\n",
      "[167]\ttraining's l1: 0.277202\tvalid_1's l1: 0.323187\n",
      "[168]\ttraining's l1: 0.27709\tvalid_1's l1: 0.323196\n",
      "[169]\ttraining's l1: 0.276957\tvalid_1's l1: 0.323179\n",
      "[170]\ttraining's l1: 0.276875\tvalid_1's l1: 0.32325\n",
      "[171]\ttraining's l1: 0.276734\tvalid_1's l1: 0.323296\n",
      "[172]\ttraining's l1: 0.276606\tvalid_1's l1: 0.323255\n",
      "[173]\ttraining's l1: 0.276473\tvalid_1's l1: 0.323236\n",
      "[174]\ttraining's l1: 0.276374\tvalid_1's l1: 0.323276\n",
      "[175]\ttraining's l1: 0.276286\tvalid_1's l1: 0.323306\n",
      "[176]\ttraining's l1: 0.276184\tvalid_1's l1: 0.323357\n",
      "[177]\ttraining's l1: 0.276027\tvalid_1's l1: 0.323365\n",
      "[178]\ttraining's l1: 0.275944\tvalid_1's l1: 0.323346\n",
      "[179]\ttraining's l1: 0.275847\tvalid_1's l1: 0.323394\n",
      "[180]\ttraining's l1: 0.275754\tvalid_1's l1: 0.323445\n",
      "[181]\ttraining's l1: 0.275671\tvalid_1's l1: 0.323515\n",
      "[182]\ttraining's l1: 0.275513\tvalid_1's l1: 0.323438\n",
      "[183]\ttraining's l1: 0.275432\tvalid_1's l1: 0.323504\n",
      "[184]\ttraining's l1: 0.275306\tvalid_1's l1: 0.323552\n",
      "[185]\ttraining's l1: 0.275162\tvalid_1's l1: 0.323578\n",
      "[186]\ttraining's l1: 0.275063\tvalid_1's l1: 0.323622\n",
      "[187]\ttraining's l1: 0.274958\tvalid_1's l1: 0.323716\n",
      "[188]\ttraining's l1: 0.274876\tvalid_1's l1: 0.323797\n",
      "[189]\ttraining's l1: 0.274776\tvalid_1's l1: 0.323847\n",
      "[190]\ttraining's l1: 0.274707\tvalid_1's l1: 0.323915\n",
      "[191]\ttraining's l1: 0.274576\tvalid_1's l1: 0.323872\n",
      "[192]\ttraining's l1: 0.274497\tvalid_1's l1: 0.323888\n",
      "[193]\ttraining's l1: 0.274419\tvalid_1's l1: 0.323992\n",
      "[194]\ttraining's l1: 0.27434\tvalid_1's l1: 0.324063\n",
      "[195]\ttraining's l1: 0.274257\tvalid_1's l1: 0.324136\n",
      "[196]\ttraining's l1: 0.274151\tvalid_1's l1: 0.324129\n",
      "[197]\ttraining's l1: 0.274044\tvalid_1's l1: 0.324113\n",
      "[198]\ttraining's l1: 0.273973\tvalid_1's l1: 0.324128\n",
      "[199]\ttraining's l1: 0.273902\tvalid_1's l1: 0.324155\n",
      "[200]\ttraining's l1: 0.27378\tvalid_1's l1: 0.324142\n",
      "[201]\ttraining's l1: 0.273693\tvalid_1's l1: 0.324154\n",
      "[202]\ttraining's l1: 0.273594\tvalid_1's l1: 0.324197\n",
      "[203]\ttraining's l1: 0.273512\tvalid_1's l1: 0.3243\n",
      "[204]\ttraining's l1: 0.273457\tvalid_1's l1: 0.324339\n",
      "[205]\ttraining's l1: 0.273374\tvalid_1's l1: 0.324396\n",
      "[206]\ttraining's l1: 0.273275\tvalid_1's l1: 0.324398\n",
      "[207]\ttraining's l1: 0.27321\tvalid_1's l1: 0.324463\n",
      "[208]\ttraining's l1: 0.273158\tvalid_1's l1: 0.324533\n",
      "[209]\ttraining's l1: 0.273089\tvalid_1's l1: 0.32461\n",
      "[210]\ttraining's l1: 0.272987\tvalid_1's l1: 0.32464\n",
      "[211]\ttraining's l1: 0.272919\tvalid_1's l1: 0.324663\n",
      "[212]\ttraining's l1: 0.272834\tvalid_1's l1: 0.324725\n",
      "Early stopping, best iteration is:\n",
      "[162]\ttraining's l1: 0.277952\tvalid_1's l1: 0.323178\n",
      "The rmlse of prediction is: 0.14552\n",
      "Feature importances: [7, 51, 299, 4, 93, 0, 133, 235, 220, 71, 76, 39, 57, 46, 12, 615, 835, 289, 0, 408, 209, 212, 177, 161, 162, 135, 180, 920, 270, 618, 274, 1130]\n",
      "The rmlse of prediction is: 0.14986\n"
     ]
    }
   ],
   "source": [
    "rmsle2=get_best_lgb(path,2,N=1000)  not ok, why?\n",
    "#     gbm = lgb.LGBMRegressor(objective='regression',\n",
    "#                         num_leaves=50,\n",
    "#                         learning_rate=0.01,\n",
    "#                         n_estimators=N)\n",
    "\n",
    "#     gbm.fit(x_train, y_train,\n",
    "#         eval_set=[(x_train,y_train),(x_val,y_val)],\n",
    "#         eval_metric='l1',\n",
    "#         early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567719, 36) (34104, 36)\n",
      "[1]\ttraining's l1: 1.99603\tvalid_1's l1: 2.02453\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\ttraining's l1: 1.89746\tvalid_1's l1: 1.92465\n",
      "[3]\ttraining's l1: 1.80391\tvalid_1's l1: 1.8298\n",
      "[4]\ttraining's l1: 1.71504\tvalid_1's l1: 1.73977\n",
      "[5]\ttraining's l1: 1.63073\tvalid_1's l1: 1.65435\n",
      "[6]\ttraining's l1: 1.55059\tvalid_1's l1: 1.57317\n",
      "[7]\ttraining's l1: 1.47461\tvalid_1's l1: 1.49622\n",
      "[8]\ttraining's l1: 1.40243\tvalid_1's l1: 1.42308\n",
      "[9]\ttraining's l1: 1.33399\tvalid_1's l1: 1.35372\n",
      "[10]\ttraining's l1: 1.26898\tvalid_1's l1: 1.2879\n",
      "[11]\ttraining's l1: 1.20735\tvalid_1's l1: 1.22539\n",
      "[12]\ttraining's l1: 1.14888\tvalid_1's l1: 1.16613\n",
      "[13]\ttraining's l1: 1.09339\tvalid_1's l1: 1.10994\n",
      "[14]\ttraining's l1: 1.04084\tvalid_1's l1: 1.05665\n",
      "[15]\ttraining's l1: 0.990984\tvalid_1's l1: 1.00609\n",
      "[16]\ttraining's l1: 0.943762\tvalid_1's l1: 0.958171\n",
      "[17]\ttraining's l1: 0.898988\tvalid_1's l1: 0.91276\n",
      "[18]\ttraining's l1: 0.856513\tvalid_1's l1: 0.869669\n",
      "[19]\ttraining's l1: 0.816281\tvalid_1's l1: 0.828888\n",
      "[20]\ttraining's l1: 0.778149\tvalid_1's l1: 0.79019\n",
      "[21]\ttraining's l1: 0.742047\tvalid_1's l1: 0.753632\n",
      "[22]\ttraining's l1: 0.707906\tvalid_1's l1: 0.719083\n",
      "[23]\ttraining's l1: 0.675618\tvalid_1's l1: 0.686314\n",
      "[24]\ttraining's l1: 0.644938\tvalid_1's l1: 0.655155\n",
      "[25]\ttraining's l1: 0.615795\tvalid_1's l1: 0.625583\n",
      "[26]\ttraining's l1: 0.588324\tvalid_1's l1: 0.597718\n",
      "[27]\ttraining's l1: 0.562299\tvalid_1's l1: 0.57136\n",
      "[28]\ttraining's l1: 0.537609\tvalid_1's l1: 0.546312\n",
      "[29]\ttraining's l1: 0.514313\tvalid_1's l1: 0.522716\n",
      "[30]\ttraining's l1: 0.492281\tvalid_1's l1: 0.500448\n",
      "[31]\ttraining's l1: 0.471419\tvalid_1's l1: 0.479291\n",
      "[32]\ttraining's l1: 0.451759\tvalid_1's l1: 0.459426\n",
      "[33]\ttraining's l1: 0.433094\tvalid_1's l1: 0.440527\n",
      "[34]\ttraining's l1: 0.415483\tvalid_1's l1: 0.422696\n",
      "[35]\ttraining's l1: 0.398875\tvalid_1's l1: 0.405943\n",
      "[36]\ttraining's l1: 0.383229\tvalid_1's l1: 0.390056\n",
      "[37]\ttraining's l1: 0.368481\tvalid_1's l1: 0.375174\n",
      "[38]\ttraining's l1: 0.354534\tvalid_1's l1: 0.361071\n",
      "[39]\ttraining's l1: 0.34145\tvalid_1's l1: 0.347831\n",
      "[40]\ttraining's l1: 0.329052\tvalid_1's l1: 0.335266\n",
      "[41]\ttraining's l1: 0.317372\tvalid_1's l1: 0.32341\n",
      "[42]\ttraining's l1: 0.306331\tvalid_1's l1: 0.31221\n",
      "[43]\ttraining's l1: 0.295912\tvalid_1's l1: 0.301651\n",
      "[44]\ttraining's l1: 0.286074\tvalid_1's l1: 0.291784\n",
      "[45]\ttraining's l1: 0.27679\tvalid_1's l1: 0.282493\n",
      "[46]\ttraining's l1: 0.268007\tvalid_1's l1: 0.273712\n",
      "[47]\ttraining's l1: 0.259736\tvalid_1's l1: 0.265401\n",
      "[48]\ttraining's l1: 0.251929\tvalid_1's l1: 0.257687\n",
      "[49]\ttraining's l1: 0.244593\tvalid_1's l1: 0.250349\n",
      "[50]\ttraining's l1: 0.237715\tvalid_1's l1: 0.24353\n",
      "[51]\ttraining's l1: 0.231345\tvalid_1's l1: 0.237166\n",
      "[52]\ttraining's l1: 0.225261\tvalid_1's l1: 0.231148\n",
      "[53]\ttraining's l1: 0.219617\tvalid_1's l1: 0.225591\n",
      "[54]\ttraining's l1: 0.214298\tvalid_1's l1: 0.220251\n",
      "[55]\ttraining's l1: 0.209182\tvalid_1's l1: 0.215159\n",
      "[56]\ttraining's l1: 0.204506\tvalid_1's l1: 0.21052\n",
      "[57]\ttraining's l1: 0.200079\tvalid_1's l1: 0.206191\n",
      "[58]\ttraining's l1: 0.19578\tvalid_1's l1: 0.201931\n",
      "[59]\ttraining's l1: 0.1919\tvalid_1's l1: 0.198049\n",
      "[60]\ttraining's l1: 0.188215\tvalid_1's l1: 0.194428\n",
      "[61]\ttraining's l1: 0.184659\tvalid_1's l1: 0.190991\n",
      "[62]\ttraining's l1: 0.181285\tvalid_1's l1: 0.187666\n",
      "[63]\ttraining's l1: 0.178229\tvalid_1's l1: 0.18473\n",
      "[64]\ttraining's l1: 0.175358\tvalid_1's l1: 0.181934\n",
      "[65]\ttraining's l1: 0.172617\tvalid_1's l1: 0.179245\n",
      "[66]\ttraining's l1: 0.170018\tvalid_1's l1: 0.176715\n",
      "[67]\ttraining's l1: 0.167612\tvalid_1's l1: 0.174421\n",
      "[68]\ttraining's l1: 0.165354\tvalid_1's l1: 0.172247\n",
      "[69]\ttraining's l1: 0.163254\tvalid_1's l1: 0.170225\n",
      "[70]\ttraining's l1: 0.161268\tvalid_1's l1: 0.168358\n",
      "[71]\ttraining's l1: 0.159438\tvalid_1's l1: 0.166667\n",
      "[72]\ttraining's l1: 0.157643\tvalid_1's l1: 0.165039\n",
      "[73]\ttraining's l1: 0.156066\tvalid_1's l1: 0.163588\n",
      "[74]\ttraining's l1: 0.154534\tvalid_1's l1: 0.162194\n",
      "[75]\ttraining's l1: 0.15324\tvalid_1's l1: 0.160984\n",
      "[76]\ttraining's l1: 0.151896\tvalid_1's l1: 0.159796\n",
      "[77]\ttraining's l1: 0.150612\tvalid_1's l1: 0.158653\n",
      "[78]\ttraining's l1: 0.149518\tvalid_1's l1: 0.157668\n",
      "[79]\ttraining's l1: 0.148416\tvalid_1's l1: 0.156706\n",
      "[80]\ttraining's l1: 0.147366\tvalid_1's l1: 0.155797\n",
      "[81]\ttraining's l1: 0.146463\tvalid_1's l1: 0.155034\n",
      "[82]\ttraining's l1: 0.145538\tvalid_1's l1: 0.154214\n",
      "[83]\ttraining's l1: 0.144654\tvalid_1's l1: 0.153454\n",
      "[84]\ttraining's l1: 0.143796\tvalid_1's l1: 0.152746\n",
      "[85]\ttraining's l1: 0.143022\tvalid_1's l1: 0.152126\n",
      "[86]\ttraining's l1: 0.142316\tvalid_1's l1: 0.151519\n",
      "[87]\ttraining's l1: 0.141674\tvalid_1's l1: 0.151004\n",
      "[88]\ttraining's l1: 0.141022\tvalid_1's l1: 0.150456\n",
      "[89]\ttraining's l1: 0.140472\tvalid_1's l1: 0.150061\n",
      "[90]\ttraining's l1: 0.139847\tvalid_1's l1: 0.149595\n",
      "[91]\ttraining's l1: 0.139346\tvalid_1's l1: 0.149207\n",
      "[92]\ttraining's l1: 0.138823\tvalid_1's l1: 0.148856\n",
      "[93]\ttraining's l1: 0.138317\tvalid_1's l1: 0.148459\n",
      "[94]\ttraining's l1: 0.137912\tvalid_1's l1: 0.148211\n",
      "[95]\ttraining's l1: 0.137438\tvalid_1's l1: 0.147808\n",
      "[96]\ttraining's l1: 0.137044\tvalid_1's l1: 0.147485\n",
      "[97]\ttraining's l1: 0.136682\tvalid_1's l1: 0.147221\n",
      "[98]\ttraining's l1: 0.136274\tvalid_1's l1: 0.146868\n",
      "[99]\ttraining's l1: 0.135885\tvalid_1's l1: 0.146573\n",
      "[100]\ttraining's l1: 0.135636\tvalid_1's l1: 0.146368\n",
      "[101]\ttraining's l1: 0.135309\tvalid_1's l1: 0.146117\n",
      "[102]\ttraining's l1: 0.135027\tvalid_1's l1: 0.14586\n",
      "[103]\ttraining's l1: 0.13474\tvalid_1's l1: 0.145622\n",
      "[104]\ttraining's l1: 0.134519\tvalid_1's l1: 0.145484\n",
      "[105]\ttraining's l1: 0.134333\tvalid_1's l1: 0.145368\n",
      "[106]\ttraining's l1: 0.134012\tvalid_1's l1: 0.145168\n",
      "[107]\ttraining's l1: 0.133823\tvalid_1's l1: 0.145047\n",
      "[108]\ttraining's l1: 0.133645\tvalid_1's l1: 0.144893\n",
      "[109]\ttraining's l1: 0.133439\tvalid_1's l1: 0.144766\n",
      "[110]\ttraining's l1: 0.133247\tvalid_1's l1: 0.144664\n",
      "[111]\ttraining's l1: 0.133125\tvalid_1's l1: 0.144594\n",
      "[112]\ttraining's l1: 0.132963\tvalid_1's l1: 0.144529\n",
      "[113]\ttraining's l1: 0.132745\tvalid_1's l1: 0.144366\n",
      "[114]\ttraining's l1: 0.132587\tvalid_1's l1: 0.144293\n",
      "[115]\ttraining's l1: 0.132443\tvalid_1's l1: 0.144242\n",
      "[116]\ttraining's l1: 0.132346\tvalid_1's l1: 0.144213\n",
      "[117]\ttraining's l1: 0.132182\tvalid_1's l1: 0.144113\n",
      "[118]\ttraining's l1: 0.132084\tvalid_1's l1: 0.144082\n",
      "[119]\ttraining's l1: 0.131987\tvalid_1's l1: 0.144067\n",
      "[120]\ttraining's l1: 0.131908\tvalid_1's l1: 0.144087\n",
      "[121]\ttraining's l1: 0.131813\tvalid_1's l1: 0.144068\n",
      "[122]\ttraining's l1: 0.131712\tvalid_1's l1: 0.144069\n",
      "[123]\ttraining's l1: 0.131616\tvalid_1's l1: 0.144039\n",
      "[124]\ttraining's l1: 0.131439\tvalid_1's l1: 0.143906\n",
      "[125]\ttraining's l1: 0.131357\tvalid_1's l1: 0.143896\n",
      "[126]\ttraining's l1: 0.131175\tvalid_1's l1: 0.143815\n",
      "[127]\ttraining's l1: 0.130942\tvalid_1's l1: 0.143655\n",
      "[128]\ttraining's l1: 0.130863\tvalid_1's l1: 0.143635\n",
      "[129]\ttraining's l1: 0.130791\tvalid_1's l1: 0.143633\n",
      "[130]\ttraining's l1: 0.130714\tvalid_1's l1: 0.143659\n",
      "[131]\ttraining's l1: 0.130648\tvalid_1's l1: 0.143637\n",
      "[132]\ttraining's l1: 0.13059\tvalid_1's l1: 0.143661\n",
      "[133]\ttraining's l1: 0.130417\tvalid_1's l1: 0.143552\n",
      "[134]\ttraining's l1: 0.130257\tvalid_1's l1: 0.143472\n",
      "[135]\ttraining's l1: 0.130104\tvalid_1's l1: 0.143379\n",
      "[136]\ttraining's l1: 0.130044\tvalid_1's l1: 0.143391\n",
      "[137]\ttraining's l1: 0.129908\tvalid_1's l1: 0.143316\n",
      "[138]\ttraining's l1: 0.129761\tvalid_1's l1: 0.143253\n",
      "[139]\ttraining's l1: 0.129687\tvalid_1's l1: 0.143282\n",
      "[140]\ttraining's l1: 0.129548\tvalid_1's l1: 0.143232\n",
      "[141]\ttraining's l1: 0.129388\tvalid_1's l1: 0.143143\n",
      "[142]\ttraining's l1: 0.129337\tvalid_1's l1: 0.14315\n",
      "[143]\ttraining's l1: 0.129205\tvalid_1's l1: 0.143069\n",
      "[144]\ttraining's l1: 0.12909\tvalid_1's l1: 0.143008\n",
      "[145]\ttraining's l1: 0.128933\tvalid_1's l1: 0.142904\n",
      "[146]\ttraining's l1: 0.128883\tvalid_1's l1: 0.14292\n",
      "[147]\ttraining's l1: 0.128752\tvalid_1's l1: 0.142845\n",
      "[148]\ttraining's l1: 0.128681\tvalid_1's l1: 0.142825\n",
      "[149]\ttraining's l1: 0.128533\tvalid_1's l1: 0.142745\n",
      "[150]\ttraining's l1: 0.128408\tvalid_1's l1: 0.142712\n",
      "[151]\ttraining's l1: 0.128345\tvalid_1's l1: 0.142709\n",
      "[152]\ttraining's l1: 0.128212\tvalid_1's l1: 0.142648\n",
      "[153]\ttraining's l1: 0.128168\tvalid_1's l1: 0.14266\n",
      "[154]\ttraining's l1: 0.128114\tvalid_1's l1: 0.142656\n",
      "[155]\ttraining's l1: 0.128002\tvalid_1's l1: 0.142604\n",
      "[156]\ttraining's l1: 0.127932\tvalid_1's l1: 0.142581\n",
      "[157]\ttraining's l1: 0.127823\tvalid_1's l1: 0.142516\n",
      "[158]\ttraining's l1: 0.127779\tvalid_1's l1: 0.142572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159]\ttraining's l1: 0.127736\tvalid_1's l1: 0.142592\n",
      "[160]\ttraining's l1: 0.127682\tvalid_1's l1: 0.142612\n",
      "[161]\ttraining's l1: 0.127596\tvalid_1's l1: 0.142564\n",
      "[162]\ttraining's l1: 0.127547\tvalid_1's l1: 0.142599\n",
      "[163]\ttraining's l1: 0.127451\tvalid_1's l1: 0.142568\n",
      "[164]\ttraining's l1: 0.12738\tvalid_1's l1: 0.142546\n",
      "[165]\ttraining's l1: 0.127307\tvalid_1's l1: 0.142541\n",
      "[166]\ttraining's l1: 0.127251\tvalid_1's l1: 0.142511\n",
      "[167]\ttraining's l1: 0.127163\tvalid_1's l1: 0.142457\n",
      "[168]\ttraining's l1: 0.127121\tvalid_1's l1: 0.142458\n",
      "[169]\ttraining's l1: 0.127051\tvalid_1's l1: 0.142459\n",
      "[170]\ttraining's l1: 0.127014\tvalid_1's l1: 0.1425\n",
      "[171]\ttraining's l1: 0.126965\tvalid_1's l1: 0.142505\n",
      "[172]\ttraining's l1: 0.126928\tvalid_1's l1: 0.142511\n",
      "[173]\ttraining's l1: 0.126856\tvalid_1's l1: 0.142509\n",
      "[174]\ttraining's l1: 0.126807\tvalid_1's l1: 0.1425\n",
      "[175]\ttraining's l1: 0.126719\tvalid_1's l1: 0.142481\n",
      "[176]\ttraining's l1: 0.126645\tvalid_1's l1: 0.142447\n",
      "[177]\ttraining's l1: 0.126607\tvalid_1's l1: 0.142489\n",
      "[178]\ttraining's l1: 0.126574\tvalid_1's l1: 0.142483\n",
      "[179]\ttraining's l1: 0.126511\tvalid_1's l1: 0.142461\n",
      "[180]\ttraining's l1: 0.126465\tvalid_1's l1: 0.142489\n",
      "[181]\ttraining's l1: 0.126421\tvalid_1's l1: 0.142491\n",
      "[182]\ttraining's l1: 0.126363\tvalid_1's l1: 0.14252\n",
      "[183]\ttraining's l1: 0.126301\tvalid_1's l1: 0.142492\n",
      "[184]\ttraining's l1: 0.126258\tvalid_1's l1: 0.142504\n",
      "[185]\ttraining's l1: 0.126217\tvalid_1's l1: 0.142505\n",
      "[186]\ttraining's l1: 0.126162\tvalid_1's l1: 0.142493\n",
      "[187]\ttraining's l1: 0.126126\tvalid_1's l1: 0.142513\n",
      "[188]\ttraining's l1: 0.126063\tvalid_1's l1: 0.142512\n",
      "[189]\ttraining's l1: 0.126025\tvalid_1's l1: 0.142527\n",
      "[190]\ttraining's l1: 0.125976\tvalid_1's l1: 0.142521\n",
      "[191]\ttraining's l1: 0.125938\tvalid_1's l1: 0.142533\n",
      "[192]\ttraining's l1: 0.125871\tvalid_1's l1: 0.142532\n",
      "[193]\ttraining's l1: 0.12583\tvalid_1's l1: 0.142538\n",
      "[194]\ttraining's l1: 0.125787\tvalid_1's l1: 0.142547\n",
      "[195]\ttraining's l1: 0.12572\tvalid_1's l1: 0.142529\n",
      "[196]\ttraining's l1: 0.125684\tvalid_1's l1: 0.142541\n",
      "[197]\ttraining's l1: 0.125647\tvalid_1's l1: 0.142572\n",
      "[198]\ttraining's l1: 0.125604\tvalid_1's l1: 0.142597\n",
      "[199]\ttraining's l1: 0.125546\tvalid_1's l1: 0.142604\n",
      "[200]\ttraining's l1: 0.125499\tvalid_1's l1: 0.142599\n",
      "[201]\ttraining's l1: 0.125467\tvalid_1's l1: 0.142606\n",
      "[202]\ttraining's l1: 0.125432\tvalid_1's l1: 0.142611\n",
      "[203]\ttraining's l1: 0.125393\tvalid_1's l1: 0.142626\n",
      "[204]\ttraining's l1: 0.125345\tvalid_1's l1: 0.142621\n",
      "[205]\ttraining's l1: 0.125305\tvalid_1's l1: 0.142608\n",
      "[206]\ttraining's l1: 0.12527\tvalid_1's l1: 0.142605\n",
      "[207]\ttraining's l1: 0.125239\tvalid_1's l1: 0.142598\n",
      "[208]\ttraining's l1: 0.125201\tvalid_1's l1: 0.142624\n",
      "[209]\ttraining's l1: 0.125176\tvalid_1's l1: 0.14264\n",
      "[210]\ttraining's l1: 0.12512\tvalid_1's l1: 0.142647\n",
      "[211]\ttraining's l1: 0.125082\tvalid_1's l1: 0.142669\n",
      "[212]\ttraining's l1: 0.125058\tvalid_1's l1: 0.142703\n",
      "[213]\ttraining's l1: 0.125025\tvalid_1's l1: 0.142712\n",
      "[214]\ttraining's l1: 0.124992\tvalid_1's l1: 0.142747\n",
      "[215]\ttraining's l1: 0.12495\tvalid_1's l1: 0.142736\n",
      "[216]\ttraining's l1: 0.124907\tvalid_1's l1: 0.142718\n",
      "[217]\ttraining's l1: 0.124859\tvalid_1's l1: 0.142731\n",
      "[218]\ttraining's l1: 0.12483\tvalid_1's l1: 0.142745\n",
      "[219]\ttraining's l1: 0.124795\tvalid_1's l1: 0.14277\n",
      "[220]\ttraining's l1: 0.124768\tvalid_1's l1: 0.142794\n",
      "[221]\ttraining's l1: 0.124721\tvalid_1's l1: 0.142797\n",
      "[222]\ttraining's l1: 0.124686\tvalid_1's l1: 0.142812\n",
      "[223]\ttraining's l1: 0.124662\tvalid_1's l1: 0.142833\n",
      "[224]\ttraining's l1: 0.124635\tvalid_1's l1: 0.142854\n",
      "[225]\ttraining's l1: 0.124611\tvalid_1's l1: 0.142869\n",
      "[226]\ttraining's l1: 0.124574\tvalid_1's l1: 0.142879\n",
      "[227]\ttraining's l1: 0.124529\tvalid_1's l1: 0.14287\n",
      "[228]\ttraining's l1: 0.124505\tvalid_1's l1: 0.142882\n",
      "[229]\ttraining's l1: 0.124481\tvalid_1's l1: 0.142923\n",
      "[230]\ttraining's l1: 0.12446\tvalid_1's l1: 0.142931\n",
      "[231]\ttraining's l1: 0.124429\tvalid_1's l1: 0.142944\n",
      "[232]\ttraining's l1: 0.124405\tvalid_1's l1: 0.142957\n",
      "[233]\ttraining's l1: 0.124377\tvalid_1's l1: 0.142956\n",
      "[234]\ttraining's l1: 0.124351\tvalid_1's l1: 0.14296\n",
      "[235]\ttraining's l1: 0.124333\tvalid_1's l1: 0.142978\n",
      "[236]\ttraining's l1: 0.124303\tvalid_1's l1: 0.142999\n",
      "[237]\ttraining's l1: 0.124254\tvalid_1's l1: 0.143036\n",
      "[238]\ttraining's l1: 0.124233\tvalid_1's l1: 0.143055\n",
      "[239]\ttraining's l1: 0.124209\tvalid_1's l1: 0.143042\n",
      "[240]\ttraining's l1: 0.124189\tvalid_1's l1: 0.143058\n",
      "[241]\ttraining's l1: 0.12416\tvalid_1's l1: 0.143067\n",
      "[242]\ttraining's l1: 0.124138\tvalid_1's l1: 0.14308\n",
      "[243]\ttraining's l1: 0.124102\tvalid_1's l1: 0.143066\n",
      "[244]\ttraining's l1: 0.124071\tvalid_1's l1: 0.143092\n",
      "[245]\ttraining's l1: 0.124044\tvalid_1's l1: 0.143121\n",
      "[246]\ttraining's l1: 0.124023\tvalid_1's l1: 0.143132\n",
      "[247]\ttraining's l1: 0.124007\tvalid_1's l1: 0.143152\n",
      "[248]\ttraining's l1: 0.123984\tvalid_1's l1: 0.14317\n",
      "[249]\ttraining's l1: 0.123961\tvalid_1's l1: 0.143192\n",
      "[250]\ttraining's l1: 0.123936\tvalid_1's l1: 0.143205\n",
      "[251]\ttraining's l1: 0.123913\tvalid_1's l1: 0.143225\n",
      "[252]\ttraining's l1: 0.123881\tvalid_1's l1: 0.143212\n",
      "[253]\ttraining's l1: 0.123855\tvalid_1's l1: 0.143226\n",
      "[254]\ttraining's l1: 0.12383\tvalid_1's l1: 0.143235\n",
      "[255]\ttraining's l1: 0.123815\tvalid_1's l1: 0.143247\n",
      "[256]\ttraining's l1: 0.123796\tvalid_1's l1: 0.143264\n",
      "[257]\ttraining's l1: 0.123779\tvalid_1's l1: 0.14327\n",
      "[258]\ttraining's l1: 0.123757\tvalid_1's l1: 0.143268\n",
      "[259]\ttraining's l1: 0.123742\tvalid_1's l1: 0.143286\n",
      "[260]\ttraining's l1: 0.123719\tvalid_1's l1: 0.143301\n",
      "[261]\ttraining's l1: 0.123698\tvalid_1's l1: 0.143311\n",
      "[262]\ttraining's l1: 0.123683\tvalid_1's l1: 0.143332\n",
      "[263]\ttraining's l1: 0.123668\tvalid_1's l1: 0.143338\n",
      "[264]\ttraining's l1: 0.12364\tvalid_1's l1: 0.143358\n",
      "[265]\ttraining's l1: 0.123627\tvalid_1's l1: 0.143364\n",
      "[266]\ttraining's l1: 0.123607\tvalid_1's l1: 0.143372\n",
      "[267]\ttraining's l1: 0.123593\tvalid_1's l1: 0.143381\n",
      "[268]\ttraining's l1: 0.123578\tvalid_1's l1: 0.143402\n",
      "[269]\ttraining's l1: 0.123563\tvalid_1's l1: 0.143413\n",
      "[270]\ttraining's l1: 0.12354\tvalid_1's l1: 0.143429\n",
      "[271]\ttraining's l1: 0.123507\tvalid_1's l1: 0.143451\n",
      "[272]\ttraining's l1: 0.123483\tvalid_1's l1: 0.14347\n",
      "[273]\ttraining's l1: 0.12346\tvalid_1's l1: 0.143473\n",
      "[274]\ttraining's l1: 0.123441\tvalid_1's l1: 0.143468\n",
      "[275]\ttraining's l1: 0.12342\tvalid_1's l1: 0.143479\n",
      "[276]\ttraining's l1: 0.123392\tvalid_1's l1: 0.143468\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's l1: 0.126645\tvalid_1's l1: 0.142447\n",
      "The rmlse of prediction is: 0.09206\n",
      "Feature importances: [20, 169, 386, 20, 163, 0, 127, 287, 316, 127, 92, 64, 59, 93, 73, 760, 945, 430, 0, 451, 226, 328, 208, 285, 167, 149, 162, 1354, 192, 1176, 252, 1303]\n",
      "The rmlse of prediction is: 0.09284\n"
     ]
    }
   ],
   "source": [
    "get_best_lgb(path,1,N=1450)  it is ok\n",
    "#     gbm = lgb.LGBMRegressor(objective='regression',\n",
    "#                         num_leaves=60,\n",
    "#                         learning_rate=0.05,\n",
    "#                         n_estimators=N)\n",
    "\n",
    "#     gbm.fit(x_train, y_train,\n",
    "#         eval_set=[(x_train,y_train),(x_val,y_val)],\n",
    "#         eval_metric='l1',\n",
    "#         early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556288, 36) (34044, 36)\n",
      "[1]\ttraining's l1: 4.1637\tvalid_1's l1: 4.24108\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\ttraining's l1: 4.12262\tvalid_1's l1: 4.19928\n",
      "[3]\ttraining's l1: 4.08195\tvalid_1's l1: 4.15789\n",
      "[4]\ttraining's l1: 4.0417\tvalid_1's l1: 4.11694\n",
      "[5]\ttraining's l1: 4.00186\tvalid_1's l1: 4.07639\n",
      "[6]\ttraining's l1: 3.96241\tvalid_1's l1: 4.03628\n",
      "[7]\ttraining's l1: 3.92337\tvalid_1's l1: 3.99656\n",
      "[8]\ttraining's l1: 3.88472\tvalid_1's l1: 3.95725\n",
      "[9]\ttraining's l1: 3.84647\tvalid_1's l1: 3.91833\n",
      "[10]\ttraining's l1: 3.8086\tvalid_1's l1: 3.87981\n",
      "[11]\ttraining's l1: 3.77109\tvalid_1's l1: 3.84169\n",
      "[12]\ttraining's l1: 3.73399\tvalid_1's l1: 3.80394\n",
      "[13]\ttraining's l1: 3.69723\tvalid_1's l1: 3.76656\n",
      "[14]\ttraining's l1: 3.66085\tvalid_1's l1: 3.72956\n",
      "[15]\ttraining's l1: 3.62486\tvalid_1's l1: 3.69293\n",
      "[16]\ttraining's l1: 3.58922\tvalid_1's l1: 3.65667\n",
      "[17]\ttraining's l1: 3.55395\tvalid_1's l1: 3.62077\n",
      "[18]\ttraining's l1: 3.51903\tvalid_1's l1: 3.58525\n",
      "[19]\ttraining's l1: 3.48448\tvalid_1's l1: 3.5501\n",
      "[20]\ttraining's l1: 3.45026\tvalid_1's l1: 3.51529\n",
      "[21]\ttraining's l1: 3.41641\tvalid_1's l1: 3.48084\n",
      "[22]\ttraining's l1: 3.38289\tvalid_1's l1: 3.44672\n",
      "[23]\ttraining's l1: 3.34972\tvalid_1's l1: 3.41297\n",
      "[24]\ttraining's l1: 3.31686\tvalid_1's l1: 3.37957\n",
      "[25]\ttraining's l1: 3.28435\tvalid_1's l1: 3.3465\n",
      "[26]\ttraining's l1: 3.25218\tvalid_1's l1: 3.31376\n",
      "[27]\ttraining's l1: 3.22031\tvalid_1's l1: 3.28133\n",
      "[28]\ttraining's l1: 3.18878\tvalid_1's l1: 3.24924\n",
      "[29]\ttraining's l1: 3.15756\tvalid_1's l1: 3.21747\n",
      "[30]\ttraining's l1: 3.12666\tvalid_1's l1: 3.18602\n",
      "[31]\ttraining's l1: 3.09606\tvalid_1's l1: 3.15487\n",
      "[32]\ttraining's l1: 3.0658\tvalid_1's l1: 3.12408\n",
      "[33]\ttraining's l1: 3.03583\tvalid_1's l1: 3.0936\n",
      "[34]\ttraining's l1: 3.00618\tvalid_1's l1: 3.06342\n",
      "[35]\ttraining's l1: 2.97681\tvalid_1's l1: 3.03355\n",
      "[36]\ttraining's l1: 2.94777\tvalid_1's l1: 3.00399\n",
      "[37]\ttraining's l1: 2.91898\tvalid_1's l1: 2.9747\n",
      "[38]\ttraining's l1: 2.89049\tvalid_1's l1: 2.94572\n",
      "[39]\ttraining's l1: 2.8623\tvalid_1's l1: 2.91703\n",
      "[40]\ttraining's l1: 2.83441\tvalid_1's l1: 2.88866\n",
      "[41]\ttraining's l1: 2.80677\tvalid_1's l1: 2.86054\n",
      "[42]\ttraining's l1: 2.77942\tvalid_1's l1: 2.83269\n",
      "[43]\ttraining's l1: 2.75235\tvalid_1's l1: 2.80515\n",
      "[44]\ttraining's l1: 2.72557\tvalid_1's l1: 2.77791\n",
      "[45]\ttraining's l1: 2.69905\tvalid_1's l1: 2.75091\n",
      "[46]\ttraining's l1: 2.6728\tvalid_1's l1: 2.72422\n",
      "[47]\ttraining's l1: 2.64681\tvalid_1's l1: 2.69777\n",
      "[48]\ttraining's l1: 2.6211\tvalid_1's l1: 2.6716\n",
      "[49]\ttraining's l1: 2.59565\tvalid_1's l1: 2.64567\n",
      "[50]\ttraining's l1: 2.57045\tvalid_1's l1: 2.62005\n",
      "[51]\ttraining's l1: 2.54551\tvalid_1's l1: 2.59465\n",
      "[52]\ttraining's l1: 2.5208\tvalid_1's l1: 2.56951\n",
      "[53]\ttraining's l1: 2.49636\tvalid_1's l1: 2.54465\n",
      "[54]\ttraining's l1: 2.47217\tvalid_1's l1: 2.52005\n",
      "[55]\ttraining's l1: 2.44822\tvalid_1's l1: 2.49566\n",
      "[56]\ttraining's l1: 2.42452\tvalid_1's l1: 2.47152\n",
      "[57]\ttraining's l1: 2.40104\tvalid_1's l1: 2.44762\n",
      "[58]\ttraining's l1: 2.37779\tvalid_1's l1: 2.42393\n",
      "[59]\ttraining's l1: 2.35479\tvalid_1's l1: 2.40053\n",
      "[60]\ttraining's l1: 2.33203\tvalid_1's l1: 2.37735\n",
      "[61]\ttraining's l1: 2.3095\tvalid_1's l1: 2.35443\n",
      "[62]\ttraining's l1: 2.2872\tvalid_1's l1: 2.33169\n",
      "[63]\ttraining's l1: 2.26515\tvalid_1's l1: 2.30926\n",
      "[64]\ttraining's l1: 2.2433\tvalid_1's l1: 2.28704\n",
      "[65]\ttraining's l1: 2.22166\tvalid_1's l1: 2.26503\n",
      "[66]\ttraining's l1: 2.20025\tvalid_1's l1: 2.24321\n",
      "[67]\ttraining's l1: 2.17906\tvalid_1's l1: 2.22163\n",
      "[68]\ttraining's l1: 2.15808\tvalid_1's l1: 2.20025\n",
      "[69]\ttraining's l1: 2.13734\tvalid_1's l1: 2.17912\n",
      "[70]\ttraining's l1: 2.11679\tvalid_1's l1: 2.15819\n",
      "[71]\ttraining's l1: 2.09646\tvalid_1's l1: 2.13746\n",
      "[72]\ttraining's l1: 2.07633\tvalid_1's l1: 2.11694\n",
      "[73]\ttraining's l1: 2.05641\tvalid_1's l1: 2.09665\n",
      "[74]\ttraining's l1: 2.03678\tvalid_1's l1: 2.07663\n",
      "[75]\ttraining's l1: 2.01725\tvalid_1's l1: 2.05679\n",
      "[76]\ttraining's l1: 1.99793\tvalid_1's l1: 2.0371\n",
      "[77]\ttraining's l1: 1.97882\tvalid_1's l1: 2.01766\n",
      "[78]\ttraining's l1: 1.95989\tvalid_1's l1: 1.99836\n",
      "[79]\ttraining's l1: 1.94117\tvalid_1's l1: 1.9793\n",
      "[80]\ttraining's l1: 1.92262\tvalid_1's l1: 1.96044\n",
      "[81]\ttraining's l1: 1.90428\tvalid_1's l1: 1.94175\n",
      "[82]\ttraining's l1: 1.88611\tvalid_1's l1: 1.92321\n",
      "[83]\ttraining's l1: 1.86812\tvalid_1's l1: 1.9049\n",
      "[84]\ttraining's l1: 1.85033\tvalid_1's l1: 1.88679\n",
      "[85]\ttraining's l1: 1.83272\tvalid_1's l1: 1.86888\n",
      "[86]\ttraining's l1: 1.81536\tvalid_1's l1: 1.85119\n",
      "[87]\ttraining's l1: 1.7981\tvalid_1's l1: 1.83365\n",
      "[88]\ttraining's l1: 1.7811\tvalid_1's l1: 1.81632\n",
      "[89]\ttraining's l1: 1.76419\tvalid_1's l1: 1.79914\n",
      "[90]\ttraining's l1: 1.74754\tvalid_1's l1: 1.78218\n",
      "[91]\ttraining's l1: 1.73098\tvalid_1's l1: 1.76531\n",
      "[92]\ttraining's l1: 1.71461\tvalid_1's l1: 1.74864\n",
      "[93]\ttraining's l1: 1.69846\tvalid_1's l1: 1.73219\n",
      "[94]\ttraining's l1: 1.68244\tvalid_1's l1: 1.71585\n",
      "[95]\ttraining's l1: 1.66661\tvalid_1's l1: 1.69972\n",
      "[96]\ttraining's l1: 1.65089\tvalid_1's l1: 1.68368\n",
      "[97]\ttraining's l1: 1.63534\tvalid_1's l1: 1.66786\n",
      "[98]\ttraining's l1: 1.62\tvalid_1's l1: 1.65222\n",
      "[99]\ttraining's l1: 1.60476\tvalid_1's l1: 1.6367\n",
      "[100]\ttraining's l1: 1.58974\tvalid_1's l1: 1.62138\n",
      "[101]\ttraining's l1: 1.57483\tvalid_1's l1: 1.60618\n",
      "[102]\ttraining's l1: 1.56011\tvalid_1's l1: 1.59116\n",
      "[103]\ttraining's l1: 1.5455\tvalid_1's l1: 1.57627\n",
      "[104]\ttraining's l1: 1.53108\tvalid_1's l1: 1.56153\n",
      "[105]\ttraining's l1: 1.51683\tvalid_1's l1: 1.54697\n",
      "[106]\ttraining's l1: 1.50266\tvalid_1's l1: 1.53255\n",
      "[107]\ttraining's l1: 1.4887\tvalid_1's l1: 1.51829\n",
      "[108]\ttraining's l1: 1.47485\tvalid_1's l1: 1.50416\n",
      "[109]\ttraining's l1: 1.46118\tvalid_1's l1: 1.49017\n",
      "[110]\ttraining's l1: 1.44766\tvalid_1's l1: 1.47635\n",
      "[111]\ttraining's l1: 1.43425\tvalid_1's l1: 1.46264\n",
      "[112]\ttraining's l1: 1.42097\tvalid_1's l1: 1.44908\n",
      "[113]\ttraining's l1: 1.40782\tvalid_1's l1: 1.43565\n",
      "[114]\ttraining's l1: 1.39483\tvalid_1's l1: 1.42239\n",
      "[115]\ttraining's l1: 1.38196\tvalid_1's l1: 1.40924\n",
      "[116]\ttraining's l1: 1.36924\tvalid_1's l1: 1.39626\n",
      "[117]\ttraining's l1: 1.35665\tvalid_1's l1: 1.38343\n",
      "[118]\ttraining's l1: 1.34423\tvalid_1's l1: 1.37074\n",
      "[119]\ttraining's l1: 1.33192\tvalid_1's l1: 1.35817\n",
      "[120]\ttraining's l1: 1.31973\tvalid_1's l1: 1.34572\n",
      "[121]\ttraining's l1: 1.30765\tvalid_1's l1: 1.33338\n",
      "[122]\ttraining's l1: 1.29574\tvalid_1's l1: 1.32122\n",
      "[123]\ttraining's l1: 1.28391\tvalid_1's l1: 1.30913\n",
      "[124]\ttraining's l1: 1.27221\tvalid_1's l1: 1.29716\n",
      "[125]\ttraining's l1: 1.26062\tvalid_1's l1: 1.28531\n",
      "[126]\ttraining's l1: 1.24916\tvalid_1's l1: 1.27361\n",
      "[127]\ttraining's l1: 1.23782\tvalid_1's l1: 1.26204\n",
      "[128]\ttraining's l1: 1.22661\tvalid_1's l1: 1.25062\n",
      "[129]\ttraining's l1: 1.21552\tvalid_1's l1: 1.23929\n",
      "[130]\ttraining's l1: 1.20453\tvalid_1's l1: 1.22808\n",
      "[131]\ttraining's l1: 1.19366\tvalid_1's l1: 1.21699\n",
      "[132]\ttraining's l1: 1.18291\tvalid_1's l1: 1.20605\n",
      "[133]\ttraining's l1: 1.17228\tvalid_1's l1: 1.1952\n",
      "[134]\ttraining's l1: 1.16178\tvalid_1's l1: 1.1845\n",
      "[135]\ttraining's l1: 1.15134\tvalid_1's l1: 1.17386\n",
      "[136]\ttraining's l1: 1.14104\tvalid_1's l1: 1.16336\n",
      "[137]\ttraining's l1: 1.13081\tvalid_1's l1: 1.15294\n",
      "[138]\ttraining's l1: 1.12074\tvalid_1's l1: 1.14269\n",
      "[139]\ttraining's l1: 1.11073\tvalid_1's l1: 1.13248\n",
      "[140]\ttraining's l1: 1.10086\tvalid_1's l1: 1.12241\n",
      "[141]\ttraining's l1: 1.09107\tvalid_1's l1: 1.11244\n",
      "[142]\ttraining's l1: 1.08138\tvalid_1's l1: 1.10259\n",
      "[143]\ttraining's l1: 1.07183\tvalid_1's l1: 1.09287\n",
      "[144]\ttraining's l1: 1.06234\tvalid_1's l1: 1.0832\n",
      "[145]\ttraining's l1: 1.05299\tvalid_1's l1: 1.0737\n",
      "[146]\ttraining's l1: 1.04371\tvalid_1's l1: 1.06425\n",
      "[147]\ttraining's l1: 1.03452\tvalid_1's l1: 1.0549\n",
      "[148]\ttraining's l1: 1.02543\tvalid_1's l1: 1.04565\n",
      "[149]\ttraining's l1: 1.01644\tvalid_1's l1: 1.03648\n",
      "[150]\ttraining's l1: 1.00753\tvalid_1's l1: 1.02742\n",
      "[151]\ttraining's l1: 0.998744\tvalid_1's l1: 1.01844\n",
      "[152]\ttraining's l1: 0.990043\tvalid_1's l1: 1.00955\n",
      "[153]\ttraining's l1: 0.981411\tvalid_1's l1: 1.00077\n",
      "[154]\ttraining's l1: 0.972893\tvalid_1's l1: 0.992114\n",
      "[155]\ttraining's l1: 0.96444\tvalid_1's l1: 0.983506\n",
      "[156]\ttraining's l1: 0.956084\tvalid_1's l1: 0.975014\n",
      "[157]\ttraining's l1: 0.947803\tvalid_1's l1: 0.966586\n",
      "[158]\ttraining's l1: 0.939625\tvalid_1's l1: 0.958247\n",
      "[159]\ttraining's l1: 0.93153\tvalid_1's l1: 0.950014\n",
      "[160]\ttraining's l1: 0.923516\tvalid_1's l1: 0.94187\n",
      "[161]\ttraining's l1: 0.91561\tvalid_1's l1: 0.933828\n",
      "[162]\ttraining's l1: 0.907763\tvalid_1's l1: 0.925849\n",
      "[163]\ttraining's l1: 0.899997\tvalid_1's l1: 0.917961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164]\ttraining's l1: 0.892328\tvalid_1's l1: 0.910161\n",
      "[165]\ttraining's l1: 0.884745\tvalid_1's l1: 0.902437\n",
      "[166]\ttraining's l1: 0.877229\tvalid_1's l1: 0.894829\n",
      "[167]\ttraining's l1: 0.869796\tvalid_1's l1: 0.887279\n",
      "[168]\ttraining's l1: 0.86244\tvalid_1's l1: 0.879778\n",
      "[169]\ttraining's l1: 0.855192\tvalid_1's l1: 0.872424\n",
      "[170]\ttraining's l1: 0.848001\tvalid_1's l1: 0.865125\n",
      "[171]\ttraining's l1: 0.840884\tvalid_1's l1: 0.857923\n",
      "[172]\ttraining's l1: 0.833845\tvalid_1's l1: 0.850797\n",
      "[173]\ttraining's l1: 0.826872\tvalid_1's l1: 0.843737\n",
      "[174]\ttraining's l1: 0.819992\tvalid_1's l1: 0.836771\n",
      "[175]\ttraining's l1: 0.813181\tvalid_1's l1: 0.829861\n",
      "[176]\ttraining's l1: 0.806442\tvalid_1's l1: 0.823021\n",
      "[177]\ttraining's l1: 0.799785\tvalid_1's l1: 0.816298\n",
      "[178]\ttraining's l1: 0.793191\tvalid_1's l1: 0.809618\n",
      "[179]\ttraining's l1: 0.78667\tvalid_1's l1: 0.80301\n",
      "[180]\ttraining's l1: 0.780237\tvalid_1's l1: 0.796508\n",
      "[181]\ttraining's l1: 0.773866\tvalid_1's l1: 0.790062\n",
      "[182]\ttraining's l1: 0.767554\tvalid_1's l1: 0.783685\n",
      "[183]\ttraining's l1: 0.761326\tvalid_1's l1: 0.777377\n",
      "[184]\ttraining's l1: 0.75516\tvalid_1's l1: 0.77114\n",
      "[185]\ttraining's l1: 0.749058\tvalid_1's l1: 0.764976\n",
      "[186]\ttraining's l1: 0.743019\tvalid_1's l1: 0.758872\n",
      "[187]\ttraining's l1: 0.73704\tvalid_1's l1: 0.752813\n",
      "[188]\ttraining's l1: 0.731145\tvalid_1's l1: 0.746854\n",
      "[189]\ttraining's l1: 0.725312\tvalid_1's l1: 0.740932\n",
      "[190]\ttraining's l1: 0.719534\tvalid_1's l1: 0.735089\n",
      "[191]\ttraining's l1: 0.713834\tvalid_1's l1: 0.72931\n",
      "[192]\ttraining's l1: 0.70818\tvalid_1's l1: 0.723582\n",
      "[193]\ttraining's l1: 0.702605\tvalid_1's l1: 0.717947\n",
      "[194]\ttraining's l1: 0.697082\tvalid_1's l1: 0.712365\n",
      "[195]\ttraining's l1: 0.691601\tvalid_1's l1: 0.706823\n",
      "[196]\ttraining's l1: 0.686184\tvalid_1's l1: 0.701334\n",
      "[197]\ttraining's l1: 0.68083\tvalid_1's l1: 0.695911\n",
      "[198]\ttraining's l1: 0.675535\tvalid_1's l1: 0.690551\n",
      "[199]\ttraining's l1: 0.670303\tvalid_1's l1: 0.685255\n",
      "[200]\ttraining's l1: 0.665127\tvalid_1's l1: 0.680027\n",
      "[201]\ttraining's l1: 0.66001\tvalid_1's l1: 0.674871\n",
      "[202]\ttraining's l1: 0.654946\tvalid_1's l1: 0.669741\n",
      "[203]\ttraining's l1: 0.649939\tvalid_1's l1: 0.664686\n",
      "[204]\ttraining's l1: 0.64499\tvalid_1's l1: 0.659685\n",
      "[205]\ttraining's l1: 0.640101\tvalid_1's l1: 0.654729\n",
      "[206]\ttraining's l1: 0.635272\tvalid_1's l1: 0.649854\n",
      "[207]\ttraining's l1: 0.630502\tvalid_1's l1: 0.645026\n",
      "[208]\ttraining's l1: 0.625767\tvalid_1's l1: 0.640261\n",
      "[209]\ttraining's l1: 0.621089\tvalid_1's l1: 0.635534\n",
      "[210]\ttraining's l1: 0.616471\tvalid_1's l1: 0.630866\n",
      "[211]\ttraining's l1: 0.611897\tvalid_1's l1: 0.626236\n",
      "[212]\ttraining's l1: 0.607368\tvalid_1's l1: 0.621646\n",
      "[213]\ttraining's l1: 0.602893\tvalid_1's l1: 0.617121\n",
      "[214]\ttraining's l1: 0.598472\tvalid_1's l1: 0.612629\n",
      "[215]\ttraining's l1: 0.594095\tvalid_1's l1: 0.60819\n",
      "[216]\ttraining's l1: 0.589781\tvalid_1's l1: 0.603822\n",
      "[217]\ttraining's l1: 0.585514\tvalid_1's l1: 0.599483\n",
      "[218]\ttraining's l1: 0.581287\tvalid_1's l1: 0.595204\n",
      "[219]\ttraining's l1: 0.577105\tvalid_1's l1: 0.590958\n",
      "[220]\ttraining's l1: 0.572973\tvalid_1's l1: 0.586769\n",
      "[221]\ttraining's l1: 0.56888\tvalid_1's l1: 0.582631\n",
      "[222]\ttraining's l1: 0.564839\tvalid_1's l1: 0.578537\n",
      "[223]\ttraining's l1: 0.560856\tvalid_1's l1: 0.574509\n",
      "[224]\ttraining's l1: 0.556895\tvalid_1's l1: 0.570514\n",
      "[225]\ttraining's l1: 0.552992\tvalid_1's l1: 0.566569\n",
      "[226]\ttraining's l1: 0.549134\tvalid_1's l1: 0.562687\n",
      "[227]\ttraining's l1: 0.545328\tvalid_1's l1: 0.558853\n",
      "[228]\ttraining's l1: 0.541558\tvalid_1's l1: 0.555046\n",
      "[229]\ttraining's l1: 0.537818\tvalid_1's l1: 0.551288\n",
      "[230]\ttraining's l1: 0.534144\tvalid_1's l1: 0.547601\n",
      "[231]\ttraining's l1: 0.530485\tvalid_1's l1: 0.543924\n",
      "[232]\ttraining's l1: 0.526883\tvalid_1's l1: 0.540288\n",
      "[233]\ttraining's l1: 0.523314\tvalid_1's l1: 0.536697\n",
      "[234]\ttraining's l1: 0.519788\tvalid_1's l1: 0.53315\n",
      "[235]\ttraining's l1: 0.516296\tvalid_1's l1: 0.529649\n",
      "[236]\ttraining's l1: 0.512851\tvalid_1's l1: 0.526193\n",
      "[237]\ttraining's l1: 0.509457\tvalid_1's l1: 0.522789\n",
      "[238]\ttraining's l1: 0.506097\tvalid_1's l1: 0.519414\n",
      "[239]\ttraining's l1: 0.502775\tvalid_1's l1: 0.516095\n",
      "[240]\ttraining's l1: 0.499474\tvalid_1's l1: 0.512797\n",
      "[241]\ttraining's l1: 0.496234\tvalid_1's l1: 0.509552\n",
      "[242]\ttraining's l1: 0.493017\tvalid_1's l1: 0.506356\n",
      "[243]\ttraining's l1: 0.489899\tvalid_1's l1: 0.503211\n",
      "[244]\ttraining's l1: 0.486755\tvalid_1's l1: 0.500078\n",
      "[245]\ttraining's l1: 0.483709\tvalid_1's l1: 0.497055\n",
      "[246]\ttraining's l1: 0.480695\tvalid_1's l1: 0.494035\n",
      "[247]\ttraining's l1: 0.477667\tvalid_1's l1: 0.491024\n",
      "[248]\ttraining's l1: 0.474704\tvalid_1's l1: 0.488061\n",
      "[249]\ttraining's l1: 0.471749\tvalid_1's l1: 0.485101\n",
      "[250]\ttraining's l1: 0.468814\tvalid_1's l1: 0.482163\n",
      "[251]\ttraining's l1: 0.465973\tvalid_1's l1: 0.479321\n",
      "[252]\ttraining's l1: 0.463155\tvalid_1's l1: 0.476519\n",
      "[253]\ttraining's l1: 0.460312\tvalid_1's l1: 0.47371\n",
      "[254]\ttraining's l1: 0.457543\tvalid_1's l1: 0.470953\n",
      "[255]\ttraining's l1: 0.454836\tvalid_1's l1: 0.468278\n",
      "[256]\ttraining's l1: 0.452096\tvalid_1's l1: 0.465549\n",
      "[257]\ttraining's l1: 0.44945\tvalid_1's l1: 0.462941\n",
      "[258]\ttraining's l1: 0.446828\tvalid_1's l1: 0.460344\n",
      "[259]\ttraining's l1: 0.444176\tvalid_1's l1: 0.457706\n",
      "[260]\ttraining's l1: 0.441623\tvalid_1's l1: 0.455185\n",
      "[261]\ttraining's l1: 0.439093\tvalid_1's l1: 0.452686\n",
      "[262]\ttraining's l1: 0.436534\tvalid_1's l1: 0.450131\n",
      "[263]\ttraining's l1: 0.434036\tvalid_1's l1: 0.44766\n",
      "[264]\ttraining's l1: 0.431589\tvalid_1's l1: 0.445204\n",
      "[265]\ttraining's l1: 0.429112\tvalid_1's l1: 0.442747\n",
      "[266]\ttraining's l1: 0.42673\tvalid_1's l1: 0.440371\n",
      "[267]\ttraining's l1: 0.42434\tvalid_1's l1: 0.437998\n",
      "[268]\ttraining's l1: 0.422009\tvalid_1's l1: 0.435675\n",
      "[269]\ttraining's l1: 0.419648\tvalid_1's l1: 0.433312\n",
      "[270]\ttraining's l1: 0.417363\tvalid_1's l1: 0.431061\n",
      "[271]\ttraining's l1: 0.415076\tvalid_1's l1: 0.428774\n",
      "[272]\ttraining's l1: 0.412816\tvalid_1's l1: 0.426512\n",
      "[273]\ttraining's l1: 0.410609\tvalid_1's l1: 0.424304\n",
      "[274]\ttraining's l1: 0.408397\tvalid_1's l1: 0.422111\n",
      "[275]\ttraining's l1: 0.406252\tvalid_1's l1: 0.419995\n",
      "[276]\ttraining's l1: 0.404135\tvalid_1's l1: 0.417887\n",
      "[277]\ttraining's l1: 0.402002\tvalid_1's l1: 0.415783\n",
      "[278]\ttraining's l1: 0.399915\tvalid_1's l1: 0.413713\n",
      "[279]\ttraining's l1: 0.397819\tvalid_1's l1: 0.411626\n",
      "[280]\ttraining's l1: 0.395793\tvalid_1's l1: 0.409619\n",
      "[281]\ttraining's l1: 0.393795\tvalid_1's l1: 0.407628\n",
      "[282]\ttraining's l1: 0.391779\tvalid_1's l1: 0.405625\n",
      "[283]\ttraining's l1: 0.389833\tvalid_1's l1: 0.403686\n",
      "[284]\ttraining's l1: 0.387862\tvalid_1's l1: 0.401719\n",
      "[285]\ttraining's l1: 0.385955\tvalid_1's l1: 0.399828\n",
      "[286]\ttraining's l1: 0.38404\tvalid_1's l1: 0.397929\n",
      "[287]\ttraining's l1: 0.38217\tvalid_1's l1: 0.396076\n",
      "[288]\ttraining's l1: 0.380327\tvalid_1's l1: 0.394227\n",
      "[289]\ttraining's l1: 0.378472\tvalid_1's l1: 0.392424\n",
      "[290]\ttraining's l1: 0.376676\tvalid_1's l1: 0.390635\n",
      "[291]\ttraining's l1: 0.374882\tvalid_1's l1: 0.388848\n",
      "[292]\ttraining's l1: 0.373105\tvalid_1's l1: 0.38708\n",
      "[293]\ttraining's l1: 0.371371\tvalid_1's l1: 0.38535\n",
      "[294]\ttraining's l1: 0.369639\tvalid_1's l1: 0.383624\n",
      "[295]\ttraining's l1: 0.367922\tvalid_1's l1: 0.381922\n",
      "[296]\ttraining's l1: 0.366205\tvalid_1's l1: 0.380237\n",
      "[297]\ttraining's l1: 0.364536\tvalid_1's l1: 0.378586\n",
      "[298]\ttraining's l1: 0.362874\tvalid_1's l1: 0.376958\n",
      "[299]\ttraining's l1: 0.361251\tvalid_1's l1: 0.375369\n",
      "[300]\ttraining's l1: 0.359641\tvalid_1's l1: 0.373768\n",
      "[301]\ttraining's l1: 0.358033\tvalid_1's l1: 0.372176\n",
      "[302]\ttraining's l1: 0.356452\tvalid_1's l1: 0.37062\n",
      "[303]\ttraining's l1: 0.3549\tvalid_1's l1: 0.369086\n",
      "[304]\ttraining's l1: 0.353345\tvalid_1's l1: 0.367557\n",
      "[305]\ttraining's l1: 0.351837\tvalid_1's l1: 0.366073\n",
      "[306]\ttraining's l1: 0.350351\tvalid_1's l1: 0.364618\n",
      "[307]\ttraining's l1: 0.348858\tvalid_1's l1: 0.363164\n",
      "[308]\ttraining's l1: 0.347412\tvalid_1's l1: 0.361733\n",
      "[309]\ttraining's l1: 0.345967\tvalid_1's l1: 0.360322\n",
      "[310]\ttraining's l1: 0.344543\tvalid_1's l1: 0.358892\n",
      "[311]\ttraining's l1: 0.343124\tvalid_1's l1: 0.357511\n",
      "[312]\ttraining's l1: 0.341722\tvalid_1's l1: 0.356152\n",
      "[313]\ttraining's l1: 0.340348\tvalid_1's l1: 0.354788\n",
      "[314]\ttraining's l1: 0.338991\tvalid_1's l1: 0.353458\n",
      "[315]\ttraining's l1: 0.337656\tvalid_1's l1: 0.352164\n",
      "[316]\ttraining's l1: 0.336325\tvalid_1's l1: 0.350878\n",
      "[317]\ttraining's l1: 0.335011\tvalid_1's l1: 0.349597\n",
      "[318]\ttraining's l1: 0.333739\tvalid_1's l1: 0.348355\n",
      "[319]\ttraining's l1: 0.332469\tvalid_1's l1: 0.34712\n",
      "[320]\ttraining's l1: 0.331202\tvalid_1's l1: 0.345906\n",
      "[321]\ttraining's l1: 0.329977\tvalid_1's l1: 0.344688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[322]\ttraining's l1: 0.328754\tvalid_1's l1: 0.343485\n",
      "[323]\ttraining's l1: 0.327548\tvalid_1's l1: 0.342319\n",
      "[324]\ttraining's l1: 0.326353\tvalid_1's l1: 0.341141\n",
      "[325]\ttraining's l1: 0.325161\tvalid_1's l1: 0.33999\n",
      "[326]\ttraining's l1: 0.323989\tvalid_1's l1: 0.33884\n",
      "[327]\ttraining's l1: 0.322846\tvalid_1's l1: 0.337721\n",
      "[328]\ttraining's l1: 0.321714\tvalid_1's l1: 0.336616\n",
      "[329]\ttraining's l1: 0.320581\tvalid_1's l1: 0.335516\n",
      "[330]\ttraining's l1: 0.31949\tvalid_1's l1: 0.334434\n",
      "[331]\ttraining's l1: 0.318396\tvalid_1's l1: 0.333387\n",
      "[332]\ttraining's l1: 0.317324\tvalid_1's l1: 0.332342\n",
      "[333]\ttraining's l1: 0.31626\tvalid_1's l1: 0.331301\n",
      "[334]\ttraining's l1: 0.315219\tvalid_1's l1: 0.330287\n",
      "[335]\ttraining's l1: 0.31417\tvalid_1's l1: 0.329267\n",
      "[336]\ttraining's l1: 0.313148\tvalid_1's l1: 0.328283\n",
      "[337]\ttraining's l1: 0.312142\tvalid_1's l1: 0.327297\n",
      "[338]\ttraining's l1: 0.311145\tvalid_1's l1: 0.326335\n",
      "[339]\ttraining's l1: 0.310157\tvalid_1's l1: 0.325382\n",
      "[340]\ttraining's l1: 0.309201\tvalid_1's l1: 0.324438\n",
      "[341]\ttraining's l1: 0.308245\tvalid_1's l1: 0.323514\n",
      "[342]\ttraining's l1: 0.307296\tvalid_1's l1: 0.322587\n",
      "[343]\ttraining's l1: 0.306358\tvalid_1's l1: 0.321691\n",
      "[344]\ttraining's l1: 0.305438\tvalid_1's l1: 0.32079\n",
      "[345]\ttraining's l1: 0.304534\tvalid_1's l1: 0.319919\n",
      "[346]\ttraining's l1: 0.303617\tvalid_1's l1: 0.319049\n",
      "[347]\ttraining's l1: 0.302706\tvalid_1's l1: 0.318152\n",
      "[348]\ttraining's l1: 0.301824\tvalid_1's l1: 0.317301\n",
      "[349]\ttraining's l1: 0.300957\tvalid_1's l1: 0.316465\n",
      "[350]\ttraining's l1: 0.30009\tvalid_1's l1: 0.315636\n",
      "[351]\ttraining's l1: 0.299234\tvalid_1's l1: 0.314819\n",
      "[352]\ttraining's l1: 0.298398\tvalid_1's l1: 0.314009\n",
      "[353]\ttraining's l1: 0.297568\tvalid_1's l1: 0.313215\n",
      "[354]\ttraining's l1: 0.296758\tvalid_1's l1: 0.312424\n",
      "[355]\ttraining's l1: 0.295954\tvalid_1's l1: 0.311659\n",
      "[356]\ttraining's l1: 0.295158\tvalid_1's l1: 0.310905\n",
      "[357]\ttraining's l1: 0.294375\tvalid_1's l1: 0.310144\n",
      "[358]\ttraining's l1: 0.293603\tvalid_1's l1: 0.309427\n",
      "[359]\ttraining's l1: 0.292833\tvalid_1's l1: 0.308679\n",
      "[360]\ttraining's l1: 0.29207\tvalid_1's l1: 0.307954\n",
      "[361]\ttraining's l1: 0.291322\tvalid_1's l1: 0.307236\n",
      "[362]\ttraining's l1: 0.29059\tvalid_1's l1: 0.306538\n",
      "[363]\ttraining's l1: 0.289855\tvalid_1's l1: 0.305834\n",
      "[364]\ttraining's l1: 0.289132\tvalid_1's l1: 0.305147\n",
      "[365]\ttraining's l1: 0.288424\tvalid_1's l1: 0.304468\n",
      "[366]\ttraining's l1: 0.287722\tvalid_1's l1: 0.303794\n",
      "[367]\ttraining's l1: 0.287031\tvalid_1's l1: 0.303151\n",
      "[368]\ttraining's l1: 0.286351\tvalid_1's l1: 0.302497\n",
      "[369]\ttraining's l1: 0.285668\tvalid_1's l1: 0.301848\n",
      "[370]\ttraining's l1: 0.285034\tvalid_1's l1: 0.30123\n",
      "[371]\ttraining's l1: 0.284375\tvalid_1's l1: 0.300613\n",
      "[372]\ttraining's l1: 0.283728\tvalid_1's l1: 0.299996\n",
      "[373]\ttraining's l1: 0.283092\tvalid_1's l1: 0.299408\n",
      "[374]\ttraining's l1: 0.28245\tvalid_1's l1: 0.298798\n",
      "[375]\ttraining's l1: 0.281857\tvalid_1's l1: 0.298229\n",
      "[376]\ttraining's l1: 0.281239\tvalid_1's l1: 0.297647\n",
      "[377]\ttraining's l1: 0.280622\tvalid_1's l1: 0.297079\n",
      "[378]\ttraining's l1: 0.280049\tvalid_1's l1: 0.296532\n",
      "[379]\ttraining's l1: 0.279487\tvalid_1's l1: 0.296003\n",
      "[380]\ttraining's l1: 0.27888\tvalid_1's l1: 0.295425\n",
      "[381]\ttraining's l1: 0.278295\tvalid_1's l1: 0.294868\n",
      "[382]\ttraining's l1: 0.277696\tvalid_1's l1: 0.294307\n",
      "[383]\ttraining's l1: 0.277148\tvalid_1's l1: 0.2938\n",
      "[384]\ttraining's l1: 0.276618\tvalid_1's l1: 0.293296\n",
      "[385]\ttraining's l1: 0.276056\tvalid_1's l1: 0.292764\n",
      "[386]\ttraining's l1: 0.275509\tvalid_1's l1: 0.292246\n",
      "[387]\ttraining's l1: 0.274984\tvalid_1's l1: 0.291768\n",
      "[388]\ttraining's l1: 0.274436\tvalid_1's l1: 0.291269\n",
      "[389]\ttraining's l1: 0.273901\tvalid_1's l1: 0.290764\n",
      "[390]\ttraining's l1: 0.273388\tvalid_1's l1: 0.290297\n",
      "[391]\ttraining's l1: 0.272872\tvalid_1's l1: 0.289814\n",
      "[392]\ttraining's l1: 0.272381\tvalid_1's l1: 0.289368\n",
      "[393]\ttraining's l1: 0.271881\tvalid_1's l1: 0.288904\n",
      "[394]\ttraining's l1: 0.271409\tvalid_1's l1: 0.288475\n",
      "[395]\ttraining's l1: 0.270932\tvalid_1's l1: 0.288025\n",
      "[396]\ttraining's l1: 0.270459\tvalid_1's l1: 0.287586\n",
      "[397]\ttraining's l1: 0.269962\tvalid_1's l1: 0.287124\n",
      "[398]\ttraining's l1: 0.269499\tvalid_1's l1: 0.2867\n",
      "[399]\ttraining's l1: 0.269046\tvalid_1's l1: 0.286287\n",
      "[400]\ttraining's l1: 0.268572\tvalid_1's l1: 0.285854\n",
      "[401]\ttraining's l1: 0.26813\tvalid_1's l1: 0.285443\n",
      "[402]\ttraining's l1: 0.267661\tvalid_1's l1: 0.28502\n",
      "[403]\ttraining's l1: 0.267207\tvalid_1's l1: 0.284609\n",
      "[404]\ttraining's l1: 0.266748\tvalid_1's l1: 0.284188\n",
      "[405]\ttraining's l1: 0.26633\tvalid_1's l1: 0.283821\n",
      "[406]\ttraining's l1: 0.2659\tvalid_1's l1: 0.283423\n",
      "[407]\ttraining's l1: 0.265472\tvalid_1's l1: 0.283031\n",
      "[408]\ttraining's l1: 0.265063\tvalid_1's l1: 0.282654\n",
      "[409]\ttraining's l1: 0.26466\tvalid_1's l1: 0.282294\n",
      "[410]\ttraining's l1: 0.264235\tvalid_1's l1: 0.281911\n",
      "[411]\ttraining's l1: 0.263827\tvalid_1's l1: 0.281542\n",
      "[412]\ttraining's l1: 0.263443\tvalid_1's l1: 0.281203\n",
      "[413]\ttraining's l1: 0.263043\tvalid_1's l1: 0.280837\n",
      "[414]\ttraining's l1: 0.26265\tvalid_1's l1: 0.280484\n",
      "[415]\ttraining's l1: 0.262262\tvalid_1's l1: 0.280127\n",
      "[416]\ttraining's l1: 0.26188\tvalid_1's l1: 0.279785\n",
      "[417]\ttraining's l1: 0.261492\tvalid_1's l1: 0.279432\n",
      "[418]\ttraining's l1: 0.261127\tvalid_1's l1: 0.279103\n",
      "[419]\ttraining's l1: 0.260767\tvalid_1's l1: 0.278783\n",
      "[420]\ttraining's l1: 0.260398\tvalid_1's l1: 0.278452\n",
      "[421]\ttraining's l1: 0.260054\tvalid_1's l1: 0.278145\n",
      "[422]\ttraining's l1: 0.259698\tvalid_1's l1: 0.277835\n",
      "[423]\ttraining's l1: 0.259336\tvalid_1's l1: 0.277515\n",
      "[424]\ttraining's l1: 0.258988\tvalid_1's l1: 0.277216\n",
      "[425]\ttraining's l1: 0.258629\tvalid_1's l1: 0.276912\n",
      "[426]\ttraining's l1: 0.258289\tvalid_1's l1: 0.276607\n",
      "[427]\ttraining's l1: 0.257976\tvalid_1's l1: 0.276331\n",
      "[428]\ttraining's l1: 0.257636\tvalid_1's l1: 0.276041\n",
      "[429]\ttraining's l1: 0.257299\tvalid_1's l1: 0.275757\n",
      "[430]\ttraining's l1: 0.256965\tvalid_1's l1: 0.275473\n",
      "[431]\ttraining's l1: 0.256626\tvalid_1's l1: 0.27518\n",
      "[432]\ttraining's l1: 0.256312\tvalid_1's l1: 0.274907\n",
      "[433]\ttraining's l1: 0.255984\tvalid_1's l1: 0.27462\n",
      "[434]\ttraining's l1: 0.255678\tvalid_1's l1: 0.274351\n",
      "[435]\ttraining's l1: 0.255376\tvalid_1's l1: 0.274084\n",
      "[436]\ttraining's l1: 0.255047\tvalid_1's l1: 0.273791\n",
      "[437]\ttraining's l1: 0.25473\tvalid_1's l1: 0.273519\n",
      "[438]\ttraining's l1: 0.25441\tvalid_1's l1: 0.273257\n",
      "[439]\ttraining's l1: 0.254136\tvalid_1's l1: 0.273034\n",
      "[440]\ttraining's l1: 0.253817\tvalid_1's l1: 0.272767\n",
      "[441]\ttraining's l1: 0.253512\tvalid_1's l1: 0.272506\n",
      "[442]\ttraining's l1: 0.253204\tvalid_1's l1: 0.272249\n",
      "[443]\ttraining's l1: 0.252919\tvalid_1's l1: 0.272016\n",
      "[444]\ttraining's l1: 0.252652\tvalid_1's l1: 0.271783\n",
      "[445]\ttraining's l1: 0.252361\tvalid_1's l1: 0.271545\n",
      "[446]\ttraining's l1: 0.252121\tvalid_1's l1: 0.271336\n",
      "[447]\ttraining's l1: 0.251827\tvalid_1's l1: 0.271093\n",
      "[448]\ttraining's l1: 0.251577\tvalid_1's l1: 0.27089\n",
      "[449]\ttraining's l1: 0.251295\tvalid_1's l1: 0.27065\n",
      "[450]\ttraining's l1: 0.251053\tvalid_1's l1: 0.27045\n",
      "[451]\ttraining's l1: 0.250768\tvalid_1's l1: 0.270205\n",
      "[452]\ttraining's l1: 0.250491\tvalid_1's l1: 0.269973\n",
      "[453]\ttraining's l1: 0.25023\tvalid_1's l1: 0.269752\n",
      "[454]\ttraining's l1: 0.249996\tvalid_1's l1: 0.269568\n",
      "[455]\ttraining's l1: 0.249732\tvalid_1's l1: 0.269349\n",
      "[456]\ttraining's l1: 0.249473\tvalid_1's l1: 0.26913\n",
      "[457]\ttraining's l1: 0.249227\tvalid_1's l1: 0.268925\n",
      "[458]\ttraining's l1: 0.248985\tvalid_1's l1: 0.268728\n",
      "[459]\ttraining's l1: 0.248727\tvalid_1's l1: 0.268521\n",
      "[460]\ttraining's l1: 0.248483\tvalid_1's l1: 0.268313\n",
      "[461]\ttraining's l1: 0.248233\tvalid_1's l1: 0.268104\n",
      "[462]\ttraining's l1: 0.247986\tvalid_1's l1: 0.267904\n",
      "[463]\ttraining's l1: 0.247784\tvalid_1's l1: 0.267761\n",
      "[464]\ttraining's l1: 0.247557\tvalid_1's l1: 0.267557\n",
      "[465]\ttraining's l1: 0.247322\tvalid_1's l1: 0.26737\n",
      "[466]\ttraining's l1: 0.247088\tvalid_1's l1: 0.267187\n",
      "[467]\ttraining's l1: 0.246857\tvalid_1's l1: 0.267008\n",
      "[468]\ttraining's l1: 0.246663\tvalid_1's l1: 0.266855\n",
      "[469]\ttraining's l1: 0.246434\tvalid_1's l1: 0.266663\n",
      "[470]\ttraining's l1: 0.246218\tvalid_1's l1: 0.266485\n",
      "[471]\ttraining's l1: 0.24602\tvalid_1's l1: 0.26633\n",
      "[472]\ttraining's l1: 0.245806\tvalid_1's l1: 0.26616\n",
      "[473]\ttraining's l1: 0.245626\tvalid_1's l1: 0.266018\n",
      "[474]\ttraining's l1: 0.245408\tvalid_1's l1: 0.26584\n",
      "[475]\ttraining's l1: 0.24524\tvalid_1's l1: 0.265704\n",
      "[476]\ttraining's l1: 0.245078\tvalid_1's l1: 0.26559\n",
      "[477]\ttraining's l1: 0.244857\tvalid_1's l1: 0.265405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[478]\ttraining's l1: 0.244655\tvalid_1's l1: 0.26524\n",
      "[479]\ttraining's l1: 0.24449\tvalid_1's l1: 0.265132\n",
      "[480]\ttraining's l1: 0.244324\tvalid_1's l1: 0.265006\n",
      "[481]\ttraining's l1: 0.244163\tvalid_1's l1: 0.264882\n",
      "[482]\ttraining's l1: 0.243999\tvalid_1's l1: 0.264765\n",
      "[483]\ttraining's l1: 0.243805\tvalid_1's l1: 0.264615\n",
      "[484]\ttraining's l1: 0.243615\tvalid_1's l1: 0.264461\n",
      "[485]\ttraining's l1: 0.243463\tvalid_1's l1: 0.26434\n",
      "[486]\ttraining's l1: 0.243275\tvalid_1's l1: 0.264176\n",
      "[487]\ttraining's l1: 0.243112\tvalid_1's l1: 0.264061\n",
      "[488]\ttraining's l1: 0.242968\tvalid_1's l1: 0.263955\n",
      "[489]\ttraining's l1: 0.242775\tvalid_1's l1: 0.263794\n",
      "[490]\ttraining's l1: 0.242619\tvalid_1's l1: 0.263685\n",
      "[491]\ttraining's l1: 0.242441\tvalid_1's l1: 0.263547\n",
      "[492]\ttraining's l1: 0.242265\tvalid_1's l1: 0.26339\n",
      "[493]\ttraining's l1: 0.242082\tvalid_1's l1: 0.26324\n",
      "[494]\ttraining's l1: 0.241945\tvalid_1's l1: 0.263138\n",
      "[495]\ttraining's l1: 0.241764\tvalid_1's l1: 0.263014\n",
      "[496]\ttraining's l1: 0.24159\tvalid_1's l1: 0.262867\n",
      "[497]\ttraining's l1: 0.241459\tvalid_1's l1: 0.262768\n",
      "[498]\ttraining's l1: 0.241331\tvalid_1's l1: 0.262672\n",
      "[499]\ttraining's l1: 0.241164\tvalid_1's l1: 0.262534\n",
      "[500]\ttraining's l1: 0.241038\tvalid_1's l1: 0.262441\n",
      "[501]\ttraining's l1: 0.240891\tvalid_1's l1: 0.262353\n",
      "[502]\ttraining's l1: 0.240732\tvalid_1's l1: 0.26223\n",
      "[503]\ttraining's l1: 0.240612\tvalid_1's l1: 0.262141\n",
      "[504]\ttraining's l1: 0.24047\tvalid_1's l1: 0.262028\n",
      "[505]\ttraining's l1: 0.240349\tvalid_1's l1: 0.261933\n",
      "[506]\ttraining's l1: 0.240208\tvalid_1's l1: 0.261826\n",
      "[507]\ttraining's l1: 0.24008\tvalid_1's l1: 0.261744\n",
      "[508]\ttraining's l1: 0.239965\tvalid_1's l1: 0.261662\n",
      "[509]\ttraining's l1: 0.239822\tvalid_1's l1: 0.261571\n",
      "[510]\ttraining's l1: 0.239711\tvalid_1's l1: 0.261498\n",
      "[511]\ttraining's l1: 0.239608\tvalid_1's l1: 0.26144\n",
      "[512]\ttraining's l1: 0.239487\tvalid_1's l1: 0.261362\n",
      "[513]\ttraining's l1: 0.239382\tvalid_1's l1: 0.261286\n",
      "[514]\ttraining's l1: 0.239234\tvalid_1's l1: 0.261196\n",
      "[515]\ttraining's l1: 0.239116\tvalid_1's l1: 0.261119\n",
      "[516]\ttraining's l1: 0.238951\tvalid_1's l1: 0.260985\n",
      "[517]\ttraining's l1: 0.238859\tvalid_1's l1: 0.260938\n",
      "[518]\ttraining's l1: 0.238742\tvalid_1's l1: 0.260863\n",
      "[519]\ttraining's l1: 0.238627\tvalid_1's l1: 0.260789\n",
      "[520]\ttraining's l1: 0.238535\tvalid_1's l1: 0.260729\n",
      "[521]\ttraining's l1: 0.238438\tvalid_1's l1: 0.26067\n",
      "[522]\ttraining's l1: 0.238332\tvalid_1's l1: 0.260604\n",
      "[523]\ttraining's l1: 0.238239\tvalid_1's l1: 0.26055\n",
      "[524]\ttraining's l1: 0.238143\tvalid_1's l1: 0.260492\n",
      "[525]\ttraining's l1: 0.238035\tvalid_1's l1: 0.260423\n",
      "[526]\ttraining's l1: 0.237898\tvalid_1's l1: 0.260325\n",
      "[527]\ttraining's l1: 0.237749\tvalid_1's l1: 0.260219\n",
      "[528]\ttraining's l1: 0.237655\tvalid_1's l1: 0.260154\n",
      "[529]\ttraining's l1: 0.237554\tvalid_1's l1: 0.260091\n",
      "[530]\ttraining's l1: 0.237456\tvalid_1's l1: 0.260043\n",
      "[531]\ttraining's l1: 0.237365\tvalid_1's l1: 0.259986\n",
      "[532]\ttraining's l1: 0.23727\tvalid_1's l1: 0.25994\n",
      "[533]\ttraining's l1: 0.237123\tvalid_1's l1: 0.259839\n",
      "[534]\ttraining's l1: 0.237023\tvalid_1's l1: 0.259776\n",
      "[535]\ttraining's l1: 0.236926\tvalid_1's l1: 0.25973\n",
      "[536]\ttraining's l1: 0.236828\tvalid_1's l1: 0.259666\n",
      "[537]\ttraining's l1: 0.236684\tvalid_1's l1: 0.259581\n",
      "[538]\ttraining's l1: 0.236541\tvalid_1's l1: 0.259497\n",
      "[539]\ttraining's l1: 0.236451\tvalid_1's l1: 0.259438\n",
      "[540]\ttraining's l1: 0.236303\tvalid_1's l1: 0.259335\n",
      "[541]\ttraining's l1: 0.236219\tvalid_1's l1: 0.259279\n",
      "[542]\ttraining's l1: 0.236132\tvalid_1's l1: 0.259237\n",
      "[543]\ttraining's l1: 0.236048\tvalid_1's l1: 0.25918\n",
      "[544]\ttraining's l1: 0.23591\tvalid_1's l1: 0.259077\n",
      "[545]\ttraining's l1: 0.235816\tvalid_1's l1: 0.25902\n",
      "[546]\ttraining's l1: 0.235715\tvalid_1's l1: 0.258965\n",
      "[547]\ttraining's l1: 0.235628\tvalid_1's l1: 0.258911\n",
      "[548]\ttraining's l1: 0.2355\tvalid_1's l1: 0.258827\n",
      "[549]\ttraining's l1: 0.235417\tvalid_1's l1: 0.258774\n",
      "[550]\ttraining's l1: 0.235342\tvalid_1's l1: 0.25873\n",
      "[551]\ttraining's l1: 0.235252\tvalid_1's l1: 0.258665\n",
      "[552]\ttraining's l1: 0.235177\tvalid_1's l1: 0.258619\n",
      "[553]\ttraining's l1: 0.235055\tvalid_1's l1: 0.258519\n",
      "[554]\ttraining's l1: 0.234926\tvalid_1's l1: 0.258422\n",
      "[555]\ttraining's l1: 0.234815\tvalid_1's l1: 0.258337\n",
      "[556]\ttraining's l1: 0.234702\tvalid_1's l1: 0.258245\n",
      "[557]\ttraining's l1: 0.234589\tvalid_1's l1: 0.258161\n",
      "[558]\ttraining's l1: 0.234503\tvalid_1's l1: 0.258106\n",
      "[559]\ttraining's l1: 0.234393\tvalid_1's l1: 0.258026\n",
      "[560]\ttraining's l1: 0.234327\tvalid_1's l1: 0.257998\n",
      "[561]\ttraining's l1: 0.2342\tvalid_1's l1: 0.257907\n",
      "[562]\ttraining's l1: 0.234117\tvalid_1's l1: 0.257853\n",
      "[563]\ttraining's l1: 0.234056\tvalid_1's l1: 0.257828\n",
      "[564]\ttraining's l1: 0.233973\tvalid_1's l1: 0.257784\n",
      "[565]\ttraining's l1: 0.233868\tvalid_1's l1: 0.257712\n",
      "[566]\ttraining's l1: 0.233768\tvalid_1's l1: 0.257641\n",
      "[567]\ttraining's l1: 0.233693\tvalid_1's l1: 0.257596\n",
      "[568]\ttraining's l1: 0.233592\tvalid_1's l1: 0.25754\n",
      "[569]\ttraining's l1: 0.233511\tvalid_1's l1: 0.257495\n",
      "[570]\ttraining's l1: 0.233398\tvalid_1's l1: 0.257416\n",
      "[571]\ttraining's l1: 0.233286\tvalid_1's l1: 0.257336\n",
      "[572]\ttraining's l1: 0.233215\tvalid_1's l1: 0.257293\n",
      "[573]\ttraining's l1: 0.233134\tvalid_1's l1: 0.257245\n",
      "[574]\ttraining's l1: 0.23307\tvalid_1's l1: 0.257209\n",
      "[575]\ttraining's l1: 0.232959\tvalid_1's l1: 0.257134\n",
      "[576]\ttraining's l1: 0.232879\tvalid_1's l1: 0.257078\n",
      "[577]\ttraining's l1: 0.232781\tvalid_1's l1: 0.257016\n",
      "[578]\ttraining's l1: 0.23272\tvalid_1's l1: 0.256993\n",
      "[579]\ttraining's l1: 0.232664\tvalid_1's l1: 0.256975\n",
      "[580]\ttraining's l1: 0.232546\tvalid_1's l1: 0.256885\n",
      "[581]\ttraining's l1: 0.232466\tvalid_1's l1: 0.25684\n",
      "[582]\ttraining's l1: 0.23235\tvalid_1's l1: 0.256752\n",
      "[583]\ttraining's l1: 0.232287\tvalid_1's l1: 0.256724\n",
      "[584]\ttraining's l1: 0.232168\tvalid_1's l1: 0.256636\n",
      "[585]\ttraining's l1: 0.232102\tvalid_1's l1: 0.256601\n",
      "[586]\ttraining's l1: 0.232045\tvalid_1's l1: 0.256573\n",
      "[587]\ttraining's l1: 0.231969\tvalid_1's l1: 0.256528\n",
      "[588]\ttraining's l1: 0.231915\tvalid_1's l1: 0.256504\n",
      "[589]\ttraining's l1: 0.231862\tvalid_1's l1: 0.256489\n",
      "[590]\ttraining's l1: 0.231807\tvalid_1's l1: 0.256462\n",
      "[591]\ttraining's l1: 0.231727\tvalid_1's l1: 0.256417\n",
      "[592]\ttraining's l1: 0.231625\tvalid_1's l1: 0.256345\n",
      "[593]\ttraining's l1: 0.231559\tvalid_1's l1: 0.256314\n",
      "[594]\ttraining's l1: 0.231514\tvalid_1's l1: 0.256311\n",
      "[595]\ttraining's l1: 0.231436\tvalid_1's l1: 0.256264\n",
      "[596]\ttraining's l1: 0.231389\tvalid_1's l1: 0.256262\n",
      "[597]\ttraining's l1: 0.231343\tvalid_1's l1: 0.256257\n",
      "[598]\ttraining's l1: 0.231289\tvalid_1's l1: 0.256235\n",
      "[599]\ttraining's l1: 0.231245\tvalid_1's l1: 0.256232\n",
      "[600]\ttraining's l1: 0.231156\tvalid_1's l1: 0.256184\n",
      "[601]\ttraining's l1: 0.231051\tvalid_1's l1: 0.256102\n",
      "[602]\ttraining's l1: 0.230971\tvalid_1's l1: 0.256059\n",
      "[603]\ttraining's l1: 0.230927\tvalid_1's l1: 0.25606\n",
      "[604]\ttraining's l1: 0.230863\tvalid_1's l1: 0.256033\n",
      "[605]\ttraining's l1: 0.230821\tvalid_1's l1: 0.25603\n",
      "[606]\ttraining's l1: 0.230769\tvalid_1's l1: 0.256001\n",
      "[607]\ttraining's l1: 0.230681\tvalid_1's l1: 0.255943\n",
      "[608]\ttraining's l1: 0.230595\tvalid_1's l1: 0.255896\n",
      "[609]\ttraining's l1: 0.230528\tvalid_1's l1: 0.255863\n",
      "[610]\ttraining's l1: 0.230443\tvalid_1's l1: 0.255819\n",
      "[611]\ttraining's l1: 0.230399\tvalid_1's l1: 0.255815\n",
      "[612]\ttraining's l1: 0.230313\tvalid_1's l1: 0.255761\n",
      "[613]\ttraining's l1: 0.230273\tvalid_1's l1: 0.255753\n",
      "[614]\ttraining's l1: 0.230236\tvalid_1's l1: 0.255749\n",
      "[615]\ttraining's l1: 0.230195\tvalid_1's l1: 0.255743\n",
      "[616]\ttraining's l1: 0.230158\tvalid_1's l1: 0.255739\n",
      "[617]\ttraining's l1: 0.230114\tvalid_1's l1: 0.255732\n",
      "[618]\ttraining's l1: 0.230079\tvalid_1's l1: 0.255728\n",
      "[619]\ttraining's l1: 0.230037\tvalid_1's l1: 0.255722\n",
      "[620]\ttraining's l1: 0.229936\tvalid_1's l1: 0.255652\n",
      "[621]\ttraining's l1: 0.229851\tvalid_1's l1: 0.255592\n",
      "[622]\ttraining's l1: 0.22981\tvalid_1's l1: 0.25558\n",
      "[623]\ttraining's l1: 0.229751\tvalid_1's l1: 0.255549\n",
      "[624]\ttraining's l1: 0.229709\tvalid_1's l1: 0.255544\n",
      "[625]\ttraining's l1: 0.229611\tvalid_1's l1: 0.255474\n",
      "[626]\ttraining's l1: 0.229573\tvalid_1's l1: 0.25547\n",
      "[627]\ttraining's l1: 0.229533\tvalid_1's l1: 0.255466\n",
      "[628]\ttraining's l1: 0.229486\tvalid_1's l1: 0.255443\n",
      "[629]\ttraining's l1: 0.229447\tvalid_1's l1: 0.255438\n",
      "[630]\ttraining's l1: 0.229413\tvalid_1's l1: 0.255438\n",
      "[631]\ttraining's l1: 0.229377\tvalid_1's l1: 0.255437\n",
      "[632]\ttraining's l1: 0.229339\tvalid_1's l1: 0.255431\n",
      "[633]\ttraining's l1: 0.229303\tvalid_1's l1: 0.255443\n",
      "[634]\ttraining's l1: 0.22921\tvalid_1's l1: 0.255373\n",
      "[635]\ttraining's l1: 0.229174\tvalid_1's l1: 0.255371\n",
      "[636]\ttraining's l1: 0.229094\tvalid_1's l1: 0.255331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[637]\ttraining's l1: 0.229017\tvalid_1's l1: 0.255288\n",
      "[638]\ttraining's l1: 0.22894\tvalid_1's l1: 0.255248\n",
      "[639]\ttraining's l1: 0.228906\tvalid_1's l1: 0.255247\n",
      "[640]\ttraining's l1: 0.228829\tvalid_1's l1: 0.25521\n",
      "[641]\ttraining's l1: 0.228756\tvalid_1's l1: 0.255166\n",
      "[642]\ttraining's l1: 0.228707\tvalid_1's l1: 0.255151\n",
      "[643]\ttraining's l1: 0.228673\tvalid_1's l1: 0.255151\n",
      "[644]\ttraining's l1: 0.228599\tvalid_1's l1: 0.255111\n",
      "[645]\ttraining's l1: 0.228523\tvalid_1's l1: 0.255059\n",
      "[646]\ttraining's l1: 0.22849\tvalid_1's l1: 0.255058\n",
      "[647]\ttraining's l1: 0.228417\tvalid_1's l1: 0.25502\n",
      "[648]\ttraining's l1: 0.228366\tvalid_1's l1: 0.255013\n",
      "[649]\ttraining's l1: 0.228329\tvalid_1's l1: 0.255006\n",
      "[650]\ttraining's l1: 0.228243\tvalid_1's l1: 0.254964\n",
      "[651]\ttraining's l1: 0.228204\tvalid_1's l1: 0.254952\n",
      "[652]\ttraining's l1: 0.228123\tvalid_1's l1: 0.254915\n",
      "[653]\ttraining's l1: 0.22809\tvalid_1's l1: 0.254924\n",
      "[654]\ttraining's l1: 0.228053\tvalid_1's l1: 0.254915\n",
      "[655]\ttraining's l1: 0.228017\tvalid_1's l1: 0.254941\n",
      "[656]\ttraining's l1: 0.227933\tvalid_1's l1: 0.254885\n",
      "[657]\ttraining's l1: 0.227894\tvalid_1's l1: 0.254876\n",
      "[658]\ttraining's l1: 0.227852\tvalid_1's l1: 0.254884\n",
      "[659]\ttraining's l1: 0.227816\tvalid_1's l1: 0.254912\n",
      "[660]\ttraining's l1: 0.227742\tvalid_1's l1: 0.254863\n",
      "[661]\ttraining's l1: 0.227706\tvalid_1's l1: 0.25485\n",
      "[662]\ttraining's l1: 0.227635\tvalid_1's l1: 0.254806\n",
      "[663]\ttraining's l1: 0.227599\tvalid_1's l1: 0.254793\n",
      "[664]\ttraining's l1: 0.227529\tvalid_1's l1: 0.25475\n",
      "[665]\ttraining's l1: 0.22747\tvalid_1's l1: 0.254716\n",
      "[666]\ttraining's l1: 0.227439\tvalid_1's l1: 0.254715\n",
      "[667]\ttraining's l1: 0.227401\tvalid_1's l1: 0.254708\n",
      "[668]\ttraining's l1: 0.227361\tvalid_1's l1: 0.254698\n",
      "[669]\ttraining's l1: 0.227288\tvalid_1's l1: 0.25466\n",
      "[670]\ttraining's l1: 0.227248\tvalid_1's l1: 0.254649\n",
      "[671]\ttraining's l1: 0.227214\tvalid_1's l1: 0.254645\n",
      "[672]\ttraining's l1: 0.227174\tvalid_1's l1: 0.254632\n",
      "[673]\ttraining's l1: 0.227136\tvalid_1's l1: 0.254619\n",
      "[674]\ttraining's l1: 0.227102\tvalid_1's l1: 0.254616\n",
      "[675]\ttraining's l1: 0.227064\tvalid_1's l1: 0.254615\n",
      "[676]\ttraining's l1: 0.226996\tvalid_1's l1: 0.254578\n",
      "[677]\ttraining's l1: 0.226955\tvalid_1's l1: 0.254567\n",
      "[678]\ttraining's l1: 0.226919\tvalid_1's l1: 0.254562\n",
      "[679]\ttraining's l1: 0.226872\tvalid_1's l1: 0.254547\n",
      "[680]\ttraining's l1: 0.226844\tvalid_1's l1: 0.254547\n",
      "[681]\ttraining's l1: 0.226818\tvalid_1's l1: 0.254544\n",
      "[682]\ttraining's l1: 0.22679\tvalid_1's l1: 0.254542\n",
      "[683]\ttraining's l1: 0.226765\tvalid_1's l1: 0.254543\n",
      "[684]\ttraining's l1: 0.226702\tvalid_1's l1: 0.254507\n",
      "[685]\ttraining's l1: 0.226676\tvalid_1's l1: 0.254503\n",
      "[686]\ttraining's l1: 0.226647\tvalid_1's l1: 0.254517\n",
      "[687]\ttraining's l1: 0.226618\tvalid_1's l1: 0.254527\n",
      "[688]\ttraining's l1: 0.22659\tvalid_1's l1: 0.254529\n",
      "[689]\ttraining's l1: 0.226559\tvalid_1's l1: 0.254534\n",
      "[690]\ttraining's l1: 0.226533\tvalid_1's l1: 0.254534\n",
      "[691]\ttraining's l1: 0.226505\tvalid_1's l1: 0.254547\n",
      "[692]\ttraining's l1: 0.226474\tvalid_1's l1: 0.254539\n",
      "[693]\ttraining's l1: 0.226448\tvalid_1's l1: 0.254549\n",
      "[694]\ttraining's l1: 0.22642\tvalid_1's l1: 0.254574\n",
      "[695]\ttraining's l1: 0.226391\tvalid_1's l1: 0.254591\n",
      "[696]\ttraining's l1: 0.226364\tvalid_1's l1: 0.254604\n",
      "[697]\ttraining's l1: 0.226302\tvalid_1's l1: 0.254574\n",
      "[698]\ttraining's l1: 0.226275\tvalid_1's l1: 0.254571\n",
      "[699]\ttraining's l1: 0.226252\tvalid_1's l1: 0.254567\n",
      "[700]\ttraining's l1: 0.226224\tvalid_1's l1: 0.254576\n",
      "[701]\ttraining's l1: 0.22617\tvalid_1's l1: 0.254542\n",
      "[702]\ttraining's l1: 0.22613\tvalid_1's l1: 0.254525\n",
      "[703]\ttraining's l1: 0.226071\tvalid_1's l1: 0.254492\n",
      "[704]\ttraining's l1: 0.226046\tvalid_1's l1: 0.254509\n",
      "[705]\ttraining's l1: 0.225968\tvalid_1's l1: 0.254453\n",
      "[706]\ttraining's l1: 0.22591\tvalid_1's l1: 0.254417\n",
      "[707]\ttraining's l1: 0.225888\tvalid_1's l1: 0.254425\n",
      "[708]\ttraining's l1: 0.22586\tvalid_1's l1: 0.254438\n",
      "[709]\ttraining's l1: 0.225834\tvalid_1's l1: 0.254433\n",
      "[710]\ttraining's l1: 0.225777\tvalid_1's l1: 0.254397\n",
      "[711]\ttraining's l1: 0.225755\tvalid_1's l1: 0.254407\n",
      "[712]\ttraining's l1: 0.225732\tvalid_1's l1: 0.254415\n",
      "[713]\ttraining's l1: 0.22571\tvalid_1's l1: 0.254423\n",
      "[714]\ttraining's l1: 0.225683\tvalid_1's l1: 0.254434\n",
      "[715]\ttraining's l1: 0.225663\tvalid_1's l1: 0.254444\n",
      "[716]\ttraining's l1: 0.225634\tvalid_1's l1: 0.254439\n",
      "[717]\ttraining's l1: 0.22561\tvalid_1's l1: 0.254435\n",
      "[718]\ttraining's l1: 0.225585\tvalid_1's l1: 0.25445\n",
      "[719]\ttraining's l1: 0.225527\tvalid_1's l1: 0.254424\n",
      "[720]\ttraining's l1: 0.225505\tvalid_1's l1: 0.254439\n",
      "[721]\ttraining's l1: 0.22548\tvalid_1's l1: 0.254437\n",
      "[722]\ttraining's l1: 0.225457\tvalid_1's l1: 0.254453\n",
      "[723]\ttraining's l1: 0.225426\tvalid_1's l1: 0.254443\n",
      "[724]\ttraining's l1: 0.225403\tvalid_1's l1: 0.254443\n",
      "[725]\ttraining's l1: 0.225381\tvalid_1's l1: 0.254477\n",
      "[726]\ttraining's l1: 0.225362\tvalid_1's l1: 0.25451\n",
      "[727]\ttraining's l1: 0.225332\tvalid_1's l1: 0.2545\n",
      "[728]\ttraining's l1: 0.225312\tvalid_1's l1: 0.254537\n",
      "[729]\ttraining's l1: 0.225287\tvalid_1's l1: 0.254548\n",
      "[730]\ttraining's l1: 0.225263\tvalid_1's l1: 0.254546\n",
      "[731]\ttraining's l1: 0.225244\tvalid_1's l1: 0.254582\n",
      "[732]\ttraining's l1: 0.225226\tvalid_1's l1: 0.254619\n",
      "[733]\ttraining's l1: 0.2252\tvalid_1's l1: 0.254624\n",
      "[734]\ttraining's l1: 0.225181\tvalid_1's l1: 0.254662\n",
      "[735]\ttraining's l1: 0.225163\tvalid_1's l1: 0.254695\n",
      "[736]\ttraining's l1: 0.225138\tvalid_1's l1: 0.2547\n",
      "[737]\ttraining's l1: 0.225118\tvalid_1's l1: 0.254734\n",
      "[738]\ttraining's l1: 0.225092\tvalid_1's l1: 0.254737\n",
      "[739]\ttraining's l1: 0.225074\tvalid_1's l1: 0.254771\n",
      "[740]\ttraining's l1: 0.22505\tvalid_1's l1: 0.254781\n",
      "[741]\ttraining's l1: 0.224999\tvalid_1's l1: 0.254776\n",
      "[742]\ttraining's l1: 0.224974\tvalid_1's l1: 0.25478\n",
      "[743]\ttraining's l1: 0.224947\tvalid_1's l1: 0.254786\n",
      "[744]\ttraining's l1: 0.224883\tvalid_1's l1: 0.25474\n",
      "[745]\ttraining's l1: 0.224826\tvalid_1's l1: 0.254722\n",
      "[746]\ttraining's l1: 0.22481\tvalid_1's l1: 0.254757\n",
      "[747]\ttraining's l1: 0.224751\tvalid_1's l1: 0.254724\n",
      "[748]\ttraining's l1: 0.224708\tvalid_1's l1: 0.254696\n",
      "[749]\ttraining's l1: 0.224634\tvalid_1's l1: 0.254651\n",
      "[750]\ttraining's l1: 0.224617\tvalid_1's l1: 0.254678\n",
      "[751]\ttraining's l1: 0.224559\tvalid_1's l1: 0.254639\n",
      "[752]\ttraining's l1: 0.224498\tvalid_1's l1: 0.254602\n",
      "[753]\ttraining's l1: 0.224442\tvalid_1's l1: 0.254562\n",
      "[754]\ttraining's l1: 0.224373\tvalid_1's l1: 0.25452\n",
      "[755]\ttraining's l1: 0.224355\tvalid_1's l1: 0.254545\n",
      "[756]\ttraining's l1: 0.22429\tvalid_1's l1: 0.254505\n",
      "[757]\ttraining's l1: 0.224231\tvalid_1's l1: 0.254462\n",
      "[758]\ttraining's l1: 0.224177\tvalid_1's l1: 0.254421\n",
      "[759]\ttraining's l1: 0.224114\tvalid_1's l1: 0.254374\n",
      "[760]\ttraining's l1: 0.224097\tvalid_1's l1: 0.254402\n",
      "[761]\ttraining's l1: 0.224032\tvalid_1's l1: 0.254366\n",
      "[762]\ttraining's l1: 0.223968\tvalid_1's l1: 0.254333\n",
      "[763]\ttraining's l1: 0.223916\tvalid_1's l1: 0.254295\n",
      "[764]\ttraining's l1: 0.223897\tvalid_1's l1: 0.25432\n",
      "[765]\ttraining's l1: 0.223844\tvalid_1's l1: 0.254289\n",
      "[766]\ttraining's l1: 0.223783\tvalid_1's l1: 0.254255\n",
      "[767]\ttraining's l1: 0.22373\tvalid_1's l1: 0.254233\n",
      "[768]\ttraining's l1: 0.223675\tvalid_1's l1: 0.254192\n",
      "[769]\ttraining's l1: 0.223656\tvalid_1's l1: 0.254216\n",
      "[770]\ttraining's l1: 0.223595\tvalid_1's l1: 0.254182\n",
      "[771]\ttraining's l1: 0.223554\tvalid_1's l1: 0.254164\n",
      "[772]\ttraining's l1: 0.223501\tvalid_1's l1: 0.254128\n",
      "[773]\ttraining's l1: 0.223431\tvalid_1's l1: 0.254095\n",
      "[774]\ttraining's l1: 0.223413\tvalid_1's l1: 0.254125\n",
      "[775]\ttraining's l1: 0.223358\tvalid_1's l1: 0.254089\n",
      "[776]\ttraining's l1: 0.223306\tvalid_1's l1: 0.254062\n",
      "[777]\ttraining's l1: 0.223257\tvalid_1's l1: 0.254037\n",
      "[778]\ttraining's l1: 0.223208\tvalid_1's l1: 0.254009\n",
      "[779]\ttraining's l1: 0.223149\tvalid_1's l1: 0.253971\n",
      "[780]\ttraining's l1: 0.223133\tvalid_1's l1: 0.254\n",
      "[781]\ttraining's l1: 0.223068\tvalid_1's l1: 0.253966\n",
      "[782]\ttraining's l1: 0.22302\tvalid_1's l1: 0.253933\n",
      "[783]\ttraining's l1: 0.222977\tvalid_1's l1: 0.253916\n",
      "[784]\ttraining's l1: 0.222958\tvalid_1's l1: 0.253939\n",
      "[785]\ttraining's l1: 0.222898\tvalid_1's l1: 0.253911\n",
      "[786]\ttraining's l1: 0.222845\tvalid_1's l1: 0.253875\n",
      "[787]\ttraining's l1: 0.222798\tvalid_1's l1: 0.253853\n",
      "[788]\ttraining's l1: 0.222751\tvalid_1's l1: 0.253828\n",
      "[789]\ttraining's l1: 0.22272\tvalid_1's l1: 0.253817\n",
      "[790]\ttraining's l1: 0.222659\tvalid_1's l1: 0.253786\n",
      "[791]\ttraining's l1: 0.222643\tvalid_1's l1: 0.253814\n",
      "[792]\ttraining's l1: 0.222598\tvalid_1's l1: 0.253794\n",
      "[793]\ttraining's l1: 0.222544\tvalid_1's l1: 0.253755\n",
      "[794]\ttraining's l1: 0.222499\tvalid_1's l1: 0.253721\n",
      "[795]\ttraining's l1: 0.222465\tvalid_1's l1: 0.253704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[796]\ttraining's l1: 0.222422\tvalid_1's l1: 0.253686\n",
      "[797]\ttraining's l1: 0.222369\tvalid_1's l1: 0.253652\n",
      "[798]\ttraining's l1: 0.222352\tvalid_1's l1: 0.253683\n",
      "[799]\ttraining's l1: 0.222306\tvalid_1's l1: 0.253657\n",
      "[800]\ttraining's l1: 0.222262\tvalid_1's l1: 0.253629\n",
      "[801]\ttraining's l1: 0.222221\tvalid_1's l1: 0.253606\n",
      "[802]\ttraining's l1: 0.222203\tvalid_1's l1: 0.253638\n",
      "[803]\ttraining's l1: 0.222158\tvalid_1's l1: 0.253618\n",
      "[804]\ttraining's l1: 0.222115\tvalid_1's l1: 0.253596\n",
      "[805]\ttraining's l1: 0.222064\tvalid_1's l1: 0.253565\n",
      "[806]\ttraining's l1: 0.22202\tvalid_1's l1: 0.25353\n",
      "[807]\ttraining's l1: 0.221996\tvalid_1's l1: 0.253526\n",
      "[808]\ttraining's l1: 0.221942\tvalid_1's l1: 0.253502\n",
      "[809]\ttraining's l1: 0.2219\tvalid_1's l1: 0.253484\n",
      "[810]\ttraining's l1: 0.22185\tvalid_1's l1: 0.253464\n",
      "[811]\ttraining's l1: 0.221811\tvalid_1's l1: 0.253458\n",
      "[812]\ttraining's l1: 0.221777\tvalid_1's l1: 0.253438\n",
      "[813]\ttraining's l1: 0.221762\tvalid_1's l1: 0.253473\n",
      "[814]\ttraining's l1: 0.221714\tvalid_1's l1: 0.253443\n",
      "[815]\ttraining's l1: 0.221678\tvalid_1's l1: 0.253458\n",
      "[816]\ttraining's l1: 0.221638\tvalid_1's l1: 0.253428\n",
      "[817]\ttraining's l1: 0.221592\tvalid_1's l1: 0.2534\n",
      "[818]\ttraining's l1: 0.22157\tvalid_1's l1: 0.253395\n",
      "[819]\ttraining's l1: 0.221527\tvalid_1's l1: 0.253379\n",
      "[820]\ttraining's l1: 0.221488\tvalid_1's l1: 0.253353\n",
      "[821]\ttraining's l1: 0.221446\tvalid_1's l1: 0.253338\n",
      "[822]\ttraining's l1: 0.221419\tvalid_1's l1: 0.25333\n",
      "[823]\ttraining's l1: 0.221369\tvalid_1's l1: 0.253309\n",
      "[824]\ttraining's l1: 0.221327\tvalid_1's l1: 0.253279\n",
      "[825]\ttraining's l1: 0.221285\tvalid_1's l1: 0.253266\n",
      "[826]\ttraining's l1: 0.221263\tvalid_1's l1: 0.253261\n",
      "[827]\ttraining's l1: 0.221221\tvalid_1's l1: 0.253236\n",
      "[828]\ttraining's l1: 0.221192\tvalid_1's l1: 0.253224\n",
      "[829]\ttraining's l1: 0.22115\tvalid_1's l1: 0.253201\n",
      "[830]\ttraining's l1: 0.221125\tvalid_1's l1: 0.253189\n",
      "[831]\ttraining's l1: 0.221089\tvalid_1's l1: 0.253213\n",
      "[832]\ttraining's l1: 0.221048\tvalid_1's l1: 0.253187\n",
      "[833]\ttraining's l1: 0.221002\tvalid_1's l1: 0.253168\n",
      "[834]\ttraining's l1: 0.220979\tvalid_1's l1: 0.253165\n",
      "[835]\ttraining's l1: 0.220934\tvalid_1's l1: 0.253155\n",
      "[836]\ttraining's l1: 0.2209\tvalid_1's l1: 0.253143\n",
      "[837]\ttraining's l1: 0.220861\tvalid_1's l1: 0.253128\n",
      "[838]\ttraining's l1: 0.220831\tvalid_1's l1: 0.253149\n",
      "[839]\ttraining's l1: 0.22081\tvalid_1's l1: 0.253143\n",
      "[840]\ttraining's l1: 0.220764\tvalid_1's l1: 0.253131\n",
      "[841]\ttraining's l1: 0.220725\tvalid_1's l1: 0.253115\n",
      "[842]\ttraining's l1: 0.22069\tvalid_1's l1: 0.253104\n",
      "[843]\ttraining's l1: 0.220667\tvalid_1's l1: 0.253102\n",
      "[844]\ttraining's l1: 0.220629\tvalid_1's l1: 0.253093\n",
      "[845]\ttraining's l1: 0.220598\tvalid_1's l1: 0.253075\n",
      "[846]\ttraining's l1: 0.220567\tvalid_1's l1: 0.253097\n",
      "[847]\ttraining's l1: 0.220546\tvalid_1's l1: 0.253091\n",
      "[848]\ttraining's l1: 0.220507\tvalid_1's l1: 0.253077\n",
      "[849]\ttraining's l1: 0.220468\tvalid_1's l1: 0.253059\n",
      "[850]\ttraining's l1: 0.220435\tvalid_1's l1: 0.253049\n",
      "[851]\ttraining's l1: 0.220413\tvalid_1's l1: 0.253039\n",
      "[852]\ttraining's l1: 0.220369\tvalid_1's l1: 0.253022\n",
      "[853]\ttraining's l1: 0.220335\tvalid_1's l1: 0.253002\n",
      "[854]\ttraining's l1: 0.220315\tvalid_1's l1: 0.253002\n",
      "[855]\ttraining's l1: 0.220284\tvalid_1's l1: 0.252989\n",
      "[856]\ttraining's l1: 0.220252\tvalid_1's l1: 0.252974\n",
      "[857]\ttraining's l1: 0.220216\tvalid_1's l1: 0.252966\n",
      "[858]\ttraining's l1: 0.220189\tvalid_1's l1: 0.252954\n",
      "[859]\ttraining's l1: 0.220148\tvalid_1's l1: 0.252943\n",
      "[860]\ttraining's l1: 0.220123\tvalid_1's l1: 0.252943\n",
      "[861]\ttraining's l1: 0.220091\tvalid_1's l1: 0.252929\n",
      "[862]\ttraining's l1: 0.220052\tvalid_1's l1: 0.25292\n",
      "[863]\ttraining's l1: 0.220017\tvalid_1's l1: 0.252901\n",
      "[864]\ttraining's l1: 0.219992\tvalid_1's l1: 0.252914\n",
      "[865]\ttraining's l1: 0.219971\tvalid_1's l1: 0.252918\n",
      "[866]\ttraining's l1: 0.21994\tvalid_1's l1: 0.252902\n",
      "[867]\ttraining's l1: 0.219913\tvalid_1's l1: 0.252887\n",
      "[868]\ttraining's l1: 0.219886\tvalid_1's l1: 0.252869\n",
      "[869]\ttraining's l1: 0.219858\tvalid_1's l1: 0.252855\n",
      "[870]\ttraining's l1: 0.219832\tvalid_1's l1: 0.252842\n",
      "[871]\ttraining's l1: 0.219806\tvalid_1's l1: 0.25286\n",
      "[872]\ttraining's l1: 0.219776\tvalid_1's l1: 0.252849\n",
      "[873]\ttraining's l1: 0.219753\tvalid_1's l1: 0.25285\n",
      "[874]\ttraining's l1: 0.219721\tvalid_1's l1: 0.252851\n",
      "[875]\ttraining's l1: 0.219688\tvalid_1's l1: 0.252835\n",
      "[876]\ttraining's l1: 0.219657\tvalid_1's l1: 0.252829\n",
      "[877]\ttraining's l1: 0.219634\tvalid_1's l1: 0.252821\n",
      "[878]\ttraining's l1: 0.219606\tvalid_1's l1: 0.252807\n",
      "[879]\ttraining's l1: 0.219577\tvalid_1's l1: 0.25282\n",
      "[880]\ttraining's l1: 0.21955\tvalid_1's l1: 0.252809\n",
      "[881]\ttraining's l1: 0.21952\tvalid_1's l1: 0.252808\n",
      "[882]\ttraining's l1: 0.219493\tvalid_1's l1: 0.252794\n",
      "[883]\ttraining's l1: 0.219464\tvalid_1's l1: 0.252789\n",
      "[884]\ttraining's l1: 0.219441\tvalid_1's l1: 0.252783\n",
      "[885]\ttraining's l1: 0.21941\tvalid_1's l1: 0.252766\n",
      "[886]\ttraining's l1: 0.21938\tvalid_1's l1: 0.252767\n",
      "[887]\ttraining's l1: 0.219355\tvalid_1's l1: 0.252767\n",
      "[888]\ttraining's l1: 0.21933\tvalid_1's l1: 0.25276\n",
      "[889]\ttraining's l1: 0.219303\tvalid_1's l1: 0.252758\n",
      "[890]\ttraining's l1: 0.219274\tvalid_1's l1: 0.25275\n",
      "[891]\ttraining's l1: 0.219247\tvalid_1's l1: 0.252749\n",
      "[892]\ttraining's l1: 0.219223\tvalid_1's l1: 0.252743\n",
      "[893]\ttraining's l1: 0.219192\tvalid_1's l1: 0.252731\n",
      "[894]\ttraining's l1: 0.219168\tvalid_1's l1: 0.252718\n",
      "[895]\ttraining's l1: 0.219149\tvalid_1's l1: 0.252717\n",
      "[896]\ttraining's l1: 0.219116\tvalid_1's l1: 0.252707\n",
      "[897]\ttraining's l1: 0.21909\tvalid_1's l1: 0.252696\n",
      "[898]\ttraining's l1: 0.219066\tvalid_1's l1: 0.252696\n",
      "[899]\ttraining's l1: 0.219043\tvalid_1's l1: 0.252685\n",
      "[900]\ttraining's l1: 0.21902\tvalid_1's l1: 0.252679\n",
      "[901]\ttraining's l1: 0.218994\tvalid_1's l1: 0.252669\n",
      "[902]\ttraining's l1: 0.218968\tvalid_1's l1: 0.252664\n",
      "[903]\ttraining's l1: 0.218944\tvalid_1's l1: 0.252651\n",
      "[904]\ttraining's l1: 0.21891\tvalid_1's l1: 0.252637\n",
      "[905]\ttraining's l1: 0.218893\tvalid_1's l1: 0.252641\n",
      "[906]\ttraining's l1: 0.218868\tvalid_1's l1: 0.252632\n",
      "[907]\ttraining's l1: 0.218843\tvalid_1's l1: 0.252634\n",
      "[908]\ttraining's l1: 0.218819\tvalid_1's l1: 0.252621\n",
      "[909]\ttraining's l1: 0.218803\tvalid_1's l1: 0.252628\n",
      "[910]\ttraining's l1: 0.218776\tvalid_1's l1: 0.252613\n",
      "[911]\ttraining's l1: 0.218746\tvalid_1's l1: 0.252604\n",
      "[912]\ttraining's l1: 0.218728\tvalid_1's l1: 0.252615\n",
      "[913]\ttraining's l1: 0.218701\tvalid_1's l1: 0.252601\n",
      "[914]\ttraining's l1: 0.218676\tvalid_1's l1: 0.252598\n",
      "[915]\ttraining's l1: 0.218651\tvalid_1's l1: 0.252587\n",
      "[916]\ttraining's l1: 0.21863\tvalid_1's l1: 0.252587\n",
      "[917]\ttraining's l1: 0.218606\tvalid_1's l1: 0.252569\n",
      "[918]\ttraining's l1: 0.218589\tvalid_1's l1: 0.252582\n",
      "[919]\ttraining's l1: 0.218568\tvalid_1's l1: 0.252582\n",
      "[920]\ttraining's l1: 0.21854\tvalid_1's l1: 0.252569\n",
      "[921]\ttraining's l1: 0.218513\tvalid_1's l1: 0.252552\n",
      "[922]\ttraining's l1: 0.218495\tvalid_1's l1: 0.252551\n",
      "[923]\ttraining's l1: 0.218471\tvalid_1's l1: 0.252535\n",
      "[924]\ttraining's l1: 0.218446\tvalid_1's l1: 0.25252\n",
      "[925]\ttraining's l1: 0.218427\tvalid_1's l1: 0.252521\n",
      "[926]\ttraining's l1: 0.218401\tvalid_1's l1: 0.252524\n",
      "[927]\ttraining's l1: 0.218379\tvalid_1's l1: 0.252517\n",
      "[928]\ttraining's l1: 0.21836\tvalid_1's l1: 0.252518\n",
      "[929]\ttraining's l1: 0.218335\tvalid_1's l1: 0.252517\n",
      "[930]\ttraining's l1: 0.218316\tvalid_1's l1: 0.252517\n",
      "[931]\ttraining's l1: 0.218293\tvalid_1's l1: 0.252506\n",
      "[932]\ttraining's l1: 0.218272\tvalid_1's l1: 0.252511\n",
      "[933]\ttraining's l1: 0.218248\tvalid_1's l1: 0.252502\n",
      "[934]\ttraining's l1: 0.21823\tvalid_1's l1: 0.252499\n",
      "[935]\ttraining's l1: 0.218202\tvalid_1's l1: 0.252482\n",
      "[936]\ttraining's l1: 0.218185\tvalid_1's l1: 0.252478\n",
      "[937]\ttraining's l1: 0.218165\tvalid_1's l1: 0.252471\n",
      "[938]\ttraining's l1: 0.218139\tvalid_1's l1: 0.252472\n",
      "[939]\ttraining's l1: 0.218123\tvalid_1's l1: 0.252471\n",
      "[940]\ttraining's l1: 0.218106\tvalid_1's l1: 0.25247\n",
      "[941]\ttraining's l1: 0.218084\tvalid_1's l1: 0.25246\n",
      "[942]\ttraining's l1: 0.218058\tvalid_1's l1: 0.252451\n",
      "[943]\ttraining's l1: 0.218042\tvalid_1's l1: 0.252449\n",
      "[944]\ttraining's l1: 0.218021\tvalid_1's l1: 0.252444\n",
      "[945]\ttraining's l1: 0.218005\tvalid_1's l1: 0.252442\n",
      "[946]\ttraining's l1: 0.217985\tvalid_1's l1: 0.252432\n",
      "[947]\ttraining's l1: 0.21796\tvalid_1's l1: 0.25243\n",
      "[948]\ttraining's l1: 0.217946\tvalid_1's l1: 0.252432\n",
      "[949]\ttraining's l1: 0.217932\tvalid_1's l1: 0.252424\n",
      "[950]\ttraining's l1: 0.217912\tvalid_1's l1: 0.252411\n",
      "[951]\ttraining's l1: 0.217897\tvalid_1's l1: 0.25241\n",
      "[952]\ttraining's l1: 0.217873\tvalid_1's l1: 0.2524\n",
      "[953]\ttraining's l1: 0.217849\tvalid_1's l1: 0.252393\n",
      "[954]\ttraining's l1: 0.217835\tvalid_1's l1: 0.252395\n",
      "[955]\ttraining's l1: 0.217821\tvalid_1's l1: 0.2524\n",
      "[956]\ttraining's l1: 0.217796\tvalid_1's l1: 0.252395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[957]\ttraining's l1: 0.217777\tvalid_1's l1: 0.252392\n",
      "[958]\ttraining's l1: 0.217762\tvalid_1's l1: 0.252391\n",
      "[959]\ttraining's l1: 0.217749\tvalid_1's l1: 0.252389\n",
      "[960]\ttraining's l1: 0.217726\tvalid_1's l1: 0.252381\n",
      "[961]\ttraining's l1: 0.217709\tvalid_1's l1: 0.252384\n",
      "[962]\ttraining's l1: 0.217688\tvalid_1's l1: 0.252379\n",
      "[963]\ttraining's l1: 0.217673\tvalid_1's l1: 0.252381\n",
      "[964]\ttraining's l1: 0.217646\tvalid_1's l1: 0.252376\n",
      "[965]\ttraining's l1: 0.217631\tvalid_1's l1: 0.252376\n",
      "[966]\ttraining's l1: 0.217618\tvalid_1's l1: 0.252378\n",
      "[967]\ttraining's l1: 0.217604\tvalid_1's l1: 0.252372\n",
      "[968]\ttraining's l1: 0.217584\tvalid_1's l1: 0.252369\n",
      "[969]\ttraining's l1: 0.217567\tvalid_1's l1: 0.252368\n",
      "[970]\ttraining's l1: 0.217548\tvalid_1's l1: 0.252373\n",
      "[971]\ttraining's l1: 0.217526\tvalid_1's l1: 0.252372\n",
      "[972]\ttraining's l1: 0.217508\tvalid_1's l1: 0.252371\n",
      "[973]\ttraining's l1: 0.217488\tvalid_1's l1: 0.252368\n",
      "[974]\ttraining's l1: 0.217472\tvalid_1's l1: 0.25237\n",
      "[975]\ttraining's l1: 0.217451\tvalid_1's l1: 0.252361\n",
      "[976]\ttraining's l1: 0.217427\tvalid_1's l1: 0.252361\n",
      "[977]\ttraining's l1: 0.217413\tvalid_1's l1: 0.252362\n",
      "[978]\ttraining's l1: 0.217395\tvalid_1's l1: 0.25236\n",
      "[979]\ttraining's l1: 0.217377\tvalid_1's l1: 0.252354\n",
      "[980]\ttraining's l1: 0.217359\tvalid_1's l1: 0.25235\n",
      "[981]\ttraining's l1: 0.217338\tvalid_1's l1: 0.252338\n",
      "[982]\ttraining's l1: 0.217319\tvalid_1's l1: 0.252337\n",
      "[983]\ttraining's l1: 0.217304\tvalid_1's l1: 0.25234\n",
      "[984]\ttraining's l1: 0.217284\tvalid_1's l1: 0.252336\n",
      "[985]\ttraining's l1: 0.21726\tvalid_1's l1: 0.252329\n",
      "[986]\ttraining's l1: 0.217243\tvalid_1's l1: 0.252332\n",
      "[987]\ttraining's l1: 0.217225\tvalid_1's l1: 0.252325\n",
      "[988]\ttraining's l1: 0.217208\tvalid_1's l1: 0.252328\n",
      "[989]\ttraining's l1: 0.217195\tvalid_1's l1: 0.252334\n",
      "[990]\ttraining's l1: 0.217175\tvalid_1's l1: 0.252325\n",
      "[991]\ttraining's l1: 0.217157\tvalid_1's l1: 0.252332\n",
      "[992]\ttraining's l1: 0.217139\tvalid_1's l1: 0.252349\n",
      "[993]\ttraining's l1: 0.217127\tvalid_1's l1: 0.252352\n",
      "[994]\ttraining's l1: 0.217115\tvalid_1's l1: 0.252352\n",
      "[995]\ttraining's l1: 0.217099\tvalid_1's l1: 0.252355\n",
      "[996]\ttraining's l1: 0.217082\tvalid_1's l1: 0.252371\n",
      "[997]\ttraining's l1: 0.217069\tvalid_1's l1: 0.252375\n",
      "[998]\ttraining's l1: 0.217056\tvalid_1's l1: 0.25238\n",
      "[999]\ttraining's l1: 0.217041\tvalid_1's l1: 0.252382\n",
      "[1000]\ttraining's l1: 0.217018\tvalid_1's l1: 0.252369\n",
      "[1001]\ttraining's l1: 0.217003\tvalid_1's l1: 0.252371\n",
      "[1002]\ttraining's l1: 0.216983\tvalid_1's l1: 0.252367\n",
      "[1003]\ttraining's l1: 0.216967\tvalid_1's l1: 0.252371\n",
      "[1004]\ttraining's l1: 0.216949\tvalid_1's l1: 0.25236\n",
      "[1005]\ttraining's l1: 0.216923\tvalid_1's l1: 0.252355\n",
      "[1006]\ttraining's l1: 0.216912\tvalid_1's l1: 0.252358\n",
      "[1007]\ttraining's l1: 0.216899\tvalid_1's l1: 0.25236\n",
      "[1008]\ttraining's l1: 0.216883\tvalid_1's l1: 0.252362\n",
      "[1009]\ttraining's l1: 0.216865\tvalid_1's l1: 0.252357\n",
      "[1010]\ttraining's l1: 0.216848\tvalid_1's l1: 0.252356\n",
      "[1011]\ttraining's l1: 0.216837\tvalid_1's l1: 0.25236\n",
      "[1012]\ttraining's l1: 0.216819\tvalid_1's l1: 0.25235\n",
      "[1013]\ttraining's l1: 0.216796\tvalid_1's l1: 0.252348\n",
      "[1014]\ttraining's l1: 0.216785\tvalid_1's l1: 0.252353\n",
      "[1015]\ttraining's l1: 0.216767\tvalid_1's l1: 0.252357\n",
      "[1016]\ttraining's l1: 0.216755\tvalid_1's l1: 0.252359\n",
      "[1017]\ttraining's l1: 0.216739\tvalid_1's l1: 0.252382\n",
      "[1018]\ttraining's l1: 0.216723\tvalid_1's l1: 0.25238\n",
      "[1019]\ttraining's l1: 0.216712\tvalid_1's l1: 0.252381\n",
      "[1020]\ttraining's l1: 0.216696\tvalid_1's l1: 0.252399\n",
      "[1021]\ttraining's l1: 0.216684\tvalid_1's l1: 0.252403\n",
      "[1022]\ttraining's l1: 0.216658\tvalid_1's l1: 0.252393\n",
      "[1023]\ttraining's l1: 0.21664\tvalid_1's l1: 0.252387\n",
      "[1024]\ttraining's l1: 0.216628\tvalid_1's l1: 0.252391\n",
      "[1025]\ttraining's l1: 0.216603\tvalid_1's l1: 0.252383\n",
      "[1026]\ttraining's l1: 0.216582\tvalid_1's l1: 0.252393\n",
      "[1027]\ttraining's l1: 0.216571\tvalid_1's l1: 0.252397\n",
      "[1028]\ttraining's l1: 0.21656\tvalid_1's l1: 0.252402\n",
      "[1029]\ttraining's l1: 0.216535\tvalid_1's l1: 0.2524\n",
      "[1030]\ttraining's l1: 0.216524\tvalid_1's l1: 0.252405\n",
      "[1031]\ttraining's l1: 0.216513\tvalid_1's l1: 0.252403\n",
      "[1032]\ttraining's l1: 0.216502\tvalid_1's l1: 0.252415\n",
      "[1033]\ttraining's l1: 0.216483\tvalid_1's l1: 0.252412\n",
      "[1034]\ttraining's l1: 0.216459\tvalid_1's l1: 0.252403\n",
      "[1035]\ttraining's l1: 0.216446\tvalid_1's l1: 0.252399\n",
      "[1036]\ttraining's l1: 0.21643\tvalid_1's l1: 0.252395\n",
      "[1037]\ttraining's l1: 0.216411\tvalid_1's l1: 0.252397\n",
      "[1038]\ttraining's l1: 0.216398\tvalid_1's l1: 0.252396\n",
      "[1039]\ttraining's l1: 0.216378\tvalid_1's l1: 0.252397\n",
      "[1040]\ttraining's l1: 0.216354\tvalid_1's l1: 0.252391\n",
      "[1041]\ttraining's l1: 0.216342\tvalid_1's l1: 0.252391\n",
      "[1042]\ttraining's l1: 0.216325\tvalid_1's l1: 0.252396\n",
      "[1043]\ttraining's l1: 0.216312\tvalid_1's l1: 0.252403\n",
      "[1044]\ttraining's l1: 0.216294\tvalid_1's l1: 0.252401\n",
      "[1045]\ttraining's l1: 0.21628\tvalid_1's l1: 0.252408\n",
      "[1046]\ttraining's l1: 0.216269\tvalid_1's l1: 0.252412\n",
      "[1047]\ttraining's l1: 0.216244\tvalid_1's l1: 0.252408\n",
      "[1048]\ttraining's l1: 0.216233\tvalid_1's l1: 0.252414\n",
      "[1049]\ttraining's l1: 0.216223\tvalid_1's l1: 0.252417\n",
      "[1050]\ttraining's l1: 0.216201\tvalid_1's l1: 0.252419\n",
      "[1051]\ttraining's l1: 0.216179\tvalid_1's l1: 0.252417\n",
      "[1052]\ttraining's l1: 0.216161\tvalid_1's l1: 0.252416\n",
      "[1053]\ttraining's l1: 0.21615\tvalid_1's l1: 0.252424\n",
      "[1054]\ttraining's l1: 0.216137\tvalid_1's l1: 0.252428\n",
      "[1055]\ttraining's l1: 0.216115\tvalid_1's l1: 0.252435\n",
      "[1056]\ttraining's l1: 0.216101\tvalid_1's l1: 0.252432\n",
      "[1057]\ttraining's l1: 0.216085\tvalid_1's l1: 0.252431\n",
      "[1058]\ttraining's l1: 0.216073\tvalid_1's l1: 0.252444\n",
      "[1059]\ttraining's l1: 0.216052\tvalid_1's l1: 0.252451\n",
      "[1060]\ttraining's l1: 0.216039\tvalid_1's l1: 0.252457\n",
      "[1061]\ttraining's l1: 0.216027\tvalid_1's l1: 0.252456\n",
      "[1062]\ttraining's l1: 0.216007\tvalid_1's l1: 0.252456\n",
      "[1063]\ttraining's l1: 0.215993\tvalid_1's l1: 0.252455\n",
      "[1064]\ttraining's l1: 0.215968\tvalid_1's l1: 0.252456\n",
      "[1065]\ttraining's l1: 0.215953\tvalid_1's l1: 0.252456\n",
      "[1066]\ttraining's l1: 0.215942\tvalid_1's l1: 0.252457\n",
      "[1067]\ttraining's l1: 0.21592\tvalid_1's l1: 0.252458\n",
      "[1068]\ttraining's l1: 0.215907\tvalid_1's l1: 0.25246\n",
      "[1069]\ttraining's l1: 0.215886\tvalid_1's l1: 0.252461\n",
      "[1070]\ttraining's l1: 0.215873\tvalid_1's l1: 0.252456\n",
      "[1071]\ttraining's l1: 0.215858\tvalid_1's l1: 0.252455\n",
      "[1072]\ttraining's l1: 0.215846\tvalid_1's l1: 0.252466\n",
      "[1073]\ttraining's l1: 0.215833\tvalid_1's l1: 0.252471\n",
      "[1074]\ttraining's l1: 0.215818\tvalid_1's l1: 0.252469\n",
      "[1075]\ttraining's l1: 0.215805\tvalid_1's l1: 0.252483\n",
      "[1076]\ttraining's l1: 0.215779\tvalid_1's l1: 0.252481\n",
      "[1077]\ttraining's l1: 0.215769\tvalid_1's l1: 0.252481\n",
      "[1078]\ttraining's l1: 0.215747\tvalid_1's l1: 0.252482\n",
      "[1079]\ttraining's l1: 0.215736\tvalid_1's l1: 0.252489\n",
      "[1080]\ttraining's l1: 0.215722\tvalid_1's l1: 0.252488\n",
      "[1081]\ttraining's l1: 0.215704\tvalid_1's l1: 0.252493\n",
      "[1082]\ttraining's l1: 0.215692\tvalid_1's l1: 0.252495\n",
      "[1083]\ttraining's l1: 0.215667\tvalid_1's l1: 0.252492\n",
      "[1084]\ttraining's l1: 0.215655\tvalid_1's l1: 0.252491\n",
      "[1085]\ttraining's l1: 0.215643\tvalid_1's l1: 0.252503\n",
      "[1086]\ttraining's l1: 0.215627\tvalid_1's l1: 0.252508\n",
      "[1087]\ttraining's l1: 0.215616\tvalid_1's l1: 0.252511\n",
      "[1088]\ttraining's l1: 0.215599\tvalid_1's l1: 0.252516\n",
      "[1089]\ttraining's l1: 0.215575\tvalid_1's l1: 0.252509\n",
      "[1090]\ttraining's l1: 0.215544\tvalid_1's l1: 0.252512\n",
      "Early stopping, best iteration is:\n",
      "[990]\ttraining's l1: 0.217175\tvalid_1's l1: 0.252325\n",
      "The rmlse of prediction is: 0.12019\n",
      "Feature importances: [0, 1469, 2835, 64, 1178, 0, 1051, 2110, 1938, 1115, 628, 455, 48, 179, 850, 4784, 7014, 3591, 0, 2896, 1842, 1626, 1459, 1750, 1380, 1681, 1283, 7978, 1174, 7144, 1234, 7554]\n",
      "The rmlse of prediction is: 0.12325\n"
     ]
    }
   ],
   "source": [
    "get_best_lgb(path,5,N=1450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405264, 36) (24748, 36)\n",
      "[1]\ttraining's l1: 2.02065\tvalid_1's l1: 2.018\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\ttraining's l1: 2.00071\tvalid_1's l1: 1.9982\n",
      "[3]\ttraining's l1: 1.98098\tvalid_1's l1: 1.9786\n",
      "[4]\ttraining's l1: 1.96144\tvalid_1's l1: 1.95919\n",
      "[5]\ttraining's l1: 1.94211\tvalid_1's l1: 1.93999\n",
      "[6]\ttraining's l1: 1.92298\tvalid_1's l1: 1.92098\n",
      "[7]\ttraining's l1: 1.90404\tvalid_1's l1: 1.90217\n",
      "[8]\ttraining's l1: 1.88529\tvalid_1's l1: 1.88356\n",
      "[9]\ttraining's l1: 1.86676\tvalid_1's l1: 1.86514\n",
      "[10]\ttraining's l1: 1.8484\tvalid_1's l1: 1.84693\n",
      "[11]\ttraining's l1: 1.83023\tvalid_1's l1: 1.82891\n",
      "[12]\ttraining's l1: 1.81225\tvalid_1's l1: 1.81107\n",
      "[13]\ttraining's l1: 1.79446\tvalid_1's l1: 1.79342\n",
      "[14]\ttraining's l1: 1.77687\tvalid_1's l1: 1.77595\n",
      "[15]\ttraining's l1: 1.75945\tvalid_1's l1: 1.75865\n",
      "[16]\ttraining's l1: 1.7422\tvalid_1's l1: 1.7415\n",
      "[17]\ttraining's l1: 1.72515\tvalid_1's l1: 1.72455\n",
      "[18]\ttraining's l1: 1.70828\tvalid_1's l1: 1.7078\n",
      "[19]\ttraining's l1: 1.69157\tvalid_1's l1: 1.69123\n",
      "[20]\ttraining's l1: 1.67505\tvalid_1's l1: 1.67482\n",
      "[21]\ttraining's l1: 1.65869\tvalid_1's l1: 1.65859\n",
      "[22]\ttraining's l1: 1.6425\tvalid_1's l1: 1.64253\n",
      "[23]\ttraining's l1: 1.62648\tvalid_1's l1: 1.62664\n",
      "[24]\ttraining's l1: 1.61063\tvalid_1's l1: 1.61093\n",
      "[25]\ttraining's l1: 1.59493\tvalid_1's l1: 1.59537\n",
      "[26]\ttraining's l1: 1.57942\tvalid_1's l1: 1.57998\n",
      "[27]\ttraining's l1: 1.56405\tvalid_1's l1: 1.56473\n",
      "[28]\ttraining's l1: 1.54885\tvalid_1's l1: 1.54967\n",
      "[29]\ttraining's l1: 1.53381\tvalid_1's l1: 1.53475\n",
      "[30]\ttraining's l1: 1.51893\tvalid_1's l1: 1.52\n",
      "[31]\ttraining's l1: 1.50419\tvalid_1's l1: 1.50538\n",
      "[32]\ttraining's l1: 1.48961\tvalid_1's l1: 1.49089\n",
      "[33]\ttraining's l1: 1.47516\tvalid_1's l1: 1.47658\n",
      "[34]\ttraining's l1: 1.46087\tvalid_1's l1: 1.4624\n",
      "[35]\ttraining's l1: 1.44672\tvalid_1's l1: 1.44839\n",
      "[36]\ttraining's l1: 1.43273\tvalid_1's l1: 1.43454\n",
      "[37]\ttraining's l1: 1.41889\tvalid_1's l1: 1.42084\n",
      "[38]\ttraining's l1: 1.40519\tvalid_1's l1: 1.40728\n",
      "[39]\ttraining's l1: 1.39165\tvalid_1's l1: 1.39389\n",
      "[40]\ttraining's l1: 1.37824\tvalid_1's l1: 1.3806\n",
      "[41]\ttraining's l1: 1.36498\tvalid_1's l1: 1.36748\n",
      "[42]\ttraining's l1: 1.35187\tvalid_1's l1: 1.3545\n",
      "[43]\ttraining's l1: 1.3389\tvalid_1's l1: 1.34162\n",
      "[44]\ttraining's l1: 1.32606\tvalid_1's l1: 1.3289\n",
      "[45]\ttraining's l1: 1.31335\tvalid_1's l1: 1.31631\n",
      "[46]\ttraining's l1: 1.30077\tvalid_1's l1: 1.3038\n",
      "[47]\ttraining's l1: 1.28833\tvalid_1's l1: 1.29145\n",
      "[48]\ttraining's l1: 1.27602\tvalid_1's l1: 1.27921\n",
      "[49]\ttraining's l1: 1.26386\tvalid_1's l1: 1.26715\n",
      "[50]\ttraining's l1: 1.25182\tvalid_1's l1: 1.25517\n",
      "[51]\ttraining's l1: 1.23992\tvalid_1's l1: 1.24338\n",
      "[52]\ttraining's l1: 1.22813\tvalid_1's l1: 1.23166\n",
      "[53]\ttraining's l1: 1.21646\tvalid_1's l1: 1.22007\n",
      "[54]\ttraining's l1: 1.20494\tvalid_1's l1: 1.20864\n",
      "[55]\ttraining's l1: 1.19352\tvalid_1's l1: 1.19731\n",
      "[56]\ttraining's l1: 1.18222\tvalid_1's l1: 1.18611\n",
      "[57]\ttraining's l1: 1.17105\tvalid_1's l1: 1.175\n",
      "[58]\ttraining's l1: 1.16\tvalid_1's l1: 1.16404\n",
      "[59]\ttraining's l1: 1.14907\tvalid_1's l1: 1.15319\n",
      "[60]\ttraining's l1: 1.13826\tvalid_1's l1: 1.14243\n",
      "[61]\ttraining's l1: 1.12755\tvalid_1's l1: 1.13181\n",
      "[62]\ttraining's l1: 1.11694\tvalid_1's l1: 1.12126\n",
      "[63]\ttraining's l1: 1.10647\tvalid_1's l1: 1.11084\n",
      "[64]\ttraining's l1: 1.09611\tvalid_1's l1: 1.10054\n",
      "[65]\ttraining's l1: 1.08582\tvalid_1's l1: 1.0903\n",
      "[66]\ttraining's l1: 1.07566\tvalid_1's l1: 1.08018\n",
      "[67]\ttraining's l1: 1.06562\tvalid_1's l1: 1.07019\n",
      "[68]\ttraining's l1: 1.05569\tvalid_1's l1: 1.06032\n",
      "[69]\ttraining's l1: 1.04583\tvalid_1's l1: 1.0505\n",
      "[70]\ttraining's l1: 1.0361\tvalid_1's l1: 1.04081\n",
      "[71]\ttraining's l1: 1.02648\tvalid_1's l1: 1.03123\n",
      "[72]\ttraining's l1: 1.01694\tvalid_1's l1: 1.02173\n",
      "[73]\ttraining's l1: 1.00752\tvalid_1's l1: 1.01237\n",
      "[74]\ttraining's l1: 0.998196\tvalid_1's l1: 1.00307\n",
      "[75]\ttraining's l1: 0.988971\tvalid_1's l1: 0.993883\n",
      "[76]\ttraining's l1: 0.979826\tvalid_1's l1: 0.984797\n",
      "[77]\ttraining's l1: 0.970784\tvalid_1's l1: 0.975786\n",
      "[78]\ttraining's l1: 0.961848\tvalid_1's l1: 0.966888\n",
      "[79]\ttraining's l1: 0.953004\tvalid_1's l1: 0.958079\n",
      "[80]\ttraining's l1: 0.94423\tvalid_1's l1: 0.949345\n",
      "[81]\ttraining's l1: 0.935569\tvalid_1's l1: 0.940749\n",
      "[82]\ttraining's l1: 0.927\tvalid_1's l1: 0.93223\n",
      "[83]\ttraining's l1: 0.918519\tvalid_1's l1: 0.923784\n",
      "[84]\ttraining's l1: 0.91012\tvalid_1's l1: 0.915429\n",
      "[85]\ttraining's l1: 0.901816\tvalid_1's l1: 0.90716\n",
      "[86]\ttraining's l1: 0.893602\tvalid_1's l1: 0.898996\n",
      "[87]\ttraining's l1: 0.885467\tvalid_1's l1: 0.890915\n",
      "[88]\ttraining's l1: 0.877419\tvalid_1's l1: 0.882894\n",
      "[89]\ttraining's l1: 0.869455\tvalid_1's l1: 0.874958\n",
      "[90]\ttraining's l1: 0.861572\tvalid_1's l1: 0.867111\n",
      "[91]\ttraining's l1: 0.853752\tvalid_1's l1: 0.859322\n",
      "[92]\ttraining's l1: 0.846038\tvalid_1's l1: 0.85163\n",
      "[93]\ttraining's l1: 0.838401\tvalid_1's l1: 0.844028\n",
      "[94]\ttraining's l1: 0.830829\tvalid_1's l1: 0.836469\n",
      "[95]\ttraining's l1: 0.823352\tvalid_1's l1: 0.829013\n",
      "[96]\ttraining's l1: 0.815959\tvalid_1's l1: 0.821655\n",
      "[97]\ttraining's l1: 0.808629\tvalid_1's l1: 0.814356\n",
      "[98]\ttraining's l1: 0.801366\tvalid_1's l1: 0.807116\n",
      "[99]\ttraining's l1: 0.794198\tvalid_1's l1: 0.799977\n",
      "[100]\ttraining's l1: 0.787092\tvalid_1's l1: 0.792888\n",
      "[101]\ttraining's l1: 0.780065\tvalid_1's l1: 0.78587\n",
      "[102]\ttraining's l1: 0.773106\tvalid_1's l1: 0.77894\n",
      "[103]\ttraining's l1: 0.766213\tvalid_1's l1: 0.77207\n",
      "[104]\ttraining's l1: 0.759409\tvalid_1's l1: 0.765278\n",
      "[105]\ttraining's l1: 0.752672\tvalid_1's l1: 0.758547\n",
      "[106]\ttraining's l1: 0.746\tvalid_1's l1: 0.751885\n",
      "[107]\ttraining's l1: 0.739404\tvalid_1's l1: 0.745314\n",
      "[108]\ttraining's l1: 0.732876\tvalid_1's l1: 0.738789\n",
      "[109]\ttraining's l1: 0.726414\tvalid_1's l1: 0.732353\n",
      "[110]\ttraining's l1: 0.720012\tvalid_1's l1: 0.725982\n",
      "[111]\ttraining's l1: 0.71369\tvalid_1's l1: 0.719681\n",
      "[112]\ttraining's l1: 0.707427\tvalid_1's l1: 0.713432\n",
      "[113]\ttraining's l1: 0.70123\tvalid_1's l1: 0.707271\n",
      "[114]\ttraining's l1: 0.69509\tvalid_1's l1: 0.701144\n",
      "[115]\ttraining's l1: 0.689016\tvalid_1's l1: 0.695089\n",
      "[116]\ttraining's l1: 0.683023\tvalid_1's l1: 0.689109\n",
      "[117]\ttraining's l1: 0.677083\tvalid_1's l1: 0.683196\n",
      "[118]\ttraining's l1: 0.671204\tvalid_1's l1: 0.677341\n",
      "[119]\ttraining's l1: 0.6654\tvalid_1's l1: 0.671553\n",
      "[120]\ttraining's l1: 0.659639\tvalid_1's l1: 0.665816\n",
      "[121]\ttraining's l1: 0.653938\tvalid_1's l1: 0.660138\n",
      "[122]\ttraining's l1: 0.648312\tvalid_1's l1: 0.654526\n",
      "[123]\ttraining's l1: 0.642741\tvalid_1's l1: 0.64898\n",
      "[124]\ttraining's l1: 0.637226\tvalid_1's l1: 0.643509\n",
      "[125]\ttraining's l1: 0.63176\tvalid_1's l1: 0.638066\n",
      "[126]\ttraining's l1: 0.626368\tvalid_1's l1: 0.632691\n",
      "[127]\ttraining's l1: 0.621029\tvalid_1's l1: 0.627375\n",
      "[128]\ttraining's l1: 0.615733\tvalid_1's l1: 0.622117\n",
      "[129]\ttraining's l1: 0.610501\tvalid_1's l1: 0.616898\n",
      "[130]\ttraining's l1: 0.605333\tvalid_1's l1: 0.611762\n",
      "[131]\ttraining's l1: 0.600208\tvalid_1's l1: 0.606657\n",
      "[132]\ttraining's l1: 0.595142\tvalid_1's l1: 0.601611\n",
      "[133]\ttraining's l1: 0.590137\tvalid_1's l1: 0.596633\n",
      "[134]\ttraining's l1: 0.585177\tvalid_1's l1: 0.591694\n",
      "[135]\ttraining's l1: 0.580277\tvalid_1's l1: 0.586813\n",
      "[136]\ttraining's l1: 0.575432\tvalid_1's l1: 0.581981\n",
      "[137]\ttraining's l1: 0.57063\tvalid_1's l1: 0.577199\n",
      "[138]\ttraining's l1: 0.565891\tvalid_1's l1: 0.572484\n",
      "[139]\ttraining's l1: 0.561201\tvalid_1's l1: 0.567833\n",
      "[140]\ttraining's l1: 0.556554\tvalid_1's l1: 0.563219\n",
      "[141]\ttraining's l1: 0.551967\tvalid_1's l1: 0.558682\n",
      "[142]\ttraining's l1: 0.547435\tvalid_1's l1: 0.55419\n",
      "[143]\ttraining's l1: 0.542948\tvalid_1's l1: 0.549731\n",
      "[144]\ttraining's l1: 0.538503\tvalid_1's l1: 0.545319\n",
      "[145]\ttraining's l1: 0.534108\tvalid_1's l1: 0.540938\n",
      "[146]\ttraining's l1: 0.529768\tvalid_1's l1: 0.536619\n",
      "[147]\ttraining's l1: 0.525471\tvalid_1's l1: 0.532357\n",
      "[148]\ttraining's l1: 0.52122\tvalid_1's l1: 0.52814\n",
      "[149]\ttraining's l1: 0.517018\tvalid_1's l1: 0.523974\n",
      "[150]\ttraining's l1: 0.512855\tvalid_1's l1: 0.519856\n",
      "[151]\ttraining's l1: 0.508737\tvalid_1's l1: 0.515779\n",
      "[152]\ttraining's l1: 0.504667\tvalid_1's l1: 0.511747\n",
      "[153]\ttraining's l1: 0.500629\tvalid_1's l1: 0.507752\n",
      "[154]\ttraining's l1: 0.496641\tvalid_1's l1: 0.503794\n",
      "[155]\ttraining's l1: 0.492702\tvalid_1's l1: 0.499899\n",
      "[156]\ttraining's l1: 0.488811\tvalid_1's l1: 0.49605\n",
      "[157]\ttraining's l1: 0.484951\tvalid_1's l1: 0.492233\n",
      "[158]\ttraining's l1: 0.48114\tvalid_1's l1: 0.488474\n",
      "[159]\ttraining's l1: 0.477379\tvalid_1's l1: 0.484745\n",
      "[160]\ttraining's l1: 0.473647\tvalid_1's l1: 0.481038\n",
      "[161]\ttraining's l1: 0.469959\tvalid_1's l1: 0.477388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[162]\ttraining's l1: 0.466307\tvalid_1's l1: 0.473759\n",
      "[163]\ttraining's l1: 0.462698\tvalid_1's l1: 0.47018\n",
      "[164]\ttraining's l1: 0.459135\tvalid_1's l1: 0.466649\n",
      "[165]\ttraining's l1: 0.455604\tvalid_1's l1: 0.463154\n",
      "[166]\ttraining's l1: 0.452115\tvalid_1's l1: 0.459687\n",
      "[167]\ttraining's l1: 0.448657\tvalid_1's l1: 0.456264\n",
      "[168]\ttraining's l1: 0.445248\tvalid_1's l1: 0.452878\n",
      "[169]\ttraining's l1: 0.44187\tvalid_1's l1: 0.449535\n",
      "[170]\ttraining's l1: 0.438533\tvalid_1's l1: 0.446216\n",
      "[171]\ttraining's l1: 0.435228\tvalid_1's l1: 0.442932\n",
      "[172]\ttraining's l1: 0.431968\tvalid_1's l1: 0.439704\n",
      "[173]\ttraining's l1: 0.428737\tvalid_1's l1: 0.436502\n",
      "[174]\ttraining's l1: 0.425546\tvalid_1's l1: 0.433336\n",
      "[175]\ttraining's l1: 0.422387\tvalid_1's l1: 0.430207\n",
      "[176]\ttraining's l1: 0.419259\tvalid_1's l1: 0.427114\n",
      "[177]\ttraining's l1: 0.416169\tvalid_1's l1: 0.424059\n",
      "[178]\ttraining's l1: 0.41311\tvalid_1's l1: 0.421036\n",
      "[179]\ttraining's l1: 0.410089\tvalid_1's l1: 0.418044\n",
      "[180]\ttraining's l1: 0.407099\tvalid_1's l1: 0.415083\n",
      "[181]\ttraining's l1: 0.404134\tvalid_1's l1: 0.412155\n",
      "[182]\ttraining's l1: 0.401208\tvalid_1's l1: 0.409262\n",
      "[183]\ttraining's l1: 0.398312\tvalid_1's l1: 0.406403\n",
      "[184]\ttraining's l1: 0.395446\tvalid_1's l1: 0.403563\n",
      "[185]\ttraining's l1: 0.392619\tvalid_1's l1: 0.400773\n",
      "[186]\ttraining's l1: 0.389816\tvalid_1's l1: 0.398005\n",
      "[187]\ttraining's l1: 0.387047\tvalid_1's l1: 0.395273\n",
      "[188]\ttraining's l1: 0.384303\tvalid_1's l1: 0.392569\n",
      "[189]\ttraining's l1: 0.381594\tvalid_1's l1: 0.38988\n",
      "[190]\ttraining's l1: 0.378912\tvalid_1's l1: 0.387227\n",
      "[191]\ttraining's l1: 0.376261\tvalid_1's l1: 0.384595\n",
      "[192]\ttraining's l1: 0.373635\tvalid_1's l1: 0.382002\n",
      "[193]\ttraining's l1: 0.371036\tvalid_1's l1: 0.379427\n",
      "[194]\ttraining's l1: 0.368472\tvalid_1's l1: 0.376896\n",
      "[195]\ttraining's l1: 0.365932\tvalid_1's l1: 0.374385\n",
      "[196]\ttraining's l1: 0.363419\tvalid_1's l1: 0.371907\n",
      "[197]\ttraining's l1: 0.36094\tvalid_1's l1: 0.369468\n",
      "[198]\ttraining's l1: 0.358482\tvalid_1's l1: 0.367043\n",
      "[199]\ttraining's l1: 0.356055\tvalid_1's l1: 0.364648\n",
      "[200]\ttraining's l1: 0.35365\tvalid_1's l1: 0.362286\n",
      "[201]\ttraining's l1: 0.35127\tvalid_1's l1: 0.359946\n",
      "[202]\ttraining's l1: 0.34892\tvalid_1's l1: 0.357639\n",
      "[203]\ttraining's l1: 0.346592\tvalid_1's l1: 0.355343\n",
      "[204]\ttraining's l1: 0.344295\tvalid_1's l1: 0.353073\n",
      "[205]\ttraining's l1: 0.342015\tvalid_1's l1: 0.350841\n",
      "[206]\ttraining's l1: 0.339763\tvalid_1's l1: 0.348641\n",
      "[207]\ttraining's l1: 0.337536\tvalid_1's l1: 0.346449\n",
      "[208]\ttraining's l1: 0.335334\tvalid_1's l1: 0.344292\n",
      "[209]\ttraining's l1: 0.333159\tvalid_1's l1: 0.342152\n",
      "[210]\ttraining's l1: 0.331007\tvalid_1's l1: 0.340052\n",
      "[211]\ttraining's l1: 0.328879\tvalid_1's l1: 0.337961\n",
      "[212]\ttraining's l1: 0.326782\tvalid_1's l1: 0.335903\n",
      "[213]\ttraining's l1: 0.324703\tvalid_1's l1: 0.33386\n",
      "[214]\ttraining's l1: 0.322648\tvalid_1's l1: 0.331836\n",
      "[215]\ttraining's l1: 0.320618\tvalid_1's l1: 0.329844\n",
      "[216]\ttraining's l1: 0.318612\tvalid_1's l1: 0.327872\n",
      "[217]\ttraining's l1: 0.316626\tvalid_1's l1: 0.325927\n",
      "[218]\ttraining's l1: 0.314662\tvalid_1's l1: 0.324002\n",
      "[219]\ttraining's l1: 0.312738\tvalid_1's l1: 0.322119\n",
      "[220]\ttraining's l1: 0.310816\tvalid_1's l1: 0.320237\n",
      "[221]\ttraining's l1: 0.308915\tvalid_1's l1: 0.31837\n",
      "[222]\ttraining's l1: 0.307034\tvalid_1's l1: 0.316531\n",
      "[223]\ttraining's l1: 0.305178\tvalid_1's l1: 0.314704\n",
      "[224]\ttraining's l1: 0.303343\tvalid_1's l1: 0.312917\n",
      "[225]\ttraining's l1: 0.301539\tvalid_1's l1: 0.311156\n",
      "[226]\ttraining's l1: 0.299758\tvalid_1's l1: 0.309406\n",
      "[227]\ttraining's l1: 0.297994\tvalid_1's l1: 0.307681\n",
      "[228]\ttraining's l1: 0.29625\tvalid_1's l1: 0.305967\n",
      "[229]\ttraining's l1: 0.294515\tvalid_1's l1: 0.304271\n",
      "[230]\ttraining's l1: 0.292809\tvalid_1's l1: 0.302598\n",
      "[231]\ttraining's l1: 0.291124\tvalid_1's l1: 0.300965\n",
      "[232]\ttraining's l1: 0.289457\tvalid_1's l1: 0.29934\n",
      "[233]\ttraining's l1: 0.287813\tvalid_1's l1: 0.297724\n",
      "[234]\ttraining's l1: 0.286177\tvalid_1's l1: 0.296133\n",
      "[235]\ttraining's l1: 0.284568\tvalid_1's l1: 0.294554\n",
      "[236]\ttraining's l1: 0.282974\tvalid_1's l1: 0.293001\n",
      "[237]\ttraining's l1: 0.28139\tvalid_1's l1: 0.291463\n",
      "[238]\ttraining's l1: 0.279828\tvalid_1's l1: 0.289934\n",
      "[239]\ttraining's l1: 0.278288\tvalid_1's l1: 0.288455\n",
      "[240]\ttraining's l1: 0.276762\tvalid_1's l1: 0.286958\n",
      "[241]\ttraining's l1: 0.275253\tvalid_1's l1: 0.285489\n",
      "[242]\ttraining's l1: 0.273753\tvalid_1's l1: 0.284035\n",
      "[243]\ttraining's l1: 0.272279\tvalid_1's l1: 0.282594\n",
      "[244]\ttraining's l1: 0.270817\tvalid_1's l1: 0.281186\n",
      "[245]\ttraining's l1: 0.269372\tvalid_1's l1: 0.279787\n",
      "[246]\ttraining's l1: 0.267941\tvalid_1's l1: 0.278399\n",
      "[247]\ttraining's l1: 0.266534\tvalid_1's l1: 0.277033\n",
      "[248]\ttraining's l1: 0.26514\tvalid_1's l1: 0.275678\n",
      "[249]\ttraining's l1: 0.263758\tvalid_1's l1: 0.274348\n",
      "[250]\ttraining's l1: 0.262406\tvalid_1's l1: 0.273039\n",
      "[251]\ttraining's l1: 0.26106\tvalid_1's l1: 0.271735\n",
      "[252]\ttraining's l1: 0.259729\tvalid_1's l1: 0.27045\n",
      "[253]\ttraining's l1: 0.258422\tvalid_1's l1: 0.269191\n",
      "[254]\ttraining's l1: 0.257123\tvalid_1's l1: 0.267945\n",
      "[255]\ttraining's l1: 0.255844\tvalid_1's l1: 0.2667\n",
      "[256]\ttraining's l1: 0.254564\tvalid_1's l1: 0.26546\n",
      "[257]\ttraining's l1: 0.253305\tvalid_1's l1: 0.264242\n",
      "[258]\ttraining's l1: 0.252055\tvalid_1's l1: 0.263034\n",
      "[259]\ttraining's l1: 0.250818\tvalid_1's l1: 0.261858\n",
      "[260]\ttraining's l1: 0.249601\tvalid_1's l1: 0.260691\n",
      "[261]\ttraining's l1: 0.248393\tvalid_1's l1: 0.259549\n",
      "[262]\ttraining's l1: 0.247211\tvalid_1's l1: 0.258409\n",
      "[263]\ttraining's l1: 0.246042\tvalid_1's l1: 0.257278\n",
      "[264]\ttraining's l1: 0.244875\tvalid_1's l1: 0.256158\n",
      "[265]\ttraining's l1: 0.243722\tvalid_1's l1: 0.255049\n",
      "[266]\ttraining's l1: 0.242587\tvalid_1's l1: 0.253961\n",
      "[267]\ttraining's l1: 0.241468\tvalid_1's l1: 0.252887\n",
      "[268]\ttraining's l1: 0.240349\tvalid_1's l1: 0.251819\n",
      "[269]\ttraining's l1: 0.239245\tvalid_1's l1: 0.250768\n",
      "[270]\ttraining's l1: 0.238158\tvalid_1's l1: 0.249726\n",
      "[271]\ttraining's l1: 0.237094\tvalid_1's l1: 0.248713\n",
      "[272]\ttraining's l1: 0.236042\tvalid_1's l1: 0.247704\n",
      "[273]\ttraining's l1: 0.234987\tvalid_1's l1: 0.2467\n",
      "[274]\ttraining's l1: 0.233945\tvalid_1's l1: 0.245706\n",
      "[275]\ttraining's l1: 0.232927\tvalid_1's l1: 0.244731\n",
      "[276]\ttraining's l1: 0.231908\tvalid_1's l1: 0.243754\n",
      "[277]\ttraining's l1: 0.230913\tvalid_1's l1: 0.2428\n",
      "[278]\ttraining's l1: 0.229924\tvalid_1's l1: 0.241855\n",
      "[279]\ttraining's l1: 0.228944\tvalid_1's l1: 0.240913\n",
      "[280]\ttraining's l1: 0.227991\tvalid_1's l1: 0.239999\n",
      "[281]\ttraining's l1: 0.227048\tvalid_1's l1: 0.239086\n",
      "[282]\ttraining's l1: 0.226096\tvalid_1's l1: 0.238164\n",
      "[283]\ttraining's l1: 0.225158\tvalid_1's l1: 0.23727\n",
      "[284]\ttraining's l1: 0.22425\tvalid_1's l1: 0.236406\n",
      "[285]\ttraining's l1: 0.223351\tvalid_1's l1: 0.235543\n",
      "[286]\ttraining's l1: 0.222444\tvalid_1's l1: 0.234676\n",
      "[287]\ttraining's l1: 0.221546\tvalid_1's l1: 0.23381\n",
      "[288]\ttraining's l1: 0.220664\tvalid_1's l1: 0.232967\n",
      "[289]\ttraining's l1: 0.219804\tvalid_1's l1: 0.232153\n",
      "[290]\ttraining's l1: 0.218941\tvalid_1's l1: 0.231322\n",
      "[291]\ttraining's l1: 0.218093\tvalid_1's l1: 0.230503\n",
      "[292]\ttraining's l1: 0.217248\tvalid_1's l1: 0.22969\n",
      "[293]\ttraining's l1: 0.216427\tvalid_1's l1: 0.228899\n",
      "[294]\ttraining's l1: 0.215601\tvalid_1's l1: 0.228114\n",
      "[295]\ttraining's l1: 0.214804\tvalid_1's l1: 0.227347\n",
      "[296]\ttraining's l1: 0.213995\tvalid_1's l1: 0.226582\n",
      "[297]\ttraining's l1: 0.213198\tvalid_1's l1: 0.225823\n",
      "[298]\ttraining's l1: 0.212428\tvalid_1's l1: 0.225087\n",
      "[299]\ttraining's l1: 0.211659\tvalid_1's l1: 0.224361\n",
      "[300]\ttraining's l1: 0.210892\tvalid_1's l1: 0.223633\n",
      "[301]\ttraining's l1: 0.210131\tvalid_1's l1: 0.222915\n",
      "[302]\ttraining's l1: 0.209395\tvalid_1's l1: 0.222222\n",
      "[303]\ttraining's l1: 0.208664\tvalid_1's l1: 0.22153\n",
      "[304]\ttraining's l1: 0.207932\tvalid_1's l1: 0.220833\n",
      "[305]\ttraining's l1: 0.207208\tvalid_1's l1: 0.220147\n",
      "[306]\ttraining's l1: 0.206505\tvalid_1's l1: 0.219472\n",
      "[307]\ttraining's l1: 0.205788\tvalid_1's l1: 0.218798\n",
      "[308]\ttraining's l1: 0.205091\tvalid_1's l1: 0.218137\n",
      "[309]\ttraining's l1: 0.204406\tvalid_1's l1: 0.217479\n",
      "[310]\ttraining's l1: 0.203711\tvalid_1's l1: 0.21684\n",
      "[311]\ttraining's l1: 0.203036\tvalid_1's l1: 0.216196\n",
      "[312]\ttraining's l1: 0.202372\tvalid_1's l1: 0.215567\n",
      "[313]\ttraining's l1: 0.201726\tvalid_1's l1: 0.214956\n",
      "[314]\ttraining's l1: 0.201066\tvalid_1's l1: 0.214345\n",
      "[315]\ttraining's l1: 0.20042\tvalid_1's l1: 0.213736\n",
      "[316]\ttraining's l1: 0.199794\tvalid_1's l1: 0.213136\n",
      "[317]\ttraining's l1: 0.199175\tvalid_1's l1: 0.212566\n",
      "[318]\ttraining's l1: 0.198551\tvalid_1's l1: 0.211975\n",
      "[319]\ttraining's l1: 0.197931\tvalid_1's l1: 0.211404\n",
      "[320]\ttraining's l1: 0.197336\tvalid_1's l1: 0.210837\n",
      "[321]\ttraining's l1: 0.196738\tvalid_1's l1: 0.210264\n",
      "[322]\ttraining's l1: 0.196157\tvalid_1's l1: 0.209714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[323]\ttraining's l1: 0.195569\tvalid_1's l1: 0.209162\n",
      "[324]\ttraining's l1: 0.19499\tvalid_1's l1: 0.208604\n",
      "[325]\ttraining's l1: 0.194434\tvalid_1's l1: 0.208074\n",
      "[326]\ttraining's l1: 0.193876\tvalid_1's l1: 0.207549\n",
      "[327]\ttraining's l1: 0.193314\tvalid_1's l1: 0.207008\n",
      "[328]\ttraining's l1: 0.192774\tvalid_1's l1: 0.206498\n",
      "[329]\ttraining's l1: 0.19223\tvalid_1's l1: 0.205989\n",
      "[330]\ttraining's l1: 0.191691\tvalid_1's l1: 0.205486\n",
      "[331]\ttraining's l1: 0.191167\tvalid_1's l1: 0.205004\n",
      "[332]\ttraining's l1: 0.190651\tvalid_1's l1: 0.204519\n",
      "[333]\ttraining's l1: 0.190145\tvalid_1's l1: 0.20404\n",
      "[334]\ttraining's l1: 0.189635\tvalid_1's l1: 0.203557\n",
      "[335]\ttraining's l1: 0.189131\tvalid_1's l1: 0.203084\n",
      "[336]\ttraining's l1: 0.188641\tvalid_1's l1: 0.202634\n",
      "[337]\ttraining's l1: 0.188154\tvalid_1's l1: 0.20217\n",
      "[338]\ttraining's l1: 0.187676\tvalid_1's l1: 0.201727\n",
      "[339]\ttraining's l1: 0.187196\tvalid_1's l1: 0.201285\n",
      "[340]\ttraining's l1: 0.186729\tvalid_1's l1: 0.20087\n",
      "[341]\ttraining's l1: 0.186263\tvalid_1's l1: 0.200432\n",
      "[342]\ttraining's l1: 0.185807\tvalid_1's l1: 0.200005\n",
      "[343]\ttraining's l1: 0.18536\tvalid_1's l1: 0.199589\n",
      "[344]\ttraining's l1: 0.184911\tvalid_1's l1: 0.19917\n",
      "[345]\ttraining's l1: 0.184471\tvalid_1's l1: 0.198777\n",
      "[346]\ttraining's l1: 0.18403\tvalid_1's l1: 0.198364\n",
      "[347]\ttraining's l1: 0.183594\tvalid_1's l1: 0.197956\n",
      "[348]\ttraining's l1: 0.18317\tvalid_1's l1: 0.197579\n",
      "[349]\ttraining's l1: 0.182754\tvalid_1's l1: 0.197203\n",
      "[350]\ttraining's l1: 0.182342\tvalid_1's l1: 0.196826\n",
      "[351]\ttraining's l1: 0.181934\tvalid_1's l1: 0.196452\n",
      "[352]\ttraining's l1: 0.181532\tvalid_1's l1: 0.196078\n",
      "[353]\ttraining's l1: 0.181132\tvalid_1's l1: 0.195713\n",
      "[354]\ttraining's l1: 0.180744\tvalid_1's l1: 0.195349\n",
      "[355]\ttraining's l1: 0.180358\tvalid_1's l1: 0.194991\n",
      "[356]\ttraining's l1: 0.179968\tvalid_1's l1: 0.194622\n",
      "[357]\ttraining's l1: 0.179592\tvalid_1's l1: 0.19428\n",
      "[358]\ttraining's l1: 0.179215\tvalid_1's l1: 0.193938\n",
      "[359]\ttraining's l1: 0.178846\tvalid_1's l1: 0.193589\n",
      "[360]\ttraining's l1: 0.178483\tvalid_1's l1: 0.193249\n",
      "[361]\ttraining's l1: 0.178123\tvalid_1's l1: 0.192918\n",
      "[362]\ttraining's l1: 0.177774\tvalid_1's l1: 0.19259\n",
      "[363]\ttraining's l1: 0.177425\tvalid_1's l1: 0.192266\n",
      "[364]\ttraining's l1: 0.177081\tvalid_1's l1: 0.191947\n",
      "[365]\ttraining's l1: 0.176756\tvalid_1's l1: 0.191662\n",
      "[366]\ttraining's l1: 0.176421\tvalid_1's l1: 0.191363\n",
      "[367]\ttraining's l1: 0.176091\tvalid_1's l1: 0.191057\n",
      "[368]\ttraining's l1: 0.175765\tvalid_1's l1: 0.190763\n",
      "[369]\ttraining's l1: 0.175445\tvalid_1's l1: 0.190465\n",
      "[370]\ttraining's l1: 0.175127\tvalid_1's l1: 0.190178\n",
      "[371]\ttraining's l1: 0.174814\tvalid_1's l1: 0.18989\n",
      "[372]\ttraining's l1: 0.174499\tvalid_1's l1: 0.189613\n",
      "[373]\ttraining's l1: 0.174186\tvalid_1's l1: 0.189323\n",
      "[374]\ttraining's l1: 0.173876\tvalid_1's l1: 0.189043\n",
      "[375]\ttraining's l1: 0.173574\tvalid_1's l1: 0.188773\n",
      "[376]\ttraining's l1: 0.173274\tvalid_1's l1: 0.188507\n",
      "[377]\ttraining's l1: 0.172979\tvalid_1's l1: 0.18825\n",
      "[378]\ttraining's l1: 0.172691\tvalid_1's l1: 0.187998\n",
      "[379]\ttraining's l1: 0.172402\tvalid_1's l1: 0.187745\n",
      "[380]\ttraining's l1: 0.172134\tvalid_1's l1: 0.187502\n",
      "[381]\ttraining's l1: 0.171857\tvalid_1's l1: 0.187255\n",
      "[382]\ttraining's l1: 0.171571\tvalid_1's l1: 0.186996\n",
      "[383]\ttraining's l1: 0.171302\tvalid_1's l1: 0.186758\n",
      "[384]\ttraining's l1: 0.171046\tvalid_1's l1: 0.186532\n",
      "[385]\ttraining's l1: 0.170783\tvalid_1's l1: 0.186307\n",
      "[386]\ttraining's l1: 0.170521\tvalid_1's l1: 0.186077\n",
      "[387]\ttraining's l1: 0.17026\tvalid_1's l1: 0.185842\n",
      "[388]\ttraining's l1: 0.170008\tvalid_1's l1: 0.185621\n",
      "[389]\ttraining's l1: 0.169752\tvalid_1's l1: 0.18539\n",
      "[390]\ttraining's l1: 0.169521\tvalid_1's l1: 0.185193\n",
      "[391]\ttraining's l1: 0.169276\tvalid_1's l1: 0.184981\n",
      "[392]\ttraining's l1: 0.16903\tvalid_1's l1: 0.18477\n",
      "[393]\ttraining's l1: 0.168795\tvalid_1's l1: 0.18458\n",
      "[394]\ttraining's l1: 0.168555\tvalid_1's l1: 0.18438\n",
      "[395]\ttraining's l1: 0.168322\tvalid_1's l1: 0.184178\n",
      "[396]\ttraining's l1: 0.168085\tvalid_1's l1: 0.18398\n",
      "[397]\ttraining's l1: 0.167869\tvalid_1's l1: 0.183798\n",
      "[398]\ttraining's l1: 0.167647\tvalid_1's l1: 0.183606\n",
      "[399]\ttraining's l1: 0.167422\tvalid_1's l1: 0.183415\n",
      "[400]\ttraining's l1: 0.167203\tvalid_1's l1: 0.183236\n",
      "[401]\ttraining's l1: 0.166985\tvalid_1's l1: 0.183044\n",
      "[402]\ttraining's l1: 0.166782\tvalid_1's l1: 0.182883\n",
      "[403]\ttraining's l1: 0.166566\tvalid_1's l1: 0.182699\n",
      "[404]\ttraining's l1: 0.166355\tvalid_1's l1: 0.182532\n",
      "[405]\ttraining's l1: 0.166166\tvalid_1's l1: 0.182369\n",
      "[406]\ttraining's l1: 0.165968\tvalid_1's l1: 0.182203\n",
      "[407]\ttraining's l1: 0.165787\tvalid_1's l1: 0.182051\n",
      "[408]\ttraining's l1: 0.165583\tvalid_1's l1: 0.181887\n",
      "[409]\ttraining's l1: 0.1654\tvalid_1's l1: 0.181743\n",
      "[410]\ttraining's l1: 0.165202\tvalid_1's l1: 0.181581\n",
      "[411]\ttraining's l1: 0.16503\tvalid_1's l1: 0.181432\n",
      "[412]\ttraining's l1: 0.164853\tvalid_1's l1: 0.181293\n",
      "[413]\ttraining's l1: 0.164661\tvalid_1's l1: 0.18113\n",
      "[414]\ttraining's l1: 0.164495\tvalid_1's l1: 0.18099\n",
      "[415]\ttraining's l1: 0.164306\tvalid_1's l1: 0.180836\n",
      "[416]\ttraining's l1: 0.164146\tvalid_1's l1: 0.180709\n",
      "[417]\ttraining's l1: 0.163972\tvalid_1's l1: 0.180562\n",
      "[418]\ttraining's l1: 0.163814\tvalid_1's l1: 0.180432\n",
      "[419]\ttraining's l1: 0.163633\tvalid_1's l1: 0.180281\n",
      "[420]\ttraining's l1: 0.163458\tvalid_1's l1: 0.18014\n",
      "[421]\ttraining's l1: 0.163287\tvalid_1's l1: 0.179995\n",
      "[422]\ttraining's l1: 0.163135\tvalid_1's l1: 0.179863\n",
      "[423]\ttraining's l1: 0.162964\tvalid_1's l1: 0.179728\n",
      "[424]\ttraining's l1: 0.162805\tvalid_1's l1: 0.17961\n",
      "[425]\ttraining's l1: 0.162638\tvalid_1's l1: 0.179477\n",
      "[426]\ttraining's l1: 0.162474\tvalid_1's l1: 0.179347\n",
      "[427]\ttraining's l1: 0.162329\tvalid_1's l1: 0.179231\n",
      "[428]\ttraining's l1: 0.162165\tvalid_1's l1: 0.179108\n",
      "[429]\ttraining's l1: 0.162006\tvalid_1's l1: 0.178979\n",
      "[430]\ttraining's l1: 0.161846\tvalid_1's l1: 0.178843\n",
      "[431]\ttraining's l1: 0.161698\tvalid_1's l1: 0.17873\n",
      "[432]\ttraining's l1: 0.161549\tvalid_1's l1: 0.178614\n",
      "[433]\ttraining's l1: 0.161392\tvalid_1's l1: 0.178492\n",
      "[434]\ttraining's l1: 0.161262\tvalid_1's l1: 0.178395\n",
      "[435]\ttraining's l1: 0.161099\tvalid_1's l1: 0.178262\n",
      "[436]\ttraining's l1: 0.160967\tvalid_1's l1: 0.178154\n",
      "[437]\ttraining's l1: 0.160816\tvalid_1's l1: 0.178045\n",
      "[438]\ttraining's l1: 0.160681\tvalid_1's l1: 0.177939\n",
      "[439]\ttraining's l1: 0.160542\tvalid_1's l1: 0.177828\n",
      "[440]\ttraining's l1: 0.160409\tvalid_1's l1: 0.177731\n",
      "[441]\ttraining's l1: 0.16027\tvalid_1's l1: 0.177621\n",
      "[442]\ttraining's l1: 0.160158\tvalid_1's l1: 0.177533\n",
      "[443]\ttraining's l1: 0.160026\tvalid_1's l1: 0.177427\n",
      "[444]\ttraining's l1: 0.159915\tvalid_1's l1: 0.177339\n",
      "[445]\ttraining's l1: 0.159804\tvalid_1's l1: 0.177259\n",
      "[446]\ttraining's l1: 0.159683\tvalid_1's l1: 0.177169\n",
      "[447]\ttraining's l1: 0.159578\tvalid_1's l1: 0.177093\n",
      "[448]\ttraining's l1: 0.159445\tvalid_1's l1: 0.176996\n",
      "[449]\ttraining's l1: 0.159337\tvalid_1's l1: 0.176917\n",
      "[450]\ttraining's l1: 0.159217\tvalid_1's l1: 0.176832\n",
      "[451]\ttraining's l1: 0.159095\tvalid_1's l1: 0.17674\n",
      "[452]\ttraining's l1: 0.158998\tvalid_1's l1: 0.176674\n",
      "[453]\ttraining's l1: 0.158886\tvalid_1's l1: 0.176584\n",
      "[454]\ttraining's l1: 0.158789\tvalid_1's l1: 0.176513\n",
      "[455]\ttraining's l1: 0.158696\tvalid_1's l1: 0.176444\n",
      "[456]\ttraining's l1: 0.158577\tvalid_1's l1: 0.176361\n",
      "[457]\ttraining's l1: 0.158472\tvalid_1's l1: 0.176282\n",
      "[458]\ttraining's l1: 0.158358\tvalid_1's l1: 0.1762\n",
      "[459]\ttraining's l1: 0.158252\tvalid_1's l1: 0.176122\n",
      "[460]\ttraining's l1: 0.158132\tvalid_1's l1: 0.176035\n",
      "[461]\ttraining's l1: 0.15803\tvalid_1's l1: 0.175964\n",
      "[462]\ttraining's l1: 0.157941\tvalid_1's l1: 0.175903\n",
      "[463]\ttraining's l1: 0.157839\tvalid_1's l1: 0.175833\n",
      "[464]\ttraining's l1: 0.157732\tvalid_1's l1: 0.175746\n",
      "[465]\ttraining's l1: 0.157631\tvalid_1's l1: 0.17567\n",
      "[466]\ttraining's l1: 0.157523\tvalid_1's l1: 0.175595\n",
      "[467]\ttraining's l1: 0.157422\tvalid_1's l1: 0.17552\n",
      "[468]\ttraining's l1: 0.157312\tvalid_1's l1: 0.175443\n",
      "[469]\ttraining's l1: 0.157231\tvalid_1's l1: 0.175393\n",
      "[470]\ttraining's l1: 0.15714\tvalid_1's l1: 0.175332\n",
      "[471]\ttraining's l1: 0.157034\tvalid_1's l1: 0.175253\n",
      "[472]\ttraining's l1: 0.156952\tvalid_1's l1: 0.175202\n",
      "[473]\ttraining's l1: 0.156865\tvalid_1's l1: 0.175146\n",
      "[474]\ttraining's l1: 0.156764\tvalid_1's l1: 0.175078\n",
      "[475]\ttraining's l1: 0.156662\tvalid_1's l1: 0.17501\n",
      "[476]\ttraining's l1: 0.156566\tvalid_1's l1: 0.174945\n",
      "[477]\ttraining's l1: 0.156486\tvalid_1's l1: 0.174901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[478]\ttraining's l1: 0.156396\tvalid_1's l1: 0.174847\n",
      "[479]\ttraining's l1: 0.156301\tvalid_1's l1: 0.174784\n",
      "[480]\ttraining's l1: 0.156221\tvalid_1's l1: 0.17473\n",
      "[481]\ttraining's l1: 0.156128\tvalid_1's l1: 0.17466\n",
      "[482]\ttraining's l1: 0.156054\tvalid_1's l1: 0.174622\n",
      "[483]\ttraining's l1: 0.155965\tvalid_1's l1: 0.174564\n",
      "[484]\ttraining's l1: 0.155892\tvalid_1's l1: 0.174531\n",
      "[485]\ttraining's l1: 0.1558\tvalid_1's l1: 0.174463\n",
      "[486]\ttraining's l1: 0.155717\tvalid_1's l1: 0.17441\n",
      "[487]\ttraining's l1: 0.155634\tvalid_1's l1: 0.174353\n",
      "[488]\ttraining's l1: 0.155566\tvalid_1's l1: 0.174303\n",
      "[489]\ttraining's l1: 0.155468\tvalid_1's l1: 0.17424\n",
      "[490]\ttraining's l1: 0.15539\tvalid_1's l1: 0.174186\n",
      "[491]\ttraining's l1: 0.155309\tvalid_1's l1: 0.174134\n",
      "[492]\ttraining's l1: 0.155243\tvalid_1's l1: 0.174095\n",
      "[493]\ttraining's l1: 0.155158\tvalid_1's l1: 0.174042\n",
      "[494]\ttraining's l1: 0.155075\tvalid_1's l1: 0.173987\n",
      "[495]\ttraining's l1: 0.154996\tvalid_1's l1: 0.173931\n",
      "[496]\ttraining's l1: 0.154928\tvalid_1's l1: 0.17389\n",
      "[497]\ttraining's l1: 0.154865\tvalid_1's l1: 0.173854\n",
      "[498]\ttraining's l1: 0.154791\tvalid_1's l1: 0.173812\n",
      "[499]\ttraining's l1: 0.154704\tvalid_1's l1: 0.173752\n",
      "[500]\ttraining's l1: 0.154632\tvalid_1's l1: 0.17371\n",
      "[501]\ttraining's l1: 0.154573\tvalid_1's l1: 0.17368\n",
      "[502]\ttraining's l1: 0.154502\tvalid_1's l1: 0.173639\n",
      "[503]\ttraining's l1: 0.15444\tvalid_1's l1: 0.173606\n",
      "[504]\ttraining's l1: 0.154388\tvalid_1's l1: 0.173574\n",
      "[505]\ttraining's l1: 0.154312\tvalid_1's l1: 0.17352\n",
      "[506]\ttraining's l1: 0.154234\tvalid_1's l1: 0.173464\n",
      "[507]\ttraining's l1: 0.154178\tvalid_1's l1: 0.173434\n",
      "[508]\ttraining's l1: 0.15412\tvalid_1's l1: 0.173404\n",
      "[509]\ttraining's l1: 0.154064\tvalid_1's l1: 0.173378\n",
      "[510]\ttraining's l1: 0.15401\tvalid_1's l1: 0.173347\n",
      "[511]\ttraining's l1: 0.153959\tvalid_1's l1: 0.173321\n",
      "[512]\ttraining's l1: 0.153907\tvalid_1's l1: 0.173298\n",
      "[513]\ttraining's l1: 0.153854\tvalid_1's l1: 0.173262\n",
      "[514]\ttraining's l1: 0.153788\tvalid_1's l1: 0.173221\n",
      "[515]\ttraining's l1: 0.153737\tvalid_1's l1: 0.173186\n",
      "[516]\ttraining's l1: 0.153688\tvalid_1's l1: 0.173166\n",
      "[517]\ttraining's l1: 0.153638\tvalid_1's l1: 0.173135\n",
      "[518]\ttraining's l1: 0.15357\tvalid_1's l1: 0.173099\n",
      "[519]\ttraining's l1: 0.153521\tvalid_1's l1: 0.173068\n",
      "[520]\ttraining's l1: 0.153452\tvalid_1's l1: 0.173034\n",
      "[521]\ttraining's l1: 0.153405\tvalid_1's l1: 0.173004\n",
      "[522]\ttraining's l1: 0.153335\tvalid_1's l1: 0.172962\n",
      "[523]\ttraining's l1: 0.153287\tvalid_1's l1: 0.172938\n",
      "[524]\ttraining's l1: 0.153224\tvalid_1's l1: 0.172904\n",
      "[525]\ttraining's l1: 0.153179\tvalid_1's l1: 0.172881\n",
      "[526]\ttraining's l1: 0.153134\tvalid_1's l1: 0.172868\n",
      "[527]\ttraining's l1: 0.153074\tvalid_1's l1: 0.172834\n",
      "[528]\ttraining's l1: 0.153029\tvalid_1's l1: 0.172811\n",
      "[529]\ttraining's l1: 0.152962\tvalid_1's l1: 0.172777\n",
      "[530]\ttraining's l1: 0.15292\tvalid_1's l1: 0.172756\n",
      "[531]\ttraining's l1: 0.152854\tvalid_1's l1: 0.172714\n",
      "[532]\ttraining's l1: 0.15281\tvalid_1's l1: 0.172697\n",
      "[533]\ttraining's l1: 0.152766\tvalid_1's l1: 0.172678\n",
      "[534]\ttraining's l1: 0.152699\tvalid_1's l1: 0.17264\n",
      "[535]\ttraining's l1: 0.152633\tvalid_1's l1: 0.172606\n",
      "[536]\ttraining's l1: 0.15257\tvalid_1's l1: 0.172567\n",
      "[537]\ttraining's l1: 0.152527\tvalid_1's l1: 0.172552\n",
      "[538]\ttraining's l1: 0.152488\tvalid_1's l1: 0.172539\n",
      "[539]\ttraining's l1: 0.152424\tvalid_1's l1: 0.172507\n",
      "[540]\ttraining's l1: 0.152384\tvalid_1's l1: 0.172488\n",
      "[541]\ttraining's l1: 0.152344\tvalid_1's l1: 0.172467\n",
      "[542]\ttraining's l1: 0.152283\tvalid_1's l1: 0.172442\n",
      "[543]\ttraining's l1: 0.15225\tvalid_1's l1: 0.172429\n",
      "[544]\ttraining's l1: 0.152187\tvalid_1's l1: 0.172399\n",
      "[545]\ttraining's l1: 0.152149\tvalid_1's l1: 0.172393\n",
      "[546]\ttraining's l1: 0.152117\tvalid_1's l1: 0.172382\n",
      "[547]\ttraining's l1: 0.152074\tvalid_1's l1: 0.172361\n",
      "[548]\ttraining's l1: 0.152011\tvalid_1's l1: 0.172327\n",
      "[549]\ttraining's l1: 0.151982\tvalid_1's l1: 0.172316\n",
      "[550]\ttraining's l1: 0.151932\tvalid_1's l1: 0.172292\n",
      "[551]\ttraining's l1: 0.151886\tvalid_1's l1: 0.172279\n",
      "[552]\ttraining's l1: 0.151858\tvalid_1's l1: 0.172289\n",
      "[553]\ttraining's l1: 0.151822\tvalid_1's l1: 0.172277\n",
      "[554]\ttraining's l1: 0.151792\tvalid_1's l1: 0.17227\n",
      "[555]\ttraining's l1: 0.151767\tvalid_1's l1: 0.172275\n",
      "[556]\ttraining's l1: 0.151732\tvalid_1's l1: 0.17226\n",
      "[557]\ttraining's l1: 0.1517\tvalid_1's l1: 0.172252\n",
      "[558]\ttraining's l1: 0.151655\tvalid_1's l1: 0.172238\n",
      "[559]\ttraining's l1: 0.151617\tvalid_1's l1: 0.172231\n",
      "[560]\ttraining's l1: 0.151573\tvalid_1's l1: 0.172213\n",
      "[561]\ttraining's l1: 0.15153\tvalid_1's l1: 0.172199\n",
      "[562]\ttraining's l1: 0.151485\tvalid_1's l1: 0.172187\n",
      "[563]\ttraining's l1: 0.15146\tvalid_1's l1: 0.172182\n",
      "[564]\ttraining's l1: 0.151416\tvalid_1's l1: 0.172165\n",
      "[565]\ttraining's l1: 0.15138\tvalid_1's l1: 0.172151\n",
      "[566]\ttraining's l1: 0.151336\tvalid_1's l1: 0.172134\n",
      "[567]\ttraining's l1: 0.151289\tvalid_1's l1: 0.172115\n",
      "[568]\ttraining's l1: 0.151253\tvalid_1's l1: 0.172107\n",
      "[569]\ttraining's l1: 0.151205\tvalid_1's l1: 0.172088\n",
      "[570]\ttraining's l1: 0.15117\tvalid_1's l1: 0.172082\n",
      "[571]\ttraining's l1: 0.151135\tvalid_1's l1: 0.172077\n",
      "[572]\ttraining's l1: 0.151111\tvalid_1's l1: 0.172086\n",
      "[573]\ttraining's l1: 0.151086\tvalid_1's l1: 0.172093\n",
      "[574]\ttraining's l1: 0.15106\tvalid_1's l1: 0.1721\n",
      "[575]\ttraining's l1: 0.151035\tvalid_1's l1: 0.172103\n",
      "[576]\ttraining's l1: 0.150992\tvalid_1's l1: 0.172088\n",
      "[577]\ttraining's l1: 0.15095\tvalid_1's l1: 0.172069\n",
      "[578]\ttraining's l1: 0.150907\tvalid_1's l1: 0.172056\n",
      "[579]\ttraining's l1: 0.150866\tvalid_1's l1: 0.172042\n",
      "[580]\ttraining's l1: 0.150824\tvalid_1's l1: 0.172029\n",
      "[581]\ttraining's l1: 0.150779\tvalid_1's l1: 0.172018\n",
      "[582]\ttraining's l1: 0.150734\tvalid_1's l1: 0.172003\n",
      "[583]\ttraining's l1: 0.150692\tvalid_1's l1: 0.171992\n",
      "[584]\ttraining's l1: 0.150645\tvalid_1's l1: 0.171975\n",
      "[585]\ttraining's l1: 0.150614\tvalid_1's l1: 0.171971\n",
      "[586]\ttraining's l1: 0.150571\tvalid_1's l1: 0.171955\n",
      "[587]\ttraining's l1: 0.15053\tvalid_1's l1: 0.171943\n",
      "[588]\ttraining's l1: 0.150486\tvalid_1's l1: 0.171927\n",
      "[589]\ttraining's l1: 0.150441\tvalid_1's l1: 0.171903\n",
      "[590]\ttraining's l1: 0.150402\tvalid_1's l1: 0.171884\n",
      "[591]\ttraining's l1: 0.150356\tvalid_1's l1: 0.171857\n",
      "[592]\ttraining's l1: 0.150317\tvalid_1's l1: 0.171848\n",
      "[593]\ttraining's l1: 0.150279\tvalid_1's l1: 0.171837\n",
      "[594]\ttraining's l1: 0.150256\tvalid_1's l1: 0.17183\n",
      "[595]\ttraining's l1: 0.150215\tvalid_1's l1: 0.17181\n",
      "[596]\ttraining's l1: 0.150167\tvalid_1's l1: 0.171786\n",
      "[597]\ttraining's l1: 0.150144\tvalid_1's l1: 0.171779\n",
      "[598]\ttraining's l1: 0.150121\tvalid_1's l1: 0.171772\n",
      "[599]\ttraining's l1: 0.150083\tvalid_1's l1: 0.171757\n",
      "[600]\ttraining's l1: 0.150059\tvalid_1's l1: 0.171757\n",
      "[601]\ttraining's l1: 0.150023\tvalid_1's l1: 0.171743\n",
      "[602]\ttraining's l1: 0.149977\tvalid_1's l1: 0.171717\n",
      "[603]\ttraining's l1: 0.149942\tvalid_1's l1: 0.171702\n",
      "[604]\ttraining's l1: 0.149902\tvalid_1's l1: 0.171686\n",
      "[605]\ttraining's l1: 0.149855\tvalid_1's l1: 0.171661\n",
      "[606]\ttraining's l1: 0.14982\tvalid_1's l1: 0.171649\n",
      "[607]\ttraining's l1: 0.149798\tvalid_1's l1: 0.171648\n",
      "[608]\ttraining's l1: 0.149775\tvalid_1's l1: 0.171642\n",
      "[609]\ttraining's l1: 0.149738\tvalid_1's l1: 0.17163\n",
      "[610]\ttraining's l1: 0.149717\tvalid_1's l1: 0.171643\n",
      "[611]\ttraining's l1: 0.149697\tvalid_1's l1: 0.171648\n",
      "[612]\ttraining's l1: 0.149653\tvalid_1's l1: 0.171627\n",
      "[613]\ttraining's l1: 0.149631\tvalid_1's l1: 0.171647\n",
      "[614]\ttraining's l1: 0.149598\tvalid_1's l1: 0.171633\n",
      "[615]\ttraining's l1: 0.149578\tvalid_1's l1: 0.171645\n",
      "[616]\ttraining's l1: 0.149534\tvalid_1's l1: 0.171628\n",
      "[617]\ttraining's l1: 0.149507\tvalid_1's l1: 0.171619\n",
      "[618]\ttraining's l1: 0.14948\tvalid_1's l1: 0.171614\n",
      "[619]\ttraining's l1: 0.149459\tvalid_1's l1: 0.171635\n",
      "[620]\ttraining's l1: 0.149439\tvalid_1's l1: 0.171655\n",
      "[621]\ttraining's l1: 0.149401\tvalid_1's l1: 0.171648\n",
      "[622]\ttraining's l1: 0.149369\tvalid_1's l1: 0.171637\n",
      "[623]\ttraining's l1: 0.14935\tvalid_1's l1: 0.171653\n",
      "[624]\ttraining's l1: 0.14933\tvalid_1's l1: 0.171671\n",
      "[625]\ttraining's l1: 0.149303\tvalid_1's l1: 0.171662\n",
      "[626]\ttraining's l1: 0.14927\tvalid_1's l1: 0.171649\n",
      "[627]\ttraining's l1: 0.14925\tvalid_1's l1: 0.171663\n",
      "[628]\ttraining's l1: 0.149212\tvalid_1's l1: 0.171655\n",
      "[629]\ttraining's l1: 0.149183\tvalid_1's l1: 0.171646\n",
      "[630]\ttraining's l1: 0.149152\tvalid_1's l1: 0.171641\n",
      "[631]\ttraining's l1: 0.149133\tvalid_1's l1: 0.171655\n",
      "[632]\ttraining's l1: 0.149101\tvalid_1's l1: 0.171651\n",
      "[633]\ttraining's l1: 0.149083\tvalid_1's l1: 0.171663\n",
      "[634]\ttraining's l1: 0.149052\tvalid_1's l1: 0.171655\n",
      "[635]\ttraining's l1: 0.149018\tvalid_1's l1: 0.171641\n",
      "[636]\ttraining's l1: 0.148984\tvalid_1's l1: 0.171638\n",
      "[637]\ttraining's l1: 0.148954\tvalid_1's l1: 0.171633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[638]\ttraining's l1: 0.148938\tvalid_1's l1: 0.171646\n",
      "[639]\ttraining's l1: 0.148905\tvalid_1's l1: 0.171645\n",
      "[640]\ttraining's l1: 0.148874\tvalid_1's l1: 0.171639\n",
      "[641]\ttraining's l1: 0.148841\tvalid_1's l1: 0.171632\n",
      "[642]\ttraining's l1: 0.148809\tvalid_1's l1: 0.171629\n",
      "[643]\ttraining's l1: 0.148777\tvalid_1's l1: 0.171622\n",
      "[644]\ttraining's l1: 0.148764\tvalid_1's l1: 0.171628\n",
      "[645]\ttraining's l1: 0.148734\tvalid_1's l1: 0.171626\n",
      "[646]\ttraining's l1: 0.148702\tvalid_1's l1: 0.171617\n",
      "[647]\ttraining's l1: 0.148683\tvalid_1's l1: 0.171638\n",
      "[648]\ttraining's l1: 0.148653\tvalid_1's l1: 0.171634\n",
      "[649]\ttraining's l1: 0.14862\tvalid_1's l1: 0.171623\n",
      "[650]\ttraining's l1: 0.148591\tvalid_1's l1: 0.171623\n",
      "[651]\ttraining's l1: 0.148576\tvalid_1's l1: 0.171629\n",
      "[652]\ttraining's l1: 0.14854\tvalid_1's l1: 0.171612\n",
      "[653]\ttraining's l1: 0.148513\tvalid_1's l1: 0.17161\n",
      "[654]\ttraining's l1: 0.148478\tvalid_1's l1: 0.171601\n",
      "[655]\ttraining's l1: 0.148461\tvalid_1's l1: 0.171626\n",
      "[656]\ttraining's l1: 0.148429\tvalid_1's l1: 0.171628\n",
      "[657]\ttraining's l1: 0.14841\tvalid_1's l1: 0.171631\n",
      "[658]\ttraining's l1: 0.148373\tvalid_1's l1: 0.171628\n",
      "[659]\ttraining's l1: 0.148348\tvalid_1's l1: 0.171629\n",
      "[660]\ttraining's l1: 0.148329\tvalid_1's l1: 0.171635\n",
      "[661]\ttraining's l1: 0.148311\tvalid_1's l1: 0.17164\n",
      "[662]\ttraining's l1: 0.148274\tvalid_1's l1: 0.171627\n",
      "[663]\ttraining's l1: 0.148257\tvalid_1's l1: 0.171651\n",
      "[664]\ttraining's l1: 0.148239\tvalid_1's l1: 0.171656\n",
      "[665]\ttraining's l1: 0.148221\tvalid_1's l1: 0.171661\n",
      "[666]\ttraining's l1: 0.148204\tvalid_1's l1: 0.171665\n",
      "[667]\ttraining's l1: 0.148187\tvalid_1's l1: 0.171671\n",
      "[668]\ttraining's l1: 0.14817\tvalid_1's l1: 0.171676\n",
      "[669]\ttraining's l1: 0.148155\tvalid_1's l1: 0.1717\n",
      "[670]\ttraining's l1: 0.148128\tvalid_1's l1: 0.171705\n",
      "[671]\ttraining's l1: 0.148098\tvalid_1's l1: 0.171694\n",
      "[672]\ttraining's l1: 0.148081\tvalid_1's l1: 0.171707\n",
      "[673]\ttraining's l1: 0.148047\tvalid_1's l1: 0.171694\n",
      "[674]\ttraining's l1: 0.148019\tvalid_1's l1: 0.171694\n",
      "[675]\ttraining's l1: 0.148004\tvalid_1's l1: 0.171703\n",
      "[676]\ttraining's l1: 0.147974\tvalid_1's l1: 0.171702\n",
      "[677]\ttraining's l1: 0.14796\tvalid_1's l1: 0.171713\n",
      "[678]\ttraining's l1: 0.147943\tvalid_1's l1: 0.171721\n",
      "[679]\ttraining's l1: 0.147917\tvalid_1's l1: 0.171721\n",
      "[680]\ttraining's l1: 0.147903\tvalid_1's l1: 0.171731\n",
      "[681]\ttraining's l1: 0.147869\tvalid_1's l1: 0.171722\n",
      "[682]\ttraining's l1: 0.147856\tvalid_1's l1: 0.171731\n",
      "[683]\ttraining's l1: 0.147841\tvalid_1's l1: 0.171758\n",
      "[684]\ttraining's l1: 0.147816\tvalid_1's l1: 0.171756\n",
      "[685]\ttraining's l1: 0.147799\tvalid_1's l1: 0.171775\n",
      "[686]\ttraining's l1: 0.147787\tvalid_1's l1: 0.171787\n",
      "[687]\ttraining's l1: 0.147771\tvalid_1's l1: 0.171803\n",
      "[688]\ttraining's l1: 0.147758\tvalid_1's l1: 0.17183\n",
      "[689]\ttraining's l1: 0.14773\tvalid_1's l1: 0.171837\n",
      "[690]\ttraining's l1: 0.147698\tvalid_1's l1: 0.171835\n",
      "[691]\ttraining's l1: 0.147682\tvalid_1's l1: 0.171854\n",
      "[692]\ttraining's l1: 0.147671\tvalid_1's l1: 0.171866\n",
      "[693]\ttraining's l1: 0.147651\tvalid_1's l1: 0.171868\n",
      "[694]\ttraining's l1: 0.147626\tvalid_1's l1: 0.171867\n",
      "[695]\ttraining's l1: 0.147607\tvalid_1's l1: 0.171871\n",
      "[696]\ttraining's l1: 0.147587\tvalid_1's l1: 0.171873\n",
      "[697]\ttraining's l1: 0.147563\tvalid_1's l1: 0.171868\n",
      "[698]\ttraining's l1: 0.147542\tvalid_1's l1: 0.17187\n",
      "[699]\ttraining's l1: 0.147523\tvalid_1's l1: 0.171873\n",
      "[700]\ttraining's l1: 0.147503\tvalid_1's l1: 0.171874\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[700]\ttraining's l1: 0.147503\tvalid_1's l1: 0.171874\n",
      "The rmlse of prediction is: 0.09949\n",
      "Feature importances: [0, 425, 2082, 19, 767, 0, 650, 1657, 1244, 353, 496, 143, 83, 75, 116, 2813, 4812, 1493, 0, 2212, 1508, 1193, 1356, 1122, 1114, 715, 865, 5163, 2257, 4826, 1903, 6838]\n",
      "The rmlse of prediction is: 0.1001\n"
     ]
    }
   ],
   "source": [
    "get_best_lgb(path,3,N=700)\n",
    "# rmsle4=get_lgb(path,4,N=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle6=get_lgb(path,6,N=300) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
