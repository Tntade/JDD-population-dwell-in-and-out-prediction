{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_train = pd.read_csv('../data/flow_train.csv')\n",
    "tran_train = pd.read_csv('../data/transition_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========数据初探==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26852, 6) (2480320, 6)\n"
     ]
    }
   ],
   "source": [
    "print(flow_train.shape,tran_train.shape)\n",
    "#flow_train,6列数据，2.7万样本，1个日期，2文本，3数据列，3M多内存\n",
    "#tran_train,6列数据，24.8万样本，1日期，4文本，1数据列，382M多内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c7537db4101856877ea6381d0174283c    6028\n",
      "3f7f0ce35d6d0a08377eb2efe2189f4f    6028\n",
      "58a33c947775af5de36841c9f553317d    4384\n",
      "a20d041605db832309e26c003c626719    4384\n",
      "06d86ef037e4bd311b94467c3320ff38    3014\n",
      "5615dc7c1af1f7dabd80bd8b8ecb1ea0    2740\n",
      "ee2ff207184bf16b4a0aec0f97900c27     274\n",
      "Name: city_code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(flow_train.city_code.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(flow_train.columns)\n",
    "# #日期，城市编码，区县编码，当天驻留人数，当天流入人数，当天流出人数\n",
    "# print(tran_train.columns)\n",
    "# #日期，出发城市编号，出发区县编号，到达城市编号，到达区县编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tran_train.date_dt.min())\n",
    "# print(tran_train.date_dt.max())\n",
    "# print(tran_train.year.value_counts())\n",
    "# print(tran_train.month.value_counts())\n",
    "# print(tran_train.day.value_counts())\n",
    "\n",
    "#flow_train 2017.6.1-2018.3.1 \n",
    "#tran_train 2017.6.1-2018.3.1\n",
    "#2017.6.1为周四\n",
    "\n",
    "#所以未来15天的预测 ，便是从2018.3.2-2018.3.16\n",
    "#所以分别预测驻留人数、流入人数、流出人数，那么分别将他们依次作为标签去进行预测\n",
    "#是否可以即把他们当作标签又当作特征，当预测dwell时候，flow_in,flow_out均作为特征，\n",
    "#然后预测flow_in时，将dwell,flow_out作为特征\n",
    "#是否可以将（flow_in,flow_out）作为一个特征呢？？？不妨尝试下！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下为算法乱想，忽略\n",
    "#flow_in和flow_out之间满足线性关系，y=a(flow_in)+b(flow_out)？\n",
    "#而a b分别通过以上的特征进行预测，又是一个线性关系，例a=w1*x1+w2*x2+w3*x3,b也是如此\n",
    "#所以变成一个算法预测a b，然后得到新的特征a b，再用linearRegressor去预测y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_train['flow_in_dwell']=round(flow_train['flow_in']/flow_train['dwell'],6)\n",
    "flow_train['flow_out_dwell']=round(flow_train['flow_out']/flow_train['dwell'],6)\n",
    "flow_train['flow_in_out']=round(flow_train['flow_in']/flow_train['flow_out'],6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_train['address'] = flow_train['city_code']+':'+flow_train['district_code']\n",
    "address = list(set(flow_train['address']))\n",
    "\n",
    "tran_train['start_address']=tran_train['o_city_code']+':'+tran_train['o_district_code']\n",
    "tran_train['arr_address']=tran_train['d_city_code']+':'+tran_train['d_district_code']\n",
    "start_address=list(set(tran_train['start_address']))\n",
    "arr_address=list(set(tran_train['arr_address']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>此处代码证明flow_train的数据来自于tran_train，所以大可以安心的从tran_train中提取新特征即可<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据tran_train计算98个区县的流出人数=sum(cnt)\n",
    "# tran_train['address']=tran_train['start_address']\n",
    "# tmp=tran_train.groupby(['date_dt','address'],as_index=False)['cnt'].agg({'cnt':'sum'})\n",
    "# tmp.reset_index()\n",
    "# tmp.rename(columns={'cnt':'flow_out_cnt'},inplace=True)\n",
    "# # print(tmp)\n",
    "# flow_train=pd.merge(flow_train,tmp,on=['date_dt','address'],how='left')\n",
    "# # print(flow_train.shape)\n",
    "\n",
    "# #事实证明，flow_out_cnt==flow_out,因此flow_train.address==tran_train.start_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt=tran_train.groupby(['date_dt','arr_address'],as_index=False)['cnt'].agg({'cnt':'sum'})\n",
    "# tt.reset_index()\n",
    "# tt.rename(columns={'cnt':'flow_in_cnt'},inplace=True)\n",
    "\n",
    "# tt['address']=tt['arr_address']\n",
    "\n",
    "# flow_train=pd.merge(flow_train,tt,on=['date_dt','address'],how='left')\n",
    "# # print(flow_train[['flow_in','flow_in_cnt']])\n",
    "\n",
    "# 事实证明，flow_in_cnt==flow_in\n",
    "# 因此，flow_train中的flow_in and flow_out均来自于tran_train表格，并且没有遗漏，数据完全吻合\n",
    "# 因此，只要从tran_train中提取feature，对应的拼接到flow_train中即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>以上代码证明flow_train的数据来自于tran_train，所以大可以安心的从tran_train中提取新特征即可<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===对tran_train处理,得到新特征合并到flow_train里====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26852, 10)\n"
     ]
    }
   ],
   "source": [
    "print(flow_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=tran_train.groupby(['date_dt','start_address'],as_index=False)['cnt'].agg({'cnt':'count'})\n",
    "tmp['address']=tmp['start_address']\n",
    "tmp['flow_out_count']=tmp['cnt']\n",
    "flow_train=flow_train.merge(tmp[['date_dt','address','flow_out_count']],how='left',on=['date_dt','address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp=tran_train.groupby(['date_dt','arr_address'],as_index=False)['cnt'].agg({'cnt':'count'})\n",
    "tmp['address']=tmp['arr_address']\n",
    "tmp['flow_in_count']=tmp['cnt']\n",
    "flow_train=flow_train.merge(tmp[['date_dt','address','flow_in_count']],how='left',on=['date_dt','address'])\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date_dt', 'city_code', 'district_code', 'dwell', 'flow_in', 'flow_out',\n",
      "       'flow_in_dwell', 'flow_out_dwell', 'flow_in_out', 'address',\n",
      "       'flow_out_count', 'flow_in_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(flow_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year(date):\n",
    "    date = str(date)\n",
    "    return int(date[0:4])\n",
    "def month(date):\n",
    "    date = str(date)\n",
    "    return int(date[4:6])\n",
    "def day(date):\n",
    "    date = str(date)\n",
    "    return int(date[6:8])\n",
    "\n",
    "flow_train['year'] = flow_train['date_dt'].apply(year)\n",
    "flow_train['month'] = flow_train['date_dt'].apply(month)\n",
    "flow_train['day'] = flow_train['date_dt'].apply(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "flow_train['weekd']=flow_train['date_dt'].apply(lambda x:datetime.strptime(str(x),'%Y%m%d').weekday())\n",
    "flow_train['weekd']=flow_train['weekd']+1\n",
    "flow_train['weekd']=flow_train['weekd'].astype(int)\n",
    "\n",
    "#1-7分别代表周一到周日\n",
    "# tran_train['weekd']=tran_train['date_dt'].apply(lambda x:datetime.strptime(str(x),'%Y%m%d').weekday())\n",
    "# tran_train['weekd']=tran_train['weekd']+1\n",
    "\n",
    "#分成上中下旬，不过用weekd的特征已经代替了，再加上便是冗余数据了\n",
    "# def get_xun(x):\n",
    "#     if x<=10:\n",
    "#         return 1\n",
    "#     elif x<=20:\n",
    "#         return 2\n",
    "#     else:\n",
    "#         return 3\n",
    "    \n",
    "# flow_train['xun']=0\n",
    "# flow_train['xun']=flow_train['day'].apply(get_xun)\n",
    "# tran_train['xun']=0\n",
    "# tran_train['xun']=tran_train['xun'].apply(get_xun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date_dt', 'city_code', 'district_code', 'dwell', 'flow_in', 'flow_out',\n",
      "       'flow_in_dwell', 'flow_out_dwell', 'flow_in_out', 'address',\n",
      "       'flow_out_count', 'flow_in_count', 'year', 'month', 'day', 'weekd'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(flow_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_train['flow_in_mean']=round(flow_train['flow_in']/flow_train['flow_in_count'],6)\n",
    "flow_train['flow_out_mean']=round(flow_train['flow_out']/flow_train['flow_out_count'],6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "flow_train['address_le']=le.fit_transform(flow_train['address'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.区县之间的距离？？？<br><br>2.一维的交叉特征的nunique\\count\\sum\\mean\\std\\crt基本特征，由于还不是特别数量，建议一个一写！<br><br>3.同一个区县的样本进出数量，日平均进出样本数量；星期1234567分别进出样本数量；<br><br>4.离开某一区县的第一次时间，对应的是星期几，第二次时间，对应的是星期几，对应的间隔时间，看看是否有规律可寻；以及进入某一区县的间隔天数，离开某一区县进入 另一区县的间隔天数，每月的次数合计，17年合计，18年合计，17年平均，18年平均；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===============以下模型部分=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以确定是回归模型：\n",
    "每一群模型，建立一个Pipeline去执行察看结果\n",
    "#模型：LinearRegressor\n",
    "#模型：Lasso\\Ridge加入了l1 l2正则化，防止过拟合\n",
    "#模型：神经网络LasticNet？\n",
    "\n",
    "#模型：SVM\n",
    "#模型：KNN（按照城市各自训练的时候，感觉比较适用）\n",
    "\n",
    "#模型：GDBT\n",
    "#模型：lightGBM训练，但是容易过拟合，需要好好优化参数\n",
    "#模型：XGBoost\n",
    "#模型：Adaboost\n",
    "#模型：Catboost\n",
    "\n",
    "#模型：以上最好的模型用BaggingRegressor()融合\n",
    "#模型：以上几个，bagging(选择几个比较好的模型，按照成绩进行权重分配)\n",
    "#模型：以上几个，blending(选择好的与不好的几个，进行blending)\n",
    "#模型：基本属于线性可分型，不适用stacking融合#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "star = '20180301'\n",
    "dates = []\n",
    "for i in range(1,16):\n",
    "    date_format = datetime.datetime.strptime(star,'%Y%m%d')\n",
    "    fut_date = date_format + datetime.timedelta(days=i)\n",
    "    dates.append(int(datetime.datetime.strftime(fut_date,'%Y%m%d')))\n",
    "# print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'date_dt':dates})\n",
    "test_df['year'] = test_df['date_dt'].apply(year)\n",
    "test_df['month'] = test_df['date_dt'].apply(month)\n",
    "test_df['day'] = test_df['date_dt'].apply(day)\n",
    "test_df['date_dt'] = test_df['date_dt'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['date_dt', 'city_code', 'district_code', 'dwell', 'flow_in', 'flow_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_train_1 = flow_train[flow_train['date_dt'] < 20180214]\n",
    "flow_test_1 = flow_train[flow_train['date_dt'] >= 20180214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['dwell','flow_in','flow_out']\n",
    "feature = ['flow_in_dwell','flow_out_dwell','flow_in_out','flow_out_count','flow_in_count','year',\n",
    "          'month','day','weekd','flow_in_mean','flow_out_mean','address_le']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l1: 25.7687\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[2]\tvalid_0's l1: 24.5175\n",
      "[3]\tvalid_0's l1: 23.2233\n",
      "[4]\tvalid_0's l1: 21.9311\n",
      "[5]\tvalid_0's l1: 20.8786\n",
      "[6]\tvalid_0's l1: 19.7658\n",
      "[7]\tvalid_0's l1: 18.6619\n",
      "[8]\tvalid_0's l1: 17.6277\n",
      "[9]\tvalid_0's l1: 16.6407\n",
      "[10]\tvalid_0's l1: 15.7905\n",
      "[11]\tvalid_0's l1: 14.9251\n",
      "[12]\tvalid_0's l1: 14.1581\n",
      "[13]\tvalid_0's l1: 13.3926\n",
      "[14]\tvalid_0's l1: 12.6496\n",
      "[15]\tvalid_0's l1: 12.0287\n",
      "[16]\tvalid_0's l1: 11.4129\n",
      "[17]\tvalid_0's l1: 10.7698\n",
      "[18]\tvalid_0's l1: 10.2209\n",
      "[19]\tvalid_0's l1: 9.68541\n",
      "[20]\tvalid_0's l1: 9.10159\n",
      "[21]\tvalid_0's l1: 8.52576\n",
      "[22]\tvalid_0's l1: 8.07405\n",
      "[23]\tvalid_0's l1: 7.63391\n",
      "[24]\tvalid_0's l1: 7.28163\n",
      "[25]\tvalid_0's l1: 6.82898\n",
      "[26]\tvalid_0's l1: 6.48747\n",
      "[27]\tvalid_0's l1: 6.12216\n",
      "[28]\tvalid_0's l1: 5.81819\n",
      "[29]\tvalid_0's l1: 5.37889\n",
      "[30]\tvalid_0's l1: 5.16695\n",
      "[31]\tvalid_0's l1: 4.93243\n",
      "[32]\tvalid_0's l1: 4.66791\n",
      "[33]\tvalid_0's l1: 4.31503\n",
      "[34]\tvalid_0's l1: 4.07617\n",
      "[35]\tvalid_0's l1: 3.85216\n",
      "[36]\tvalid_0's l1: 3.69812\n",
      "[37]\tvalid_0's l1: 3.42584\n",
      "[38]\tvalid_0's l1: 3.34732\n",
      "[39]\tvalid_0's l1: 3.19632\n",
      "[40]\tvalid_0's l1: 2.94709\n",
      "[41]\tvalid_0's l1: 2.7344\n",
      "[42]\tvalid_0's l1: 2.60272\n",
      "[43]\tvalid_0's l1: 2.38908\n",
      "[44]\tvalid_0's l1: 2.29994\n",
      "[45]\tvalid_0's l1: 2.22972\n",
      "[46]\tvalid_0's l1: 2.084\n",
      "[47]\tvalid_0's l1: 1.91529\n",
      "[48]\tvalid_0's l1: 1.84756\n",
      "[49]\tvalid_0's l1: 1.70153\n",
      "[50]\tvalid_0's l1: 1.65722\n",
      "[51]\tvalid_0's l1: 1.54853\n",
      "[52]\tvalid_0's l1: 1.50437\n",
      "[53]\tvalid_0's l1: 1.3995\n",
      "[54]\tvalid_0's l1: 1.37665\n",
      "[55]\tvalid_0's l1: 1.39669\n",
      "[56]\tvalid_0's l1: 1.38802\n",
      "[57]\tvalid_0's l1: 1.3501\n",
      "[58]\tvalid_0's l1: 1.32962\n",
      "[59]\tvalid_0's l1: 1.35231\n",
      "[60]\tvalid_0's l1: 1.33435\n",
      "[61]\tvalid_0's l1: 1.33127\n",
      "[62]\tvalid_0's l1: 1.31578\n",
      "[63]\tvalid_0's l1: 1.33278\n",
      "[64]\tvalid_0's l1: 1.32155\n",
      "[65]\tvalid_0's l1: 1.3308\n",
      "[66]\tvalid_0's l1: 1.3147\n",
      "[67]\tvalid_0's l1: 1.31117\n",
      "[68]\tvalid_0's l1: 1.29773\n",
      "[69]\tvalid_0's l1: 1.28521\n",
      "[70]\tvalid_0's l1: 1.30656\n",
      "[71]\tvalid_0's l1: 1.32308\n",
      "[72]\tvalid_0's l1: 1.32086\n",
      "[73]\tvalid_0's l1: 1.32888\n",
      "[74]\tvalid_0's l1: 1.35683\n",
      "[75]\tvalid_0's l1: 1.3504\n",
      "[76]\tvalid_0's l1: 1.35694\n",
      "[77]\tvalid_0's l1: 1.33649\n",
      "[78]\tvalid_0's l1: 1.35097\n",
      "[79]\tvalid_0's l1: 1.37275\n",
      "[80]\tvalid_0's l1: 1.38938\n",
      "[81]\tvalid_0's l1: 1.3574\n",
      "[82]\tvalid_0's l1: 1.35589\n",
      "[83]\tvalid_0's l1: 1.3396\n",
      "[84]\tvalid_0's l1: 1.35338\n",
      "[85]\tvalid_0's l1: 1.34241\n",
      "[86]\tvalid_0's l1: 1.32616\n",
      "[87]\tvalid_0's l1: 1.31396\n",
      "[88]\tvalid_0's l1: 1.33376\n",
      "[89]\tvalid_0's l1: 1.35247\n",
      "[90]\tvalid_0's l1: 1.37025\n",
      "[91]\tvalid_0's l1: 1.36897\n",
      "[92]\tvalid_0's l1: 1.36999\n",
      "[93]\tvalid_0's l1: 1.38487\n",
      "[94]\tvalid_0's l1: 1.38138\n",
      "[95]\tvalid_0's l1: 1.37096\n",
      "[96]\tvalid_0's l1: 1.3876\n",
      "[97]\tvalid_0's l1: 1.37687\n",
      "[98]\tvalid_0's l1: 1.40209\n",
      "[99]\tvalid_0's l1: 1.40781\n",
      "[100]\tvalid_0's l1: 1.40237\n",
      "[101]\tvalid_0's l1: 1.3966\n",
      "[102]\tvalid_0's l1: 1.40887\n",
      "[103]\tvalid_0's l1: 1.38586\n",
      "[104]\tvalid_0's l1: 1.36632\n",
      "[105]\tvalid_0's l1: 1.36207\n",
      "[106]\tvalid_0's l1: 1.34886\n",
      "[107]\tvalid_0's l1: 1.36676\n",
      "[108]\tvalid_0's l1: 1.35565\n",
      "[109]\tvalid_0's l1: 1.35121\n",
      "[110]\tvalid_0's l1: 1.34632\n",
      "[111]\tvalid_0's l1: 1.344\n",
      "[112]\tvalid_0's l1: 1.33178\n",
      "[113]\tvalid_0's l1: 1.32728\n",
      "[114]\tvalid_0's l1: 1.30859\n",
      "[115]\tvalid_0's l1: 1.30458\n",
      "[116]\tvalid_0's l1: 1.31163\n",
      "[117]\tvalid_0's l1: 1.30875\n",
      "[118]\tvalid_0's l1: 1.29968\n",
      "[119]\tvalid_0's l1: 1.3064\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's l1: 1.28521\n",
      "The rmsle of prediction is: 0.008806928942044533\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['flow_in_dwell' 'flow_out_dwell' 'flow_in_out' 'flow_out_count'\\n 'flow_in_count' 'weekd' 'flow_in_mean' 'flow_out_mean' 'address_le'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-581-bcf0a7bbcea8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The rmsle of prediction is:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmsle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2132\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2133\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2134\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2175\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1267\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1269\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['flow_in_dwell' 'flow_out_dwell' 'flow_in_out' 'flow_out_count'\\n 'flow_in_count' 'weekd' 'flow_in_mean' 'flow_out_mean' 'address_le'] not in index\""
     ]
    }
   ],
   "source": [
    "for ad in address:\n",
    "    ad_split = ad.split(':')\n",
    "    test_df['city_code'] = ad_split[0]\n",
    "    test_df['district_code'] = ad_split[1]\n",
    "    \n",
    "    for y in label:\n",
    "        train_x = flow_train[flow_train['address']==ad][feature]\n",
    "        train_y = flow_train[flow_train['address']==ad][y]\n",
    "        test_x = flow_test_1[flow_test_1['address']==ad][feature]\n",
    "        test_y = flow_test_1[flow_test_1['address']==ad][y]\n",
    "        \n",
    "        gbm = lgb.LGBMRegressor(num_leaves=50,\n",
    "                        learning_rate=0.05,\n",
    "                        n_estimators=1000)\n",
    "        \n",
    "        gbm.fit(train_x, train_y, \n",
    "                eval_set=[(test_x, test_y)],\n",
    "                eval_metric='l1',\n",
    "                early_stopping_rounds=50)\n",
    "        \n",
    "        # predict\n",
    "        y_pred = gbm.predict(test_x, num_iteration=gbm.best_iteration_)\n",
    "        # eval\n",
    "        print('The rmsle of prediction is:', rmsle(test_y, y_pred)[1])\n",
    "        \n",
    "        test_df[y] = gbm.predict(test_df[feature])\n",
    "        \n",
    "    result = pd.concat([result,test_df[result.columns]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['date_dt'] = result['date_dt'].astype(int)\n",
    "result = result[['date_dt', 'city_code', 'district_code', 'dwell', 'flow_in', 'flow_out']]\n",
    "result.to_csv('..\\data\\predictionV1.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
